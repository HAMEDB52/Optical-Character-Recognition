!pip install kaggle
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
print("Kaggle API key uploaded and configured.")
# Step 1: Download dataset using kagglehub
import kagglehub
import os

# Download latest version
path = kagglehub.dataset_download("humansintheloop/arabic-documents-ocr-dataset")
print("Path to dataset files:", path)

# Step 2: Verify the dataset structure
def explore_dataset_structure(base_path, max_depth=3):
    """Explore and print the dataset structure"""
    def _explore(path, current_depth=0):
        if current_depth > max_depth or not os.path.exists(path):
            return

        print("  " * current_depth + f"ðŸ“ {os.path.basename(path)}/")

        try:
            items = os.listdir(path)
            for item in items[:5]:  # Show first 5 items
                item_path = os.path.join(path, item)
                if os.path.isdir(item_path):
                    _explore(item_path, current_depth + 1)
                else:
                    print("  " * (current_depth + 1) + f"ðŸ“„ {item}")
            if len(items) > 5:
                print("  " * (current_depth + 1) + f"... and {len(items) - 5} more items")
        except PermissionError:
            print("  " * (current_depth + 1) + "âŒ Permission denied")

    print("Dataset structure:")
    _explore(base_path)

# Explore the downloaded dataset
explore_dataset_structure(path)

# Step 3: Now run the OCR training with the correct path
print(f"\nNow you can run the OCR training with dataset path: {path}")
print("Use this in your ArabicOCRDataset initialization:")
print(f"dataset = ArabicOCRDataset('{path}')")

# Step 4: Alternative - Auto-run the training
print("\n" + "="*50)
print("STARTING OCR TRAINING...")
print("="*50)

# Import and run the main OCR training function
# Make sure the OCR training code is available in your environment
try:
    # Initialize dataset with the downloaded path
    from your_ocr_module import ArabicOCRDataset, CTCModel, OCRTrainer  # Replace with actual import

    dataset = ArabicOCRDataset(path)
    dataset.load_dataset()

    if len(dataset.data) == 0:
        print("No data found! Please check the dataset structure.")
    else:
        # Continue with training...
        train_dataset, val_dataset = dataset.create_dataset(batch_size=16)

        # Build model
        ctc_model = CTCModel(vocab_size=dataset.vocab_size)
        model = ctc_model.build_model()

        print("Model architecture:")
        model.summary()

        # Train model
        trainer = OCRTrainer(dataset, model)
        history = trainer.train(train_dataset, val_dataset, epochs=50)

        print("Training completed!")

except ImportError:
    print("OCR modules not found. Please run the OCR training code separately with the correct path.")
except Exception as e:
    print(f"Error during training: {e}")
    print("Please check the dataset structure and try again.")
# Complete Improved Arabic OCR Training Script
# This version fixes all the fundamental issues with CTC loss

import os
import json
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import random

class ArabicOCRDataset:
    def __init__(self, base_path=None):
        if base_path is None:
            self.base_path = self.find_dataset_path()
        else:
            self.base_path = base_path

        self.image_height = 64
        self.image_width = 256
        self.max_text_length = 12  # Much shorter for stability
        self.characters = set()
        self.char_to_idx = {}
        self.idx_to_char = {}
        self.data = []

    def find_dataset_path(self):
        """Auto-detect the correct dataset path"""
        possible_paths = [
            '/root/.cache/kagglehub/datasets/humansintheloop/arabic-documents-ocr-dataset/versions/1',
            '/kaggle/input/arabic-documents-ocr-dataset',
            './arabic-documents-ocr-dataset',
            '../input/arabic-documents-ocr-dataset'
        ]

        for path in possible_paths:
            if os.path.exists(path):
                print(f"Found dataset at: {path}")
                return path

        raise FileNotFoundError("Could not find the Arabic OCR dataset.")

    def find_documents_path(self, base_path):
        """Find the Documents folder within the dataset"""
        possible_docs_paths = [
            os.path.join(base_path, 'Documents', 'Documents'),
            os.path.join(base_path, 'Documents'),
        ]

        for docs_path in possible_docs_paths:
            if os.path.exists(docs_path):
                try:
                    contents = os.listdir(docs_path)
                    doc_types = [item for item in contents if os.path.isdir(os.path.join(docs_path, item))]
                    if len(doc_types) > 5:
                        print(f"Found documents at: {docs_path}")
                        return docs_path
                except:
                    continue
        return None

    def load_dataset(self):
        """Load and parse the dataset from JSON annotations"""
        print("Loading dataset...")
        print(f"Base path: {self.base_path}")

        docs_path = self.find_documents_path(self.base_path)
        if docs_path is None:
            print("Could not find Documents directory!")
            return

        print(f"Using documents path: {docs_path}")

        # Focus on document types with shorter, cleaner text
        priority_types = ['Label', 'Receipt', 'Business card', 'Invoice']

        for doc_type in os.listdir(docs_path):
            doc_type_path = os.path.join(docs_path, doc_type)
            if not os.path.isdir(doc_type_path):
                continue

            ann_path = os.path.join(doc_type_path, 'ann')
            img_path = os.path.join(doc_type_path, 'img')

            if not os.path.exists(ann_path) or not os.path.exists(img_path):
                continue

            print(f"Processing {doc_type} documents...")

            # Process fewer files but get better quality
            limit = 200 if doc_type in priority_types else 100
            ann_files = [f for f in os.listdir(ann_path) if f.endswith('.json')][:limit]

            files_with_text = 0
            for ann_file in ann_files:
                ann_file_path = os.path.join(ann_path, ann_file)

                # Find corresponding image
                base_name = ann_file.replace('.json', '')
                img_file_path = None

                for ext in ['.jpg', '.png', '.jpeg', '.tiff', '.bmp']:
                    potential_path = os.path.join(img_path, base_name + ext)
                    if os.path.exists(potential_path):
                        img_file_path = potential_path
                        break

                if img_file_path is None:
                    continue

                try:
                    with open(ann_file_path, 'r', encoding='utf-8') as f:
                        annotation = json.load(f)

                    text = self.extract_text_from_annotation(annotation)
                    # Much stricter filtering for clean, short text
                    if text and len(text.strip()) > 0:
                        text_clean = text.strip()
                        # Only accept very short, clean text
                        if len(text_clean) <= 12 and len(text_clean) >= 2:
                            files_with_text += 1
                            self.data.append({
                                'image_path': img_file_path,
                                'text': text_clean
                            })
                            self.characters.update(text_clean)

                            if len(self.data) <= 5:
                                print(f"  âœ“ {ann_file}: '{text_clean}'")

                except Exception as e:
                    continue

            print(f"  Found text in {files_with_text}/{len(ann_files)} files")

        print(f"Loaded {len(self.data)} samples")
        if len(self.data) > 0:
            self.build_vocabulary()

    def extract_text_from_annotation(self, annotation):
        """Extract text from the specific annotation format"""
        text = ""
        if isinstance(annotation, dict) and 'objects' in annotation:
            for obj in annotation['objects']:
                if isinstance(obj, dict) and 'tags' in obj:
                    for tag in obj['tags']:
                        if isinstance(tag, dict) and tag.get('name') == 'Transcription':
                            transcription_text = tag.get('value', '')
                            if transcription_text:
                                text += transcription_text + " "
        return text.strip()

    def build_vocabulary(self):
        """Build character vocabulary optimized for CTC"""
        # Keep vocabulary smaller and cleaner
        common_chars = sorted(list(self.characters))

        # Build vocabulary: [blank] + [pad] + [characters]
        # IMPORTANT: CTC blank must be at index 0 for TensorFlow
        chars = [''] + ['<PAD>'] + common_chars  # Empty string as blank token

        self.char_to_idx = {char: idx for idx, char in enumerate(chars)}
        self.idx_to_char = {idx: char for idx, char in enumerate(chars)}
        self.vocab_size = len(chars)
        self.blank_token_idx = 0  # Blank is at index 0

        print(f"Vocabulary size: {self.vocab_size}")
        print(f"Blank token index: {self.blank_token_idx}")
        print(f"Sample characters: {common_chars[:20]}")

    def preprocess_image(self, image_path):
        """Preprocess image for OCR"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                return None

            if len(image.shape) == 3:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

            # Resize and normalize
            image = cv2.resize(image, (self.image_width, self.image_height))
            image = image.astype(np.float32) / 255.0

            # Improve contrast
            image = (image - 0.5) * 2.0  # Normalize to [-1, 1]

            return np.expand_dims(image, axis=-1)
        except:
            return None

    def encode_text(self, text):
        """Encode text to sequence of indices"""
        encoded = []
        for char in text:
            if char in self.char_to_idx:
                encoded.append(self.char_to_idx[char])
        return encoded

    def decode_text(self, indices):
        """Decode sequence of indices to text"""
        text = ""
        for idx in indices:
            if idx < len(self.idx_to_char) and idx > 0:  # Skip blank (0) and padding
                char = self.idx_to_char[idx]
                if char != '<PAD>':
                    text += char
        return text

    def create_dataset(self, batch_size=16):
        """Create TensorFlow dataset with proper CTC format"""
        # Split data
        train_data, val_data = train_test_split(self.data, test_size=0.2, random_state=42)
        print(f"Train samples: {len(train_data)}, Val samples: {len(val_data)}")

        def generator(data_list):
            for item in data_list:
                image = self.preprocess_image(item['image_path'])
                if image is not None:
                    text_encoded = self.encode_text(item['text'])
                    if len(text_encoded) > 0 and len(text_encoded) <= self.max_text_length:
                        yield image, text_encoded

        def create_tf_dataset(data_list):
            dataset = tf.data.Dataset.from_generator(
                lambda: generator(data_list),
                output_signature=(
                    tf.TensorSpec(shape=(self.image_height, self.image_width, 1), dtype=tf.float32),
                    tf.TensorSpec(shape=(None,), dtype=tf.int32)
                )
            )

            # Pad and batch
            dataset = dataset.padded_batch(
                batch_size,
                padded_shapes=(
                    (self.image_height, self.image_width, 1),
                    (self.max_text_length,)
                ),
                padding_values=(0.0, 1)  # Use 1 for padding (PAD token index)
            )

            return dataset.prefetch(tf.data.AUTOTUNE)

        train_dataset = create_tf_dataset(train_data)
        val_dataset = create_tf_dataset(val_data)

        return train_dataset, val_dataset

class SimpleCTCModel:
    def __init__(self, vocab_size, image_height=64, image_width=256):
        self.vocab_size = vocab_size
        self.image_height = image_height
        self.image_width = image_width

    def build_model(self):
        """Build a simpler, more stable CNN-RNN model"""
        # Input
        input_img = layers.Input(shape=(self.image_height, self.image_width, 1), name='image')

        # Simpler CNN backbone
        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
        x = layers.MaxPooling2D((2, 2))(x)  # 32x128

        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        x = layers.MaxPooling2D((2, 2))(x)  # 16x64

        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)
        x = layers.MaxPooling2D((2, 1))(x)  # 8x64

        # Reshape for RNN: (batch, height, width, channels) -> (batch, width, features)
        new_shape = (x.shape[2], x.shape[1] * x.shape[3])  # (64, 8*128)
        x = layers.Permute((2, 1, 3))(x)  # (batch, width, height, channels)
        x = layers.Reshape(new_shape)(x)   # (batch, 64, 1024)

        # Dense layer to reduce features
        x = layers.Dense(128, activation='relu')(x)
        x = layers.Dropout(0.2)(x)

        # Bidirectional LSTM
        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)
        x = layers.Dropout(0.2)(x)

        # Output layer - NO softmax for CTC!
        predictions = layers.Dense(self.vocab_size, name='predictions')(x)

        model = Model(inputs=input_img, outputs=predictions)
        return model

def ctc_loss_function(y_true, y_pred):
    """Proper CTC loss function with correct data types"""
    # Get shapes
    batch_size = tf.shape(y_true)[0]
    max_time = tf.shape(y_pred)[1]

    # Calculate label lengths (non-padding tokens)
    label_length = tf.reduce_sum(tf.cast(y_true > 1, tf.int32), axis=1)  # Skip blank(0) and pad(1)

    # Input length (all timesteps available)
    input_length = tf.fill([batch_size], max_time)

    # Convert dense labels to sparse with correct data types
    indices = tf.where(y_true > 1)  # Only real characters, not blank or pad
    values = tf.gather_nd(y_true, indices)

    # Ensure correct data types for sparse tensor
    sparse_labels = tf.SparseTensor(
        indices=tf.cast(indices, tf.int64),
        values=tf.cast(values, tf.int32),  # Explicitly cast to int32
        dense_shape=tf.cast(tf.shape(y_true), tf.int64)
    )

    # CTC loss
    loss = tf.nn.ctc_loss(
        labels=sparse_labels,
        logits=y_pred,
        label_length=None,  # Not needed with sparse
        logit_length=input_length,
        logits_time_major=False,
        blank_index=0  # Blank token is at index 0
    )

    return tf.reduce_mean(loss)

class OCRTrainer:
    def __init__(self, dataset, model):
        self.dataset = dataset
        self.model = model

    def train(self, train_dataset, val_dataset, epochs=20):
        """Train the OCR model with proper settings"""

        # Test first batch
        print("\nðŸ” Analyzing first batch...")
        for images, labels in train_dataset.take(1):
            print(f"Image batch shape: {images.shape}")
            print(f"Label batch shape: {labels.shape}")
            print(f"Sample labels: {labels[0].numpy()}")
            print(f"Label range: {tf.reduce_min(labels)} to {tf.reduce_max(labels)}")

            # Test prediction shape
            pred = self.model(images, training=False)
            print(f"Prediction shape: {pred.shape}")

            # Test loss
            test_loss = ctc_loss_function(labels, pred)
            print(f"Initial CTC loss: {test_loss:.2f}")
            break

        # Compile model
        self.model.compile(
            optimizer=Adam(learning_rate=0.001, clipnorm=5.0),
            loss=ctc_loss_function
        )

        # Callbacks
        callbacks = [
            ModelCheckpoint(
                'best_ocr_model.keras',
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            ),
            EarlyStopping(
                monitor='val_loss',
                patience=8,
                verbose=1,
                restore_best_weights=True
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=4,
                verbose=1,
                min_lr=1e-6
            )
        ]

        # Train
        history = self.model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=epochs,
            callbacks=callbacks,
            verbose=1
        )

        return history

    def predict_text(self, image_path):
        """Predict text from image"""
        image = self.dataset.preprocess_image(image_path)
        if image is None:
            return "Error: Could not process image"

        # Predict
        image_batch = np.expand_dims(image, axis=0)
        predictions = self.model.predict(image_batch, verbose=0)

        # Decode with CTC
        input_length = np.array([predictions.shape[1]])
        decoded = tf.keras.backend.ctc_decode(
            predictions, input_length, greedy=True
        )[0][0]

        # Convert to text
        if len(decoded) > 0:
            decoded_indices = decoded[0].numpy()
            return self.dataset.decode_text(decoded_indices)
        return ""

def main():
    """Main training pipeline"""
    print("ðŸš€ Starting Improved Arabic OCR Training Pipeline...")

    # Initialize dataset
    dataset_path = '/root/.cache/kagglehub/datasets/humansintheloop/arabic-documents-ocr-dataset/versions/1'
    dataset = ArabicOCRDataset(dataset_path)
    dataset.load_dataset()

    if len(dataset.data) == 0:
        print("âŒ No data found! Please check your dataset.")
        return

    print(f"\nðŸ“Š Dataset Summary:")
    print(f"Total samples: {len(dataset.data)}")
    print(f"Vocabulary size: {dataset.vocab_size}")
    print(f"Max text length: {dataset.max_text_length}")
    print(f"Sample text: '{dataset.data[0]['text']}'")

    # Create datasets
    train_dataset, val_dataset = dataset.create_dataset(batch_size=16)

    # Build model
    model_builder = SimpleCTCModel(vocab_size=dataset.vocab_size)
    model = model_builder.build_model()

    print(f"\nðŸ—ï¸ Model Summary:")
    model.summary()

    # Train
    trainer = OCRTrainer(dataset, model)
    print(f"\nðŸŽ¯ Starting training...")
    history = trainer.train(train_dataset, val_dataset, epochs=20)

    # Plot results
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.yscale('log')  # Log scale for better visualization

    plt.tight_layout()
    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
    plt.show()

    # Test prediction
    print(f"\nðŸ§ª Testing prediction...")
    if len(dataset.data) > 0:
        sample_image = dataset.data[0]['image_path']
        predicted_text = trainer.predict_text(sample_image)
        actual_text = dataset.data[0]['text']

        print(f"Actual: '{actual_text}'")
        print(f"Predicted: '{predicted_text}'")

    print(f"\nâœ… Training completed! Model saved as 'best_ocr_model.keras'")

if __name__ == "__main__":
    main()
