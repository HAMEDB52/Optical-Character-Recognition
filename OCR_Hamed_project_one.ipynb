{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# load datset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def extract_and_verify_dataset(tar_path=\"/content/IIIT5K-Word_V3.0.tar\"):\n",
        "    \"\"\"Extract the IIIT5K dataset and verify its structure\"\"\"\n",
        "    # Create output directory\n",
        "    output_dir = \"iiit5k_dataset\"\n",
        "    if os.path.exists(output_dir):\n",
        "        print(f\"Removing existing directory: {output_dir}\")\n",
        "        shutil.rmtree(output_dir)\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "    # Extract the tar file\n",
        "    print(f\"Extracting {tar_path} to {output_dir}...\")\n",
        "    try:\n",
        "        with tarfile.open(tar_path, \"r\") as tar:\n",
        "            tar.extractall(output_dir)\n",
        "        print(\"Extraction completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting file: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Function to print directory structure\n",
        "    def print_directory_structure(path, prefix=\"\"):\n",
        "        if not os.path.exists(path):\n",
        "            return\n",
        "\n",
        "        items = os.listdir(path)\n",
        "        items.sort()\n",
        "\n",
        "        for i, item in enumerate(items):\n",
        "            is_last = i == len(items) - 1\n",
        "            item_path = os.path.join(path, item)\n",
        "\n",
        "            # Print item\n",
        "            print(f\"{prefix}{'‚îî‚îÄ‚îÄ ' if is_last else '‚îú‚îÄ‚îÄ '}{item}\")\n",
        "\n",
        "            # If it's a directory, recurse\n",
        "            if os.path.isdir(item_path):\n",
        "                new_prefix = prefix + (\"    \" if is_last else \"‚îÇ   \")\n",
        "                print_directory_structure(item_path, new_prefix)\n",
        "\n",
        "    # Print the directory structure\n",
        "    print(\"\\nDataset directory structure:\")\n",
        "    print(\"=\"*50)\n",
        "    print_directory_structure(output_dir)\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Verify critical components\n",
        "    print(\"\\nVerifying critical dataset components:\")\n",
        "\n",
        "    # Look for train/test directories\n",
        "    train_dir = None\n",
        "    test_dir = None\n",
        "    gt_file = None\n",
        "\n",
        "    # Search for train directory (could be named various ways)\n",
        "    for root, dirs, files in os.walk(output_dir):\n",
        "        for dir_name in dirs:\n",
        "            if \"train\" in dir_name.lower() and not any(x in dir_name.lower() for x in [\"test\", \"val\"]):\n",
        "                train_dir = os.path.join(root, dir_name)\n",
        "                print(f\"‚úì Found train directory: {train_dir}\")\n",
        "\n",
        "        for dir_name in dirs:\n",
        "            if \"test\" in dir_name.lower() and not any(x in dir_name.lower() for x in [\"train\", \"val\"]):\n",
        "                test_dir = os.path.join(root, dir_name)\n",
        "                print(f\"‚úì Found test directory: {test_dir}\")\n",
        "\n",
        "        for file in files:\n",
        "            if \"gt\" in file.lower() and file.endswith((\".mat\", \".txt\")):\n",
        "                gt_file = os.path.join(root, file)\n",
        "                print(f\"‚úì Found ground truth file: {gt_file}\")\n",
        "\n",
        "    # Check for image files\n",
        "    train_images = 0\n",
        "    test_images = 0\n",
        "\n",
        "    if train_dir and os.path.exists(train_dir):\n",
        "        train_images = len([f for f in os.listdir(train_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "        print(f\"‚úì Found {train_images} training images\")\n",
        "\n",
        "    if test_dir and os.path.exists(test_dir):\n",
        "        test_images = len([f for f in os.listdir(test_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "        print(f\"‚úì Found {test_images} test images\")\n",
        "\n",
        "    # Create a verification report\n",
        "    report = {\n",
        "        \"base_dir\": output_dir,\n",
        "        \"train_dir\": train_dir,\n",
        "        \"test_dir\": test_dir,\n",
        "        \"gt_file\": gt_file,\n",
        "        \"train_count\": train_images,\n",
        "        \"test_count\": test_images\n",
        "    }\n",
        "\n",
        "    print(\"\\nDataset verification report:\")\n",
        "    print(\"=\"*50)\n",
        "    for key, value in report.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    return report\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*50)\n",
        "    print(\"IIIT5K DATASET EXTRACTION AND VERIFICATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Check if the tar file exists\n",
        "    tar_file = \"IIIT5K-Word_V3.0.tar\"\n",
        "    if not os.path.exists(tar_file):\n",
        "        print(f\"\\nERROR: '{tar_file}' not found in current directory!\")\n",
        "        print(\"Please place the IIIT5K-Word_V3.0.tar file in this directory and run again.\")\n",
        "        print(\"Current directory contents:\")\n",
        "        for item in os.listdir(\".\"):\n",
        "            print(f\"- {item}\")\n",
        "        exit(1)\n",
        "\n",
        "    # Extract and verify\n",
        "    report = extract_and_verify_dataset(tar_file)\n",
        "\n",
        "    if report:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"NEXT STEPS FOR OCR TRAINING CODE\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"To use this dataset in the OCR training code, update the dataset paths as follows:\")\n",
        "\n",
        "        # Generate code snippet for dataset configuration\n",
        "        print(\"\\nIn your OCR training code, use these paths:\")\n",
        "        print(\"```python\")\n",
        "        print(f\"# Dataset root directory\")\n",
        "        print(f\"DATASET_ROOT = r'{os.path.abspath(report['base_dir'])}'\")\n",
        "\n",
        "        if report['train_dir']:\n",
        "            print(f\"TRAIN_DIR = r'{os.path.abspath(report['train_dir'])}'\")\n",
        "        if report['test_dir']:\n",
        "            print(f\"TEST_DIR = r'{os.path.abspath(report['test_dir'])}'\")\n",
        "        if report['gt_file']:\n",
        "            print(f\"GT_FILE = r'{os.path.abspath(report['gt_file'])}'\")\n",
        "\n",
        "        print(\"```\")\n",
        "        print(\"\\nThis information will help configure the dataset loader correctly.\")\n",
        "        print(\"Based on common IIIT5K structure, here's how to implement the dataset loader:\")\n",
        "\n",
        "        # Provide specific dataset loader implementation\n",
        "        print(\"\\n```python\")\n",
        "        print(\"class IIIT5KDataset(Dataset):\")\n",
        "        print(\"    def __init__(self, root_dir, split='train', img_height=32, img_width=100):\")\n",
        "        print(\"        self.root_dir = root_dir\")\n",
        "        print(\"        self.img_height = img_height\")\n",
        "        print(\"        self.img_width = img_width\")\n",
        "        print(\"        \")\n",
        "        print(\"        # Load ground truth\")\n",
        "        print(\"        gt_path = os.path.join(root_dir, 'gt.mat')  # Or the actual path from above\")\n",
        "        print(\"        # In practice, you'd use scipy.io.loadmat for .mat files\")\n",
        "        print(\"        # For demonstration, we'll assume a text-based GT\")\n",
        "        print(\"        \")\n",
        "        print(\"        self.samples = []\")\n",
        "        print(\"        data_dir = os.path.join(root_dir, 'train' if split == 'train' else 'test')\")\n",
        "        print(\"        gt_file = os.path.join(root_dir, f'gt_{split}.txt')\")\n",
        "        print(\"        \")\n",
        "        print(\"        # Parse ground truth file\")\n",
        "        print(\"        with open(gt_file, 'r') as f:\")\n",
        "        print(\"            for line in f:\")\n",
        "        print(\"                parts = line.strip().split(' ')\")\n",
        "        print(\"                img_name = parts[0]\")\n",
        "        print(\"                text = ' '.join(parts[1:])\")\n",
        "        print(\"                self.samples.append((os.path.join(data_dir, img_name), text))\")\n",
        "        print(\"        \")\n",
        "        print(\"        # Character set for English OCR\")\n",
        "        print(\"        self.char_set = string.ascii_uppercase + string.digits + ' '\")\n",
        "        print(\"        self.char_to_idx = {char: idx for idx, char in enumerate(self.char_set)}\")\n",
        "        print(\"        self.idx_to_char = {idx: char for idx, char in enumerate(self.char_set)}\")\n",
        "        print(\"        \")\n",
        "        print(\"        # Transformations\")\n",
        "        print(\"        self.transform = T.Compose([\")\n",
        "        print(\"            T.Resize((img_height, img_width)),\")\n",
        "        print(\"            T.Grayscale(),\")\n",
        "        print(\"            T.ToTensor(),\")\n",
        "        print(\"        ])\")\n",
        "        print(\"    \")\n",
        "        print(\"    def __len__(self):\")\n",
        "        print(\"        return len(self.samples)\")\n",
        "        print(\"    \")\n",
        "        print(\"    def __getitem__(self, idx):\")\n",
        "        print(\"        img_path, text = self.samples[idx]\")\n",
        "        print(\"        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\")\n",
        "        print(\"        \")\n",
        "        print(\"        # Convert text to indices\")\n",
        "        print(\"        text_indices = [self.char_to_idx.get(c.upper(), 0) for c in text if c.upper() in self.char_set]\")\n",
        "        print(\"        \")\n",
        "        print(\"        # Convert to tensor\")\n",
        "        print(\"        img_tensor = self.transform(Image.fromarray(img))\")\n",
        "        print(\"        text_tensor = torch.tensor(text_indices, dtype=torch.long)\")\n",
        "        print(\"        \")\n",
        "        print(\"        return img_tensor, text_tensor, len(text_indices)\")\n",
        "        print(\"```\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_B4jejSrgui",
        "outputId": "95a8b512-5557-4ff9-a2af-6b57b31de33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "IIIT5K DATASET EXTRACTION AND VERIFICATION\n",
            "==================================================\n",
            "\n",
            "ERROR: 'IIIT5K-Word_V3.0.tar' not found in current directory!\n",
            "Please place the IIIT5K-Word_V3.0.tar file in this directory and run again.\n",
            "Current directory contents:\n",
            "- .config\n",
            "- sample_data\n",
            "Extracting IIIT5K-Word_V3.0.tar to iiit5k_dataset...\n",
            "Error extracting file: [Errno 2] No such file or directory: 'IIIT5K-Word_V3.0.tar'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# deep_ocr_custom_split.py\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import scipy.io\n",
        "import random\n",
        "\n",
        "# ======================\n",
        "# 1. MODIFIED DATASET CLASS WITH CUSTOM SPLIT\n",
        "# ======================\n",
        "class CustomSplitIIIT5KDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', img_height=32, img_width=128,\n",
        "                 train_samples=4500, test_samples=500, random_seed=42):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.iiit5k_dir = os.path.join(root_dir, \"IIIT5K\")\n",
        "\n",
        "        # Set random seed for reproducible splits\n",
        "        random.seed(random_seed)\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "        # ‚úÖ STEP 1: Define vocabulary FIRST (critical fix!)\n",
        "        self.chars = string.ascii_letters + string.digits + \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
        "        self.char_to_idx = {char: idx + 1 for idx, char in enumerate(self.chars)}\n",
        "        self.idx_to_char = {idx + 1: char for idx, char in enumerate(self.chars)}\n",
        "        self.idx_to_char[0] = '<BLANK>'\n",
        "        self.num_classes = len(self.chars) + 1  # 95 + 1\n",
        "\n",
        "        print(f\"Creating custom dataset split with {train_samples} train, {test_samples} test samples...\")\n",
        "\n",
        "        # Load all data from both original train and test sets\n",
        "        all_samples = []\n",
        "\n",
        "        # Load original train data\n",
        "        train_mat_file = os.path.join(self.iiit5k_dir, \"traindata.mat\")\n",
        "        train_data_dir = os.path.join(self.iiit5k_dir, \"train\")\n",
        "        if os.path.exists(train_mat_file):\n",
        "            all_samples.extend(self._load_mat_data(train_mat_file, train_data_dir, \"train\"))\n",
        "\n",
        "        # Load original test data\n",
        "        test_mat_file = os.path.join(self.iiit5k_dir, \"testdata.mat\")\n",
        "        test_data_dir = os.path.join(self.iiit5k_dir, \"test\")\n",
        "        if os.path.exists(test_mat_file):\n",
        "            all_samples.extend(self._load_mat_data(test_mat_file, test_data_dir, \"test\"))\n",
        "\n",
        "        print(f\"Total samples loaded: {len(all_samples)}\")\n",
        "\n",
        "        # Shuffle all samples for random split\n",
        "        random.shuffle(all_samples)\n",
        "\n",
        "        # Create custom train/test split\n",
        "        total_needed = train_samples + test_samples\n",
        "        if len(all_samples) < total_needed:\n",
        "            print(f\"‚ö†Ô∏è Warning: Only {len(all_samples)} samples available, but {total_needed} requested\")\n",
        "            train_samples = min(train_samples, len(all_samples) - test_samples)\n",
        "            test_samples = len(all_samples) - train_samples\n",
        "\n",
        "        if split == 'train':\n",
        "            self.samples = all_samples[:train_samples]\n",
        "            print(f\"‚úÖ Created training set with {len(self.samples)} samples\")\n",
        "        else:  # test\n",
        "            self.samples = all_samples[train_samples:train_samples + test_samples]\n",
        "            print(f\"‚úÖ Created test set with {len(self.samples)} samples\")\n",
        "\n",
        "        if self.samples:\n",
        "            sample_texts = [t for _, t in self.samples[:10]]\n",
        "            lengths = [len(t) for _, t in self.samples]\n",
        "            print(f\"Sample texts: {sample_texts}\")\n",
        "            print(f\"Text lengths: min={min(lengths)}, max={max(lengths)}, avg={np.mean(lengths):.1f}\")\n",
        "            print(f\"Vocabulary size: {self.num_classes}\")\n",
        "\n",
        "        # Image transform\n",
        "        self.transform = T.Compose([\n",
        "            T.ToPILImage(),\n",
        "            T.Resize((img_height, img_width), antialias=True),\n",
        "            T.Grayscale(),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize((0.5,), (0.5,))  # [-1, 1]\n",
        "        ])\n",
        "\n",
        "    def _load_mat_data(self, mat_file, data_dir, original_split):\n",
        "        \"\"\"Load data from .mat file and return list of (img_path, text) tuples\"\"\"\n",
        "        samples = []\n",
        "\n",
        "        try:\n",
        "            mat_data = scipy.io.loadmat(mat_file)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load {mat_file}: {e}\")\n",
        "            return samples\n",
        "\n",
        "        data_key = f\"{original_split}data\"\n",
        "        if data_key not in mat_data:\n",
        "            available = [k for k in mat_data.keys() if not k.startswith('__')]\n",
        "            print(f\"‚ùå Key '{data_key}' not found in {mat_file}. Available: {available}\")\n",
        "            return samples\n",
        "\n",
        "        data = mat_data[data_key]\n",
        "        N = data.shape[1] if data.shape[0] == 1 else data.shape[0]\n",
        "        print(f\"Processing {N} samples from {original_split} data...\")\n",
        "\n",
        "        for i in range(N):\n",
        "            try:\n",
        "                sample = data[0, i] if data.shape[0] == 1 else data[i, 0]\n",
        "\n",
        "                # Extract image path\n",
        "                img_name = self._safe_extract_string(sample[0])\n",
        "                if not img_name:\n",
        "                    img_filename = f\"sample_{i}.png\"\n",
        "                else:\n",
        "                    img_filename = os.path.basename(img_name.strip())\n",
        "                img_path = os.path.join(data_dir, img_filename)\n",
        "\n",
        "                # Extract text\n",
        "                text = self._safe_extract_string(sample[3])\n",
        "                text = self._clean_text(text)\n",
        "\n",
        "                if os.path.exists(img_path) and 1 <= len(text) <= 23:\n",
        "                    samples.append((img_path, text))\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def _safe_extract_string(self, data):\n",
        "        \"\"\"Robustly extract string from .mat field (handles str, numeric array, object array)\"\"\"\n",
        "        try:\n",
        "            if isinstance(data, str):\n",
        "                return data.strip()\n",
        "\n",
        "            if isinstance(data, np.ndarray):\n",
        "                # Case 1: Numeric array (ASCII codes)\n",
        "                if np.issubdtype(data.dtype, np.number):\n",
        "                    chars = []\n",
        "                    for c in data.flatten():\n",
        "                        try:\n",
        "                            c_int = int(c)\n",
        "                            if 32 <= c_int <= 126:  # Printable ASCII\n",
        "                                chars.append(chr(c_int))\n",
        "                        except (ValueError, TypeError):\n",
        "                            continue\n",
        "                    return ''.join(chars).strip()\n",
        "\n",
        "                # Case 2: String array (U/S dtype)\n",
        "                elif data.dtype.kind in ['U', 'S']:\n",
        "                    return str(data.flatten()[0]).strip() if data.size > 0 else \"\"\n",
        "\n",
        "                # Case 3: Object array (common in scipy.io.loadmat)\n",
        "                elif data.dtype == np.object_:\n",
        "                    item = data.item() if data.size == 1 else data[0]\n",
        "                    if isinstance(item, str):\n",
        "                        return item.strip()\n",
        "                    return self._safe_extract_string(item)\n",
        "\n",
        "            return str(data).strip()\n",
        "        except Exception as e:\n",
        "            return \"\"\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Keep only valid characters in full ASCII set\"\"\"\n",
        "        return ''.join(c for c in text if c in self.chars).strip()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, text = self.samples[idx]\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            print(f\"‚ö†Ô∏è Warning: Failed to load image {img_path}, using blank\")\n",
        "            img = np.ones((self.img_height, self.img_width), dtype=np.uint8) * 255\n",
        "\n",
        "        text_indices = [self.char_to_idx[char] for char in text if char in self.char_to_idx]\n",
        "        img_tensor = self.transform(img)\n",
        "        return img_tensor, text_indices, text\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 2. DEEP CRNN MODEL (UNCHANGED)\n",
        "# ======================\n",
        "class DeepCRNN(nn.Module):\n",
        "    def __init__(self, img_height=32, num_classes=96, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d((2, 1), (2, 1)),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d((2, 1), (2, 1)),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d((2, 1), (2, 1)),\n",
        "        )\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=512,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=2,\n",
        "            bidirectional=True,\n",
        "            batch_first=False,\n",
        "            dropout=0.3\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_size * 2, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv = self.cnn(x)  # [B, 512, 1, W]\n",
        "        b, c, h, w = conv.size()\n",
        "        conv = conv.view(b, c * h, w).permute(2, 0, 1)  # [W, B, 512]\n",
        "        rnn_out, _ = self.rnn(conv)\n",
        "        rnn_out = self.dropout(rnn_out)\n",
        "        output = self.classifier(rnn_out)\n",
        "        return F.log_softmax(output, dim=2)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 3. CTC UTILITIES (UNCHANGED)\n",
        "# ======================\n",
        "def ctc_collate_fn(batch):\n",
        "    images, text_indices_list, raw_texts = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    targets = []\n",
        "    target_lengths = []\n",
        "    for text_indices in text_indices_list:\n",
        "        if len(text_indices) > 0:\n",
        "            targets.extend(text_indices)\n",
        "            target_lengths.append(len(text_indices))\n",
        "        else:\n",
        "            targets.append(0)\n",
        "            target_lengths.append(1)\n",
        "    return images, torch.tensor(targets), torch.tensor(target_lengths), raw_texts\n",
        "\n",
        "def ctc_decode(log_probs, idx_to_char, blank_idx=0):\n",
        "    \"\"\"Greedy CTC decoding\"\"\"\n",
        "    seq_len, batch_size, num_classes = log_probs.shape\n",
        "    predictions = []\n",
        "    for b in range(batch_size):\n",
        "        pred_indices = log_probs[:, b, :].argmax(dim=1).cpu().numpy()\n",
        "        decoded = []\n",
        "        prev_idx = blank_idx\n",
        "        for idx in pred_indices:\n",
        "            if idx != blank_idx and idx != prev_idx:\n",
        "                if idx in idx_to_char and idx_to_char[idx] != '<BLANK>':\n",
        "                    decoded.append(idx_to_char[idx])\n",
        "            prev_idx = idx\n",
        "        predictions.append(''.join(decoded))\n",
        "    return predictions\n",
        "\n",
        "def calculate_metrics(pred_texts, true_texts):\n",
        "    \"\"\"Calculate word and character accuracy\"\"\"\n",
        "    if not pred_texts or not true_texts:\n",
        "        return 0.0, 0.0\n",
        "    word_acc = sum(p == t for p, t in zip(pred_texts, true_texts)) / len(true_texts)\n",
        "    total_chars = sum(max(len(p), len(t)) for p, t in zip(pred_texts, true_texts))\n",
        "    correct_chars = sum(p[i] == t[i] for p, t in zip(pred_texts, true_texts) for i in range(min(len(p), len(t))))\n",
        "    char_acc = correct_chars / total_chars if total_chars > 0 else 0.0\n",
        "    return word_acc, char_acc\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 4. TRAINING LOOP WITH CUSTOM SPLIT\n",
        "# ======================\n",
        "def train_deep_ocr_custom_split():\n",
        "    print(\"üöÄ DEEP OCR TRAINING WITH CUSTOM SPLIT (4500 train, 500 test)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # CONFIGURATION - Updated based on your dataset structure\n",
        "    dataset_root = \"/content/iiit5k_dataset\"  # Your extracted dataset path\n",
        "\n",
        "    # Verify dataset exists\n",
        "    print(f\"üîç Checking dataset root: {dataset_root}\")\n",
        "    print(f\"üìÅ Path exists: {os.path.exists(dataset_root)}\")\n",
        "\n",
        "    if not os.path.exists(dataset_root):\n",
        "        # Try alternative paths\n",
        "        alternative_paths = [\"./iiit5k_dataset\", \"/content/iiit5k_dataset\"]\n",
        "        for alt_path in alternative_paths:\n",
        "            if os.path.exists(alt_path):\n",
        "                dataset_root = alt_path\n",
        "                print(f\"‚úÖ Found dataset at alternative path: {dataset_root}\")\n",
        "                break\n",
        "        else:\n",
        "            print(f\"‚ùå Dataset root not found at any location\")\n",
        "            print(\"üí° Checked paths:\")\n",
        "            for path in [\"/content/iiit5k_dataset\"] + alternative_paths:\n",
        "                print(f\"   {path}: {'‚úÖ' if os.path.exists(path) else '‚ùå'}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"‚úÖ Dataset root found: {dataset_root}\")\n",
        "\n",
        "    # Verify IIIT5K subdirectory structure\n",
        "    iiit5k_path = os.path.join(dataset_root, \"IIIT5K\")\n",
        "    print(f\"üîç Checking IIIT5K path: {iiit5k_path}\")\n",
        "    print(f\"üìÅ IIIT5K exists: {os.path.exists(iiit5k_path)}\")\n",
        "\n",
        "    if os.path.exists(iiit5k_path):\n",
        "        contents = os.listdir(iiit5k_path)\n",
        "        print(f\"üìä IIIT5K contents: {contents}\")\n",
        "\n",
        "        # Check required files\n",
        "        required_items = ['train', 'test', 'traindata.mat', 'testdata.mat']\n",
        "        missing_items = [item for item in required_items if item not in contents]\n",
        "\n",
        "        if missing_items:\n",
        "            print(f\"‚ùå Missing required items: {missing_items}\")\n",
        "            return None\n",
        "        else:\n",
        "            print(\"‚úÖ All required dataset components found!\")\n",
        "    else:\n",
        "        print(f\"‚ùå IIIT5K subdirectory not found at: {iiit5k_path}\")\n",
        "        return None\n",
        "\n",
        "    batch_size = 32\n",
        "    num_epochs = 250\n",
        "    learning_rate = 1e-3\n",
        "    train_samples = 4500\n",
        "    test_samples = 500\n",
        "\n",
        "    print(f\"Custom split: {train_samples} train, {test_samples} test\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Epochs: {num_epochs}\")\n",
        "    print(f\"Learning rate: {learning_rate}\")\n",
        "\n",
        "    # Load datasets with custom split\n",
        "    print(\"\\nCreating custom split datasets...\")\n",
        "    try:\n",
        "        train_dataset = CustomSplitIIIT5KDataset(\n",
        "            dataset_root, 'train', 32, 128, train_samples, test_samples\n",
        "        )\n",
        "        test_dataset = CustomSplitIIIT5KDataset(\n",
        "            dataset_root, 'test', 32, 128, train_samples, test_samples\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Dataset creation failed: {e}\")\n",
        "        return None\n",
        "\n",
        "    if len(train_dataset) == 0 or len(test_dataset) == 0:\n",
        "        print(\"‚ùå No valid data loaded.\")\n",
        "        return None\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                              collate_fn=ctc_collate_fn, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                             collate_fn=ctc_collate_fn, num_workers=0)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\nDevice: {device}\")\n",
        "\n",
        "    model = DeepCRNN(32, train_dataset.num_classes).to(device)\n",
        "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"Training batches: {len(train_loader)}\")\n",
        "    print(f\"Test batches: {len(test_loader)}\")\n",
        "\n",
        "    best_word_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_word_acc = 0.0\n",
        "        num_train_batches = 0\n",
        "\n",
        "        print(f\"\\nüìà Epoch {epoch+1}/{num_epochs}\")\n",
        "        pbar = tqdm(train_loader, desc=\"Training\")\n",
        "        for images, targets, target_lengths, raw_texts in pbar:\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            target_lengths = target_lengths.to(device)\n",
        "\n",
        "            log_probs = model(images)\n",
        "            input_lengths = torch.full((images.size(0),), log_probs.size(0), device=device, dtype=torch.long)\n",
        "            loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Log every 20 batches (adjusted for potentially more batches)\n",
        "            if num_train_batches % 20 == 0:\n",
        "                pred_texts = ctc_decode(log_probs.cpu(), train_dataset.idx_to_char)\n",
        "                word_acc, _ = calculate_metrics(pred_texts, raw_texts)\n",
        "                train_word_acc += word_acc\n",
        "                pbar.set_postfix({\"loss\": f\"{loss.item():.3f}\", \"word_acc\": f\"{word_acc:.3f}\"})\n",
        "\n",
        "            num_train_batches += 1\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss /= num_train_batches\n",
        "        train_word_acc /= (num_train_batches // 20 + 1)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds, val_truths = [], []\n",
        "        with torch.no_grad():\n",
        "            for images, targets, target_lengths, raw_texts in test_loader:\n",
        "                images = images.to(device)\n",
        "                log_probs = model(images)\n",
        "                pred_texts = ctc_decode(log_probs.cpu(), test_dataset.idx_to_char)\n",
        "                val_preds.extend(pred_texts)\n",
        "                val_truths.extend(raw_texts)\n",
        "\n",
        "        val_word_acc, val_char_acc = calculate_metrics(val_preds, val_truths)\n",
        "\n",
        "        print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
        "        print(f\"  Training Loss: {train_loss:.4f}\")\n",
        "        print(f\"  Training Word Acc: {train_word_acc:.4f}\")\n",
        "        print(f\"  Validation Word Acc: {val_word_acc:.4f}\")\n",
        "        print(f\"  Validation Char Acc: {val_char_acc:.4f}\")\n",
        "        print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        if val_word_acc > best_word_acc:\n",
        "            best_word_acc = val_word_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'val_word_acc': val_word_acc,\n",
        "                'idx_to_char': train_dataset.idx_to_char,\n",
        "                'num_classes': train_dataset.num_classes\n",
        "            }, 'deep_ocr_custom_split_model.pth')\n",
        "            print(f\"  ‚úÖ New best model saved! Word Acc: {best_word_acc:.4f}\")\n",
        "\n",
        "        # Sample predictions\n",
        "        print(f\"\\nüéØ Sample predictions:\")\n",
        "        indices = np.random.choice(len(val_preds), min(5, len(val_preds)), replace=False)\n",
        "        for i in indices:\n",
        "            print(f\"  True: '{val_truths[i]}' | Pred: '{val_preds[i]}'\")\n",
        "\n",
        "    print(f\"\\nüéâ Training completed!\")\n",
        "    print(f\"Best validation word accuracy: {best_word_acc:.4f}\")\n",
        "    return best_word_acc\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 5. MAIN EXECUTION\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"‚ö° DEEP OCR SYSTEM WITH CUSTOM SPLIT\")\n",
        "    print(\"=\" * 50)\n",
        "    choice = input(\"\\nChoose option:\\n1. Train with custom split (4500 train, 500 test)\\n2. Exit\\nChoice: \")\n",
        "    if choice == \"1\":\n",
        "        accuracy = train_deep_ocr_custom_split()\n",
        "        if accuracy is not None:\n",
        "            print(f\"\\nüèÜ Final result: {accuracy:.4f} word accuracy\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Training failed. Check error messages above.\")\n",
        "    else:\n",
        "        print(\"Goodbye! üëã\")"
      ],
      "metadata": {
        "id": "EdESf4YIL-vO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e5d969-0c35-4977-cf97-72cd12c8bb04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° DEEP OCR SYSTEM WITH CUSTOM SPLIT\n",
            "==================================================\n",
            "\n",
            "Choose option:\n",
            "1. Train with custom split (4500 train, 500 test)\n",
            "2. Exit\n",
            "Choice: 1\n",
            "üöÄ DEEP OCR TRAINING WITH CUSTOM SPLIT (4500 train, 500 test)\n",
            "============================================================\n",
            "üîç Checking dataset root: /content/iiit5k_dataset\n",
            "üìÅ Path exists: True\n",
            "‚úÖ Dataset root found: /content/iiit5k_dataset\n",
            "üîç Checking IIIT5K path: /content/iiit5k_dataset/IIIT5K\n",
            "üìÅ IIIT5K exists: True\n",
            "üìä IIIT5K contents: ['test', 'traindata.mat', 'trainCharBound.mat', 'testCharBound.mat', 'train', 'README', 'lexicon.txt', 'testdata.mat']\n",
            "‚úÖ All required dataset components found!\n",
            "Custom split: 4500 train, 500 test\n",
            "Batch size: 32\n",
            "Epochs: 250\n",
            "Learning rate: 0.001\n",
            "\n",
            "Creating custom split datasets...\n",
            "Creating custom dataset split with 4500 train, 500 test samples...\n",
            "Processing 2000 samples from train data...\n",
            "Processing 3000 samples from test data...\n",
            "Total samples loaded: 5000\n",
            "‚úÖ Created training set with 4500 samples\n",
            "Sample texts: ['YOU', 'SBI', 'MEDICINE', 'MARRY', 'YOURSELF', 'CAN', 'THIS', '21', 'VOLTS', 'WAVERLEY']\n",
            "Text lengths: min=1, max=22, avg=5.0\n",
            "Vocabulary size: 95\n",
            "Creating custom dataset split with 4500 train, 500 test samples...\n",
            "Processing 2000 samples from train data...\n",
            "Processing 3000 samples from test data...\n",
            "Total samples loaded: 5000\n",
            "‚úÖ Created test set with 500 samples\n",
            "Sample texts: ['PACK', '1800BUYKWIK', 'DAY', 'INDIA', 'INDIA', 'INDIA', 'LINY', 'GREAT', 'FOUNDRY', 'SOLVE']\n",
            "Text lengths: min=1, max=19, avg=5.0\n",
            "Vocabulary size: 95\n",
            "\n",
            "Device: cuda\n",
            "Model parameters: 5,580,255\n",
            "Training batches: 141\n",
            "Test batches: 16\n",
            "\n",
            "üìà Epoch 1/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:10<00:00, 13.73it/s, loss=3.593, word_acc=0.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 1 Summary:\n",
            "  Training Loss: 4.1951\n",
            "  Training Word Acc: 0.0000\n",
            "  Validation Word Acc: 0.0000\n",
            "  Validation Char Acc: 0.0000\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'SALT' | Pred: ''\n",
            "  True: 'BIG' | Pred: ''\n",
            "  True: 'DANGERS' | Pred: ''\n",
            "  True: 'THE' | Pred: ''\n",
            "  True: 'INVERCARGILL' | Pred: ''\n",
            "\n",
            "üìà Epoch 2/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 17.10it/s, loss=3.409, word_acc=0.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 2 Summary:\n",
            "  Training Loss: 3.5359\n",
            "  Training Word Acc: 0.0000\n",
            "  Validation Word Acc: 0.0000\n",
            "  Validation Char Acc: 0.0000\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'AN' | Pred: ''\n",
            "  True: 'WISTERIA' | Pred: ''\n",
            "  True: 'MELIN' | Pred: ''\n",
            "  True: 'EASTWOOD' | Pred: ''\n",
            "  True: 'PRODAJA' | Pred: ''\n",
            "\n",
            "üìà Epoch 3/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.42it/s, loss=3.725, word_acc=0.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 3 Summary:\n",
            "  Training Loss: 3.4614\n",
            "  Training Word Acc: 0.0000\n",
            "  Validation Word Acc: 0.0000\n",
            "  Validation Char Acc: 0.0028\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '20' | Pred: ''\n",
            "  True: 'SADLER' | Pred: ''\n",
            "  True: 'SBI' | Pred: ''\n",
            "  True: 'WHETLEY' | Pred: ''\n",
            "  True: 'WINSLET' | Pred: ''\n",
            "\n",
            "üìà Epoch 4/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.44it/s, loss=3.281, word_acc=0.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 4 Summary:\n",
            "  Training Loss: 3.3220\n",
            "  Training Word Acc: 0.0000\n",
            "  Validation Word Acc: 0.0020\n",
            "  Validation Char Acc: 0.0439\n",
            "  Learning Rate: 0.000999\n",
            "  ‚úÖ New best model saved! Word Acc: 0.0020\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'POSSOBILITIES' | Pred: 'C'\n",
            "  True: 'THE' | Pred: 'O'\n",
            "  True: 'DIAZ' | Pred: 'C'\n",
            "  True: 'IN' | Pred: 'I'\n",
            "  True: 'HOUSE' | Pred: 'C'\n",
            "\n",
            "üìà Epoch 5/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.74it/s, loss=3.131, word_acc=0.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 5 Summary:\n",
            "  Training Loss: 3.1153\n",
            "  Training Word Acc: 0.0000\n",
            "  Validation Word Acc: 0.0060\n",
            "  Validation Char Acc: 0.0587\n",
            "  Learning Rate: 0.000999\n",
            "  ‚úÖ New best model saved! Word Acc: 0.0060\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '15' | Pred: '1'\n",
            "  True: 'DAY' | Pred: 'O'\n",
            "  True: 'LIFES' | Pred: 'IE'\n",
            "  True: 'BLOOM' | Pred: 'C'\n",
            "  True: 'BANK' | Pred: 'TE'\n",
            "\n",
            "üìà Epoch 6/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.34it/s, loss=2.840, word_acc=0.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 6 Summary:\n",
            "  Training Loss: 2.8810\n",
            "  Training Word Acc: 0.0000\n",
            "  Validation Word Acc: 0.0380\n",
            "  Validation Char Acc: 0.0986\n",
            "  Learning Rate: 0.000999\n",
            "  ‚úÖ New best model saved! Word Acc: 0.0380\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ZAJAZD' | Pred: '3E'\n",
            "  True: 'REGENCY' | Pred: 'CN'\n",
            "  True: 'WISTERIA' | Pred: 'B'\n",
            "  True: 'GO' | Pred: 'C'\n",
            "  True: 'OLD' | Pred: 'O'\n",
            "\n",
            "üìà Epoch 7/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.26it/s, loss=2.467, word_acc=0.050]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 7 Summary:\n",
            "  Training Loss: 2.5441\n",
            "  Training Word Acc: 0.0336\n",
            "  Validation Word Acc: 0.0520\n",
            "  Validation Char Acc: 0.1608\n",
            "  Learning Rate: 0.000998\n",
            "  ‚úÖ New best model saved! Word Acc: 0.0520\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'LOVE' | Pred: 'TE'\n",
            "  True: 'NEWS' | Pred: 'RES'\n",
            "  True: 'HANDY' | Pred: 'AN'\n",
            "  True: 'CREATE' | Pred: 'CEE'\n",
            "  True: 'OF' | Pred: 'OF'\n",
            "\n",
            "üìà Epoch 8/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.16it/s, loss=2.317, word_acc=0.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 8 Summary:\n",
            "  Training Loss: 2.2180\n",
            "  Training Word Acc: 0.0352\n",
            "  Validation Word Acc: 0.0660\n",
            "  Validation Char Acc: 0.2010\n",
            "  Learning Rate: 0.000997\n",
            "  ‚úÖ New best model saved! Word Acc: 0.0660\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'VAL' | Pred: 'IAD'\n",
            "  True: 'BALE' | Pred: 'BAE'\n",
            "  True: 'NORTH' | Pred: 'AOE'\n",
            "  True: 'FOSTER' | Pred: 'BOSER'\n",
            "  True: 'REGENCY' | Pred: 'AEE'\n",
            "\n",
            "üìà Epoch 9/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.37it/s, loss=1.798, word_acc=0.050]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 9 Summary:\n",
            "  Training Loss: 1.8886\n",
            "  Training Word Acc: 0.0570\n",
            "  Validation Word Acc: 0.0860\n",
            "  Validation Char Acc: 0.2450\n",
            "  Learning Rate: 0.000997\n",
            "  ‚úÖ New best model saved! Word Acc: 0.0860\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '2' | Pred: '2'\n",
            "  True: 'CIUDAD' | Pred: 'CCA'\n",
            "  True: '23' | Pred: '2'\n",
            "  True: 'THIS' | Pred: 'THS'\n",
            "  True: 'OF' | Pred: 'OF'\n",
            "\n",
            "üìà Epoch 10/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 17.07it/s, loss=1.298, word_acc=0.050]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 10 Summary:\n",
            "  Training Loss: 1.5832\n",
            "  Training Word Acc: 0.1000\n",
            "  Validation Word Acc: 0.1880\n",
            "  Validation Char Acc: 0.4065\n",
            "  Learning Rate: 0.000996\n",
            "  ‚úÖ New best model saved! Word Acc: 0.1880\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'MASTERFILE' | Pred: 'TAEETE'\n",
            "  True: 'TARKOWSKI' | Pred: 'TARROWS'\n",
            "  True: 'PORTABLE' | Pred: 'PORAPE'\n",
            "  True: 'GRASSROOTSMARKETING' | Pred: 'WTNTN'\n",
            "  True: 'BADGES' | Pred: 'BALCES'\n",
            "\n",
            "üìà Epoch 11/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.40it/s, loss=1.255, word_acc=0.200]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 11 Summary:\n",
            "  Training Loss: 1.3036\n",
            "  Training Word Acc: 0.2477\n",
            "  Validation Word Acc: 0.2680\n",
            "  Validation Char Acc: 0.4927\n",
            "  Learning Rate: 0.000995\n",
            "  ‚úÖ New best model saved! Word Acc: 0.2680\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'KARI' | Pred: 'RRT'\n",
            "  True: 'COMPARISON' | Pred: 'CONWATISOM'\n",
            "  True: '5' | Pred: 'S'\n",
            "  True: 'DAY' | Pred: 'DAY'\n",
            "  True: 'REBELLION' | Pred: 'MWSION'\n",
            "\n",
            "üìà Epoch 12/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.32it/s, loss=1.287, word_acc=0.400]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 12 Summary:\n",
            "  Training Loss: 1.0790\n",
            "  Training Word Acc: 0.3234\n",
            "  Validation Word Acc: 0.3320\n",
            "  Validation Char Acc: 0.5334\n",
            "  Learning Rate: 0.000994\n",
            "  ‚úÖ New best model saved! Word Acc: 0.3320\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'VIEWERS' | Pred: 'WENERS'\n",
            "  True: 'A' | Pred: 'A'\n",
            "  True: 'INTO' | Pred: 'IO'\n",
            "  True: 'VAL' | Pred: 'VAL'\n",
            "  True: 'DIVIDE' | Pred: 'HIDE'\n",
            "\n",
            "üìà Epoch 13/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.32it/s, loss=0.623, word_acc=0.300]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 13 Summary:\n",
            "  Training Loss: 0.8823\n",
            "  Training Word Acc: 0.3891\n",
            "  Validation Word Acc: 0.3820\n",
            "  Validation Char Acc: 0.5659\n",
            "  Learning Rate: 0.000993\n",
            "  ‚úÖ New best model saved! Word Acc: 0.3820\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ONLY' | Pred: 'ONY'\n",
            "  True: 'NO' | Pred: 'WON'\n",
            "  True: 'PENANG' | Pred: 'PENANG'\n",
            "  True: 'INDIA' | Pred: 'INDA'\n",
            "  True: '77' | Pred: '7'\n",
            "\n",
            "üìà Epoch 14/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 17.15it/s, loss=0.363, word_acc=0.750]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 14 Summary:\n",
            "  Training Loss: 0.7480\n",
            "  Training Word Acc: 0.5273\n",
            "  Validation Word Acc: 0.4760\n",
            "  Validation Char Acc: 0.6542\n",
            "  Learning Rate: 0.000992\n",
            "  ‚úÖ New best model saved! Word Acc: 0.4760\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'LOVE' | Pred: 'TOA'\n",
            "  True: 'MONTGOMERY' | Pred: 'MONIGOMBENY'\n",
            "  True: 'FAILTE' | Pred: 'CALLTE'\n",
            "  True: 'GOODWILL' | Pred: 'COODYILL'\n",
            "  True: 'NORTH' | Pred: 'MORTH'\n",
            "\n",
            "üìà Epoch 15/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.31it/s, loss=0.674, word_acc=0.400]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 15 Summary:\n",
            "  Training Loss: 0.6570\n",
            "  Training Word Acc: 0.5188\n",
            "  Validation Word Acc: 0.4740\n",
            "  Validation Char Acc: 0.6711\n",
            "  Learning Rate: 0.000991\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'LOADING' | Pred: 'CEBING'\n",
            "  True: 'BANKNORTH' | Pred: 'BANKNORIH'\n",
            "  True: 'FAILTE' | Pred: 'TAILIE'\n",
            "  True: 'DENZEL' | Pred: 'DENEET'\n",
            "  True: 'MASTERFILE' | Pred: 'IAEAIE'\n",
            "\n",
            "üìà Epoch 16/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.26it/s, loss=0.500, word_acc=0.400]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 16 Summary:\n",
            "  Training Loss: 0.5703\n",
            "  Training Word Acc: 0.5109\n",
            "  Validation Word Acc: 0.5080\n",
            "  Validation Char Acc: 0.7029\n",
            "  Learning Rate: 0.000990\n",
            "  ‚úÖ New best model saved! Word Acc: 0.5080\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'THE' | Pred: 'THC'\n",
            "  True: 'HOUSE' | Pred: 'HOUSE'\n",
            "  True: 'DUKES' | Pred: 'DUKES'\n",
            "  True: 'FIRE' | Pred: 'FIRE'\n",
            "  True: 'SADLER' | Pred: 'SODNER'\n",
            "\n",
            "üìà Epoch 17/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.22it/s, loss=0.221, word_acc=0.700]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 17 Summary:\n",
            "  Training Loss: 0.4905\n",
            "  Training Word Acc: 0.6578\n",
            "  Validation Word Acc: 0.5580\n",
            "  Validation Char Acc: 0.7150\n",
            "  Learning Rate: 0.000989\n",
            "  ‚úÖ New best model saved! Word Acc: 0.5580\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PHONE' | Pred: 'PHON'\n",
            "  True: 'IS' | Pred: 'T5'\n",
            "  True: 'DEMONS' | Pred: 'DENONS'\n",
            "  True: 'FOR' | Pred: 'FOR'\n",
            "  True: 'HOME' | Pred: 'HOME'\n",
            "\n",
            "üìà Epoch 18/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.90it/s, loss=0.553, word_acc=0.500]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 18 Summary:\n",
            "  Training Loss: 0.4226\n",
            "  Training Word Acc: 0.6211\n",
            "  Validation Word Acc: 0.5900\n",
            "  Validation Char Acc: 0.7270\n",
            "  Learning Rate: 0.000987\n",
            "  ‚úÖ New best model saved! Word Acc: 0.5900\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'CANDIDATE' | Pred: 'CANOMOATE'\n",
            "  True: 'ASIA' | Pred: 'ASIA'\n",
            "  True: 'SALMON' | Pred: 'CLOS'\n",
            "  True: 'BE' | Pred: '38'\n",
            "  True: 'AHEAD' | Pred: 'AHEAD'\n",
            "\n",
            "üìà Epoch 19/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.19it/s, loss=0.495, word_acc=0.650]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 19 Summary:\n",
            "  Training Loss: 0.3666\n",
            "  Training Word Acc: 0.6828\n",
            "  Validation Word Acc: 0.6100\n",
            "  Validation Char Acc: 0.7432\n",
            "  Learning Rate: 0.000986\n",
            "  ‚úÖ New best model saved! Word Acc: 0.6100\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'SPACE' | Pred: 'SPACE'\n",
            "  True: 'ANGELS' | Pred: 'ANGELS'\n",
            "  True: 'YOURE' | Pred: 'YOURE'\n",
            "  True: 'INDIASBIN' | Pred: 'INDIAISBIN'\n",
            "  True: 'DAY' | Pred: '1'\n",
            "\n",
            "üìà Epoch 20/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.27it/s, loss=0.866, word_acc=0.300]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 20 Summary:\n",
            "  Training Loss: 0.3277\n",
            "  Training Word Acc: 0.7172\n",
            "  Validation Word Acc: 0.6000\n",
            "  Validation Char Acc: 0.7383\n",
            "  Learning Rate: 0.000984\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PRODAJA' | Pred: 'PIODBIA'\n",
            "  True: 'DIRECTLY' | Pred: 'DIRECTLY'\n",
            "  True: 'USER' | Pred: 'USER'\n",
            "  True: 'WISHES' | Pred: 'WISHES'\n",
            "  True: 'TO' | Pred: 'TO'\n",
            "\n",
            "üìà Epoch 21/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.19it/s, loss=0.391, word_acc=0.850]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 21 Summary:\n",
            "  Training Loss: 0.2957\n",
            "  Training Word Acc: 0.7664\n",
            "  Validation Word Acc: 0.6340\n",
            "  Validation Char Acc: 0.7524\n",
            "  Learning Rate: 0.000983\n",
            "  ‚úÖ New best model saved! Word Acc: 0.6340\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ALSO' | Pred: 'ALSD'\n",
            "  True: 'LINY' | Pred: 'LINW'\n",
            "  True: 'LOW' | Pred: 'LOW'\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "  True: 'US' | Pred: 'US'\n",
            "\n",
            "üìà Epoch 22/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.83it/s, loss=0.229, word_acc=0.750]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 22 Summary:\n",
            "  Training Loss: 0.2764\n",
            "  Training Word Acc: 0.7383\n",
            "  Validation Word Acc: 0.6100\n",
            "  Validation Char Acc: 0.7393\n",
            "  Learning Rate: 0.000981\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '8839660' | Pred: '8839660'\n",
            "  True: 'DEMONS' | Pred: 'DEWONS'\n",
            "  True: 'IN' | Pred: 'IN'\n",
            "  True: 'HOME' | Pred: 'HOTIE'\n",
            "  True: 'METER' | Pred: 'METER'\n",
            "\n",
            "üìà Epoch 23/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.84it/s, loss=0.574, word_acc=0.750]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 23 Summary:\n",
            "  Training Loss: 0.2338\n",
            "  Training Word Acc: 0.7812\n",
            "  Validation Word Acc: 0.6440\n",
            "  Validation Char Acc: 0.7622\n",
            "  Learning Rate: 0.000979\n",
            "  ‚úÖ New best model saved! Word Acc: 0.6440\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'CAMP' | Pred: 'CAMP'\n",
            "  True: 'TRINITYLEEDS' | Pred: 'THMILGLES'\n",
            "  True: 'MEANS' | Pred: 'MEANS'\n",
            "  True: 'ABOUT' | Pred: 'ABOUT'\n",
            "  True: '95' | Pred: '95'\n",
            "\n",
            "üìà Epoch 24/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.89it/s, loss=0.226, word_acc=0.850]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 24 Summary:\n",
            "  Training Loss: 0.2065\n",
            "  Training Word Acc: 0.8719\n",
            "  Validation Word Acc: 0.6560\n",
            "  Validation Char Acc: 0.7688\n",
            "  Learning Rate: 0.000977\n",
            "  ‚úÖ New best model saved! Word Acc: 0.6560\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'THIS' | Pred: 'THIS'\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "  True: 'DIRECTION' | Pred: 'DIRECTION'\n",
            "  True: 'DIVIDE' | Pred: 'PIVIDE'\n",
            "  True: 'WILT' | Pred: 'WILT'\n",
            "\n",
            "üìà Epoch 25/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.79it/s, loss=0.207, word_acc=0.850]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 25 Summary:\n",
            "  Training Loss: 0.1719\n",
            "  Training Word Acc: 0.8523\n",
            "  Validation Word Acc: 0.6760\n",
            "  Validation Char Acc: 0.7935\n",
            "  Learning Rate: 0.000976\n",
            "  ‚úÖ New best model saved! Word Acc: 0.6760\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'LIFES' | Pred: 'LFES'\n",
            "  True: 'LETS' | Pred: 'LETS'\n",
            "  True: '101' | Pred: '101'\n",
            "  True: 'MARZO' | Pred: 'MARZO'\n",
            "  True: 'CFN' | Pred: 'CFN'\n",
            "\n",
            "üìà Epoch 26/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.70it/s, loss=0.121, word_acc=0.900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 26 Summary:\n",
            "  Training Loss: 0.1650\n",
            "  Training Word Acc: 0.8430\n",
            "  Validation Word Acc: 0.6880\n",
            "  Validation Char Acc: 0.7810\n",
            "  Learning Rate: 0.000974\n",
            "  ‚úÖ New best model saved! Word Acc: 0.6880\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'MASTERFILE' | Pred: 'TOTERILE'\n",
            "  True: 'IMPERIAL' | Pred: 'TMPENAN'\n",
            "  True: 'BEGIN' | Pred: 'BEGIN'\n",
            "  True: 'MOTION' | Pred: 'MOTION'\n",
            "  True: 'STATE' | Pred: 'STATE'\n",
            "\n",
            "üìà Epoch 27/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.44it/s, loss=0.102, word_acc=0.800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 27 Summary:\n",
            "  Training Loss: 0.1435\n",
            "  Training Word Acc: 0.8344\n",
            "  Validation Word Acc: 0.6840\n",
            "  Validation Char Acc: 0.8051\n",
            "  Learning Rate: 0.000971\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'I' | Pred: 'I'\n",
            "  True: '3D' | Pred: '9D'\n",
            "  True: 'YOU' | Pred: 'YOU'\n",
            "  True: 'MOBILE' | Pred: 'MOBILE'\n",
            "  True: 'LINY' | Pred: 'LINY'\n",
            "\n",
            "üìà Epoch 28/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.07it/s, loss=0.328, word_acc=0.650]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 28 Summary:\n",
            "  Training Loss: 0.1308\n",
            "  Training Word Acc: 0.8000\n",
            "  Validation Word Acc: 0.6540\n",
            "  Validation Char Acc: 0.7935\n",
            "  Learning Rate: 0.000969\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ABOUT' | Pred: 'ABOUT'\n",
            "  True: '23' | Pred: '23'\n",
            "  True: 'INDIA' | Pred: 'LANA'\n",
            "  True: 'MONTGOMERY' | Pred: 'MONIGOMIERY'\n",
            "  True: 'AHEAD' | Pred: 'AHEAD'\n",
            "\n",
            "üìà Epoch 29/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.16it/s, loss=0.049, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 29 Summary:\n",
            "  Training Loss: 0.1207\n",
            "  Training Word Acc: 0.8961\n",
            "  Validation Word Acc: 0.7020\n",
            "  Validation Char Acc: 0.7887\n",
            "  Learning Rate: 0.000967\n",
            "  ‚úÖ New best model saved! Word Acc: 0.7020\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'VIEW' | Pred: 'VIEW'\n",
            "  True: 'PARKING' | Pred: 'PARKING'\n",
            "  True: 'MIRREN' | Pred: 'MIRREN'\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "  True: 'BLUBBER' | Pred: 'BTUBBER'\n",
            "\n",
            "üìà Epoch 30/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.34it/s, loss=0.040, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 30 Summary:\n",
            "  Training Loss: 0.1086\n",
            "  Training Word Acc: 0.8883\n",
            "  Validation Word Acc: 0.6920\n",
            "  Validation Char Acc: 0.8036\n",
            "  Learning Rate: 0.000965\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'EASTWOOD' | Pred: 'EASTYOOD'\n",
            "  True: 'HEFNER' | Pred: 'HEFNER'\n",
            "  True: 'KELLIMAR' | Pred: 'LEHAMAMAN'\n",
            "  True: '02' | Pred: '02'\n",
            "  True: 'ON' | Pred: 'ON'\n",
            "\n",
            "üìà Epoch 31/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.54it/s, loss=0.127, word_acc=0.850]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 31 Summary:\n",
            "  Training Loss: 0.1137\n",
            "  Training Word Acc: 0.9070\n",
            "  Validation Word Acc: 0.6620\n",
            "  Validation Char Acc: 0.7770\n",
            "  Learning Rate: 0.000963\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BOGART' | Pred: 'BOGART'\n",
            "  True: 'TOUCHDOWN' | Pred: 'FOUGPDOT'\n",
            "  True: 'AN' | Pred: 'AN'\n",
            "  True: 'VIEW' | Pred: 'WIES'\n",
            "  True: 'PRICES' | Pred: 'PRICES'\n",
            "\n",
            "üìà Epoch 32/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:09<00:00, 15.15it/s, loss=0.069, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 32 Summary:\n",
            "  Training Loss: 0.0972\n",
            "  Training Word Acc: 0.8961\n",
            "  Validation Word Acc: 0.6740\n",
            "  Validation Char Acc: 0.7915\n",
            "  Learning Rate: 0.000960\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BOGART' | Pred: 'BOGART'\n",
            "  True: 'STATE' | Pred: 'STATE'\n",
            "  True: 'NSERIES' | Pred: 'NSERIES'\n",
            "  True: 'MARILYN' | Pred: 'MARILYN'\n",
            "  True: 'KELLIMAR' | Pred: 'LEHMMAN'\n",
            "\n",
            "üìà Epoch 33/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.01it/s, loss=0.032, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 33 Summary:\n",
            "  Training Loss: 0.0873\n",
            "  Training Word Acc: 0.8766\n",
            "  Validation Word Acc: 0.6940\n",
            "  Validation Char Acc: 0.7880\n",
            "  Learning Rate: 0.000958\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'VALE' | Pred: 'VALE'\n",
            "  True: 'MASS' | Pred: 'MASS'\n",
            "  True: 'RD' | Pred: 'RD'\n",
            "  True: '3D' | Pred: 'BR'\n",
            "  True: 'BIG' | Pred: 'BIG'\n",
            "\n",
            "üìà Epoch 34/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.10it/s, loss=0.106, word_acc=0.900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 34 Summary:\n",
            "  Training Loss: 0.0779\n",
            "  Training Word Acc: 0.9172\n",
            "  Validation Word Acc: 0.6740\n",
            "  Validation Char Acc: 0.7834\n",
            "  Learning Rate: 0.000955\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'CARING' | Pred: 'CARING'\n",
            "  True: 'DIRECTION' | Pred: 'DIRECTION'\n",
            "  True: 'THIS' | Pred: 'THIS'\n",
            "  True: 'COLA' | Pred: 'COLA'\n",
            "  True: 'CHECKMATE' | Pred: 'CHECKMATE'\n",
            "\n",
            "üìà Epoch 35/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.66it/s, loss=0.118, word_acc=0.850]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 35 Summary:\n",
            "  Training Loss: 0.0766\n",
            "  Training Word Acc: 0.8875\n",
            "  Validation Word Acc: 0.7140\n",
            "  Validation Char Acc: 0.8024\n",
            "  Learning Rate: 0.000952\n",
            "  ‚úÖ New best model saved! Word Acc: 0.7140\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'SURVEY' | Pred: 'SURVEY'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'FUTURE' | Pred: 'FUTURE'\n",
            "  True: 'TWENTY' | Pred: 'INENTY'\n",
            "  True: 'RESET' | Pred: 'RESET'\n",
            "\n",
            "üìà Epoch 36/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.92it/s, loss=0.103, word_acc=0.750]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 36 Summary:\n",
            "  Training Loss: 0.0624\n",
            "  Training Word Acc: 0.8984\n",
            "  Validation Word Acc: 0.7040\n",
            "  Validation Char Acc: 0.8221\n",
            "  Learning Rate: 0.000950\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'IS' | Pred: 'IES'\n",
            "  True: 'EASTWOOD' | Pred: 'EASTWOOD'\n",
            "  True: 'BOGART' | Pred: 'BOGART'\n",
            "  True: 'RD' | Pred: 'RD'\n",
            "  True: 'CHRYSLER' | Pred: 'CHRVSLER'\n",
            "\n",
            "üìà Epoch 37/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.11it/s, loss=0.017, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 37 Summary:\n",
            "  Training Loss: 0.0593\n",
            "  Training Word Acc: 0.9414\n",
            "  Validation Word Acc: 0.7060\n",
            "  Validation Char Acc: 0.8153\n",
            "  Learning Rate: 0.000947\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'SPACE' | Pred: 'SPACE'\n",
            "  True: 'AND' | Pred: 'AND'\n",
            "  True: 'HOUSE' | Pred: 'HOUSE'\n",
            "  True: 'TWITTERCOMAPLUSK' | Pred: 'WILECOMLEPLOST'\n",
            "  True: 'HOME' | Pred: 'FOME'\n",
            "\n",
            "üìà Epoch 38/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.10it/s, loss=0.046, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 38 Summary:\n",
            "  Training Loss: 0.0514\n",
            "  Training Word Acc: 0.9430\n",
            "  Validation Word Acc: 0.7220\n",
            "  Validation Char Acc: 0.8024\n",
            "  Learning Rate: 0.000944\n",
            "  ‚úÖ New best model saved! Word Acc: 0.7220\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'FRIENDS' | Pred: 'FRIENDS'\n",
            "  True: 'EN' | Pred: 'EN'\n",
            "  True: 'LOSE' | Pred: 'LOSE'\n",
            "  True: 'FOSTER' | Pred: 'FOSTER'\n",
            "  True: 'WISHES' | Pred: 'WISHES'\n",
            "\n",
            "üìà Epoch 39/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.86it/s, loss=0.078, word_acc=0.750]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 39 Summary:\n",
            "  Training Loss: 0.0513\n",
            "  Training Word Acc: 0.9297\n",
            "  Validation Word Acc: 0.7140\n",
            "  Validation Char Acc: 0.8143\n",
            "  Learning Rate: 0.000941\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'TOUCHDOWN' | Pred: 'TOUGHDOUT'\n",
            "  True: 'SAVE' | Pred: 'SAVE'\n",
            "  True: 'RIGHT' | Pred: 'RIGHT'\n",
            "  True: 'DENZEL' | Pred: 'DENZET'\n",
            "  True: 'HEIGHT' | Pred: 'HEIGHT'\n",
            "\n",
            "üìà Epoch 40/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.13it/s, loss=0.020, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 40 Summary:\n",
            "  Training Loss: 0.0569\n",
            "  Training Word Acc: 0.9195\n",
            "  Validation Word Acc: 0.6920\n",
            "  Validation Char Acc: 0.8055\n",
            "  Learning Rate: 0.000938\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BLOOM' | Pred: 'OBIOOM'\n",
            "  True: 'IN' | Pred: 'IN'\n",
            "  True: 'GRAHAMS' | Pred: 'CRAHOINTE'\n",
            "  True: 'BE' | Pred: 'BE'\n",
            "  True: 'TELL' | Pred: 'TELL'\n",
            "\n",
            "üìà Epoch 41/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.09it/s, loss=0.071, word_acc=0.850]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 41 Summary:\n",
            "  Training Loss: 0.0446\n",
            "  Training Word Acc: 0.9187\n",
            "  Validation Word Acc: 0.7040\n",
            "  Validation Char Acc: 0.8033\n",
            "  Learning Rate: 0.000935\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'HOME' | Pred: 'HOME'\n",
            "  True: 'ACTRESS' | Pred: 'ACTRESS'\n",
            "  True: 'PITT' | Pred: 'PITT'\n",
            "  True: 'YOUR' | Pred: 'YOUR'\n",
            "  True: 'LA' | Pred: 'L'\n",
            "\n",
            "üìà Epoch 42/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.14it/s, loss=0.092, word_acc=0.700]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 42 Summary:\n",
            "  Training Loss: 0.0547\n",
            "  Training Word Acc: 0.8883\n",
            "  Validation Word Acc: 0.7040\n",
            "  Validation Char Acc: 0.8138\n",
            "  Learning Rate: 0.000932\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'HOMICIDE' | Pred: 'HOMICIDE'\n",
            "  True: 'DOLLARS' | Pred: 'DELARS'\n",
            "  True: '25' | Pred: '25'\n",
            "  True: 'VALE' | Pred: 'VALE'\n",
            "  True: '4865' | Pred: '4365'\n",
            "\n",
            "üìà Epoch 43/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.96it/s, loss=0.126, word_acc=0.900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 43 Summary:\n",
            "  Training Loss: 0.0607\n",
            "  Training Word Acc: 0.9406\n",
            "  Validation Word Acc: 0.6940\n",
            "  Validation Char Acc: 0.8010\n",
            "  Learning Rate: 0.000929\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ASHTON' | Pred: 'ASHTON'\n",
            "  True: 'A' | Pred: 'A'\n",
            "  True: '95' | Pred: '95'\n",
            "  True: 'ACTRESS' | Pred: 'ACTRESS'\n",
            "  True: 'HOME' | Pred: 'HOME'\n",
            "\n",
            "üìà Epoch 44/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.27it/s, loss=0.018, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 44 Summary:\n",
            "  Training Loss: 0.0435\n",
            "  Training Word Acc: 0.9648\n",
            "  Validation Word Acc: 0.7160\n",
            "  Validation Char Acc: 0.8121\n",
            "  Learning Rate: 0.000925\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ON' | Pred: 'ON'\n",
            "  True: 'SLATE' | Pred: 'SLATE'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'TO' | Pred: 'TO'\n",
            "  True: 'HEIGHT' | Pred: 'HEIGHT'\n",
            "\n",
            "üìà Epoch 45/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.11it/s, loss=0.056, word_acc=0.900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 45 Summary:\n",
            "  Training Loss: 0.0380\n",
            "  Training Word Acc: 0.9289\n",
            "  Validation Word Acc: 0.7200\n",
            "  Validation Char Acc: 0.8205\n",
            "  Learning Rate: 0.000922\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'MCKINNEY' | Pred: 'MEKINNEY'\n",
            "  True: 'VALE' | Pred: 'VALE'\n",
            "  True: 'TOTAL' | Pred: 'TOTAL'\n",
            "  True: 'FRUIT' | Pred: 'FUIN'\n",
            "  True: 'SALMON' | Pred: 'CATLOS'\n",
            "\n",
            "üìà Epoch 46/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.19it/s, loss=0.029, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 46 Summary:\n",
            "  Training Loss: 0.0379\n",
            "  Training Word Acc: 0.9234\n",
            "  Validation Word Acc: 0.7120\n",
            "  Validation Char Acc: 0.8152\n",
            "  Learning Rate: 0.000919\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BLANKES' | Pred: 'BLANKES'\n",
            "  True: 'SAVE' | Pred: 'SAVE'\n",
            "  True: 'YOUR' | Pred: 'YOUR'\n",
            "  True: 'MATRIX' | Pred: 'MATRIX'\n",
            "  True: 'HOUSE' | Pred: 'HIOUSE'\n",
            "\n",
            "üìà Epoch 47/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.89it/s, loss=0.068, word_acc=0.850]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 47 Summary:\n",
            "  Training Loss: 0.0302\n",
            "  Training Word Acc: 0.9422\n",
            "  Validation Word Acc: 0.7460\n",
            "  Validation Char Acc: 0.8247\n",
            "  Learning Rate: 0.000915\n",
            "  ‚úÖ New best model saved! Word Acc: 0.7460\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PHONE' | Pred: 'PHONE'\n",
            "  True: 'PET' | Pred: 'PET'\n",
            "  True: 'CHIC' | Pred: 'CHIC'\n",
            "  True: 'TERMINAL' | Pred: 'TERNNEL'\n",
            "  True: 'JUKEBOX' | Pred: 'JUKEBOX'\n",
            "\n",
            "üìà Epoch 48/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.45it/s, loss=0.068, word_acc=0.900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 48 Summary:\n",
            "  Training Loss: 0.0290\n",
            "  Training Word Acc: 0.9523\n",
            "  Validation Word Acc: 0.6980\n",
            "  Validation Char Acc: 0.8093\n",
            "  Learning Rate: 0.000912\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'IS' | Pred: 'IS'\n",
            "  True: 'CITROEN' | Pred: 'CITROEN'\n",
            "  True: 'INVERCARGILL' | Pred: 'INVEREARGIL'\n",
            "  True: '1989' | Pred: '1989'\n",
            "  True: 'THE' | Pred: 'TH'\n",
            "\n",
            "üìà Epoch 49/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.15it/s, loss=0.015, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 49 Summary:\n",
            "  Training Loss: 0.0561\n",
            "  Training Word Acc: 0.9297\n",
            "  Validation Word Acc: 0.6960\n",
            "  Validation Char Acc: 0.8114\n",
            "  Learning Rate: 0.000908\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BLOOM' | Pred: 'OBIOOM'\n",
            "  True: 'LOUIS' | Pred: 'LGLIIS'\n",
            "  True: 'REVERSE' | Pred: 'REVERSE'\n",
            "  True: '560' | Pred: '560'\n",
            "  True: 'WISHES' | Pred: 'WISHES'\n",
            "\n",
            "üìà Epoch 50/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.31it/s, loss=0.076, word_acc=0.800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 50 Summary:\n",
            "  Training Loss: 0.0548\n",
            "  Training Word Acc: 0.9242\n",
            "  Validation Word Acc: 0.7060\n",
            "  Validation Char Acc: 0.8048\n",
            "  Learning Rate: 0.000905\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'STATE' | Pred: 'STATE'\n",
            "  True: 'PARKING' | Pred: 'PARKING'\n",
            "  True: 'MARILYN' | Pred: 'NARILYN'\n",
            "  True: 'FOR' | Pred: 'FOR'\n",
            "  True: 'INTO' | Pred: 'INTO'\n",
            "\n",
            "üìà Epoch 51/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 17.01it/s, loss=0.024, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 51 Summary:\n",
            "  Training Loss: 0.0449\n",
            "  Training Word Acc: 0.9547\n",
            "  Validation Word Acc: 0.7060\n",
            "  Validation Char Acc: 0.7990\n",
            "  Learning Rate: 0.000901\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'WISHES' | Pred: 'WISHES'\n",
            "  True: 'CFN' | Pred: 'CFN'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'CITY' | Pred: 'CITY'\n",
            "  True: 'MANCHURIAN' | Pred: 'IANICHURLAN'\n",
            "\n",
            "üìà Epoch 52/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.50it/s, loss=0.012, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 52 Summary:\n",
            "  Training Loss: 0.0353\n",
            "  Training Word Acc: 0.9492\n",
            "  Validation Word Acc: 0.6920\n",
            "  Validation Char Acc: 0.7969\n",
            "  Learning Rate: 0.000897\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'DOT' | Pred: 'DOT'\n",
            "  True: 'FEW' | Pred: 'FEW'\n",
            "  True: 'HOUSE' | Pred: 'HOUSE'\n",
            "  True: 'TO' | Pred: 'TO'\n",
            "  True: 'ON' | Pred: 'ON'\n",
            "\n",
            "üìà Epoch 53/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.09it/s, loss=0.003, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 53 Summary:\n",
            "  Training Loss: 0.0249\n",
            "  Training Word Acc: 0.9648\n",
            "  Validation Word Acc: 0.7360\n",
            "  Validation Char Acc: 0.8249\n",
            "  Learning Rate: 0.000893\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ECO' | Pred: 'ECO'\n",
            "  True: 'AHEAD' | Pred: 'AHEAD'\n",
            "  True: 'REEVES' | Pred: 'REEVES'\n",
            "  True: 'HOMICIDE' | Pred: 'HOMICIDE'\n",
            "  True: 'DOLLARS' | Pred: 'DLLARS'\n",
            "\n",
            "üìà Epoch 54/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.27it/s, loss=0.055, word_acc=0.900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 54 Summary:\n",
            "  Training Loss: 0.0341\n",
            "  Training Word Acc: 0.9563\n",
            "  Validation Word Acc: 0.7000\n",
            "  Validation Char Acc: 0.8146\n",
            "  Learning Rate: 0.000889\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'TOLL' | Pred: 'TOLL'\n",
            "  True: 'TIME' | Pred: 'TE'\n",
            "  True: 'ONLY' | Pred: 'ONLY'\n",
            "  True: 'DIVIDE' | Pred: 'DIVIDE'\n",
            "  True: 'AHEAD' | Pred: 'AHEAD'\n",
            "\n",
            "üìà Epoch 55/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.73it/s, loss=0.014, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 55 Summary:\n",
            "  Training Loss: 0.0395\n",
            "  Training Word Acc: 0.9313\n",
            "  Validation Word Acc: 0.6880\n",
            "  Validation Char Acc: 0.8032\n",
            "  Learning Rate: 0.000885\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'MASTERFILE' | Pred: 'TRIASSTERFILE'\n",
            "  True: 'HOMICIDE' | Pred: 'HOMICIDE'\n",
            "  True: 'YOUR' | Pred: 'YOUS'\n",
            "  True: 'IMAX' | Pred: 'INDAK'\n",
            "  True: 'BORDER' | Pred: 'BORDER'\n",
            "\n",
            "üìà Epoch 56/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.50it/s, loss=0.024, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 56 Summary:\n",
            "  Training Loss: 0.0332\n",
            "  Training Word Acc: 0.9742\n",
            "  Validation Word Acc: 0.7000\n",
            "  Validation Char Acc: 0.8226\n",
            "  Learning Rate: 0.000881\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'ISTANBUL' | Pred: 'ISTANBU'\n",
            "  True: 'ON' | Pred: 'ON'\n",
            "  True: 'YOU' | Pred: 'YOU'\n",
            "  True: 'EXPENDABLES' | Pred: 'EXRENOABLES'\n",
            "\n",
            "üìà Epoch 57/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:09<00:00, 14.95it/s, loss=0.004, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 57 Summary:\n",
            "  Training Loss: 0.0337\n",
            "  Training Word Acc: 0.9648\n",
            "  Validation Word Acc: 0.7220\n",
            "  Validation Char Acc: 0.8174\n",
            "  Learning Rate: 0.000877\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'VEER' | Pred: 'VEER'\n",
            "  True: 'KARI' | Pred: 'KAT'\n",
            "  True: 'FLAVOR' | Pred: 'FLAVO'\n",
            "  True: 'MASTERFILE' | Pred: 'TLASTERFILE'\n",
            "  True: 'AN' | Pred: 'AN'\n",
            "\n",
            "üìà Epoch 58/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.90it/s, loss=0.035, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 58 Summary:\n",
            "  Training Loss: 0.0225\n",
            "  Training Word Acc: 0.9586\n",
            "  Validation Word Acc: 0.7260\n",
            "  Validation Char Acc: 0.8240\n",
            "  Learning Rate: 0.000873\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'MOOD' | Pred: 'MOOD'\n",
            "  True: 'WISTERIA' | Pred: 'HETERA'\n",
            "  True: 'SWEET' | Pred: 'SNEEY'\n",
            "  True: 'CHANDIGARH' | Pred: 'CHANDIGARI'\n",
            "  True: 'MELIN' | Pred: 'MELIN'\n",
            "\n",
            "üìà Epoch 59/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.73it/s, loss=0.018, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 59 Summary:\n",
            "  Training Loss: 0.0221\n",
            "  Training Word Acc: 0.9703\n",
            "  Validation Word Acc: 0.7040\n",
            "  Validation Char Acc: 0.8162\n",
            "  Learning Rate: 0.000869\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PHONE' | Pred: 'PHONE'\n",
            "  True: 'BACK' | Pred: 'BACK'\n",
            "  True: 'JUKEBOX' | Pred: 'JUKEBOX'\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "  True: '3D' | Pred: '83'\n",
            "\n",
            "üìà Epoch 60/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.42it/s, loss=0.052, word_acc=0.900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 60 Summary:\n",
            "  Training Loss: 0.0255\n",
            "  Training Word Acc: 0.9602\n",
            "  Validation Word Acc: 0.6920\n",
            "  Validation Char Acc: 0.8046\n",
            "  Learning Rate: 0.000864\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'WITH' | Pred: 'WITH'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'DAY' | Pred: 'DAY'\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "  True: 'AND' | Pred: 'AND'\n",
            "\n",
            "üìà Epoch 61/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.92it/s, loss=0.007, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 61 Summary:\n",
            "  Training Loss: 0.0185\n",
            "  Training Word Acc: 0.9570\n",
            "  Validation Word Acc: 0.7120\n",
            "  Validation Char Acc: 0.8146\n",
            "  Learning Rate: 0.000860\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "  True: 'CAN' | Pred: 'CAN'\n",
            "  True: '208' | Pred: '208'\n",
            "  True: 'PULP' | Pred: 'PULP'\n",
            "  True: 'FOR' | Pred: 'FOR'\n",
            "\n",
            "üìà Epoch 62/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.85it/s, loss=0.039, word_acc=0.900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 62 Summary:\n",
            "  Training Loss: 0.0265\n",
            "  Training Word Acc: 0.9758\n",
            "  Validation Word Acc: 0.6940\n",
            "  Validation Char Acc: 0.7958\n",
            "  Learning Rate: 0.000856\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'NIN' | Pred: 'ILIIL'\n",
            "  True: 'TOUCHDOWN' | Pred: 'TDUGHDOWE'\n",
            "  True: 'AND' | Pred: 'AND'\n",
            "  True: 'NOVO' | Pred: 'NOVO'\n",
            "  True: 'NOSTOPPING' | Pred: 'NOSTOPPIN'\n",
            "\n",
            "üìà Epoch 63/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.32it/s, loss=0.044, word_acc=0.850]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 63 Summary:\n",
            "  Training Loss: 0.0312\n",
            "  Training Word Acc: 0.9187\n",
            "  Validation Word Acc: 0.6860\n",
            "  Validation Char Acc: 0.8044\n",
            "  Learning Rate: 0.000851\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ALSO' | Pred: 'ALSO'\n",
            "  True: 'MASS' | Pred: 'MASS'\n",
            "  True: 'SHOP' | Pred: 'SLHOP'\n",
            "  True: 'NOW' | Pred: 'NO'\n",
            "  True: 'LOVE' | Pred: 'ZO'\n",
            "\n",
            "üìà Epoch 64/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.45it/s, loss=0.002, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 64 Summary:\n",
            "  Training Loss: 0.0262\n",
            "  Training Word Acc: 0.9727\n",
            "  Validation Word Acc: 0.7060\n",
            "  Validation Char Acc: 0.8016\n",
            "  Learning Rate: 0.000847\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'CHIC' | Pred: 'CHIC'\n",
            "  True: 'VEGETARIAN' | Pred: 'UECATARLAN'\n",
            "  True: 'TAXI' | Pred: 'TAXI'\n",
            "  True: 'OLD' | Pred: 'OLD'\n",
            "  True: 'JULY' | Pred: 'JULY'\n",
            "\n",
            "üìà Epoch 65/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.96it/s, loss=0.042, word_acc=0.900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 65 Summary:\n",
            "  Training Loss: 0.0198\n",
            "  Training Word Acc: 0.9563\n",
            "  Validation Word Acc: 0.7140\n",
            "  Validation Char Acc: 0.8143\n",
            "  Learning Rate: 0.000842\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ATM' | Pred: 'ATM'\n",
            "  True: 'MELIN' | Pred: 'MELIN'\n",
            "  True: 'LOADING' | Pred: 'CEADING'\n",
            "  True: 'OLD' | Pred: 'OID'\n",
            "  True: 'CHECKMATE' | Pred: 'CHECKMATE'\n",
            "\n",
            "üìà Epoch 66/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.95it/s, loss=0.002, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 66 Summary:\n",
            "  Training Loss: 0.0167\n",
            "  Training Word Acc: 0.9922\n",
            "  Validation Word Acc: 0.7060\n",
            "  Validation Char Acc: 0.8109\n",
            "  Learning Rate: 0.000838\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'NEWS' | Pred: 'NEWS'\n",
            "  True: 'SURREY' | Pred: 'SUREY'\n",
            "  True: 'INDIAN' | Pred: 'INDIAN'\n",
            "  True: 'AN' | Pred: 'AN'\n",
            "  True: 'COM' | Pred: 'COM'\n",
            "\n",
            "üìà Epoch 67/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.99it/s, loss=0.018, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 67 Summary:\n",
            "  Training Loss: 0.0162\n",
            "  Training Word Acc: 0.9766\n",
            "  Validation Word Acc: 0.7020\n",
            "  Validation Char Acc: 0.8103\n",
            "  Learning Rate: 0.000833\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PHOENIX' | Pred: 'PHOENY'\n",
            "  True: 'YOU' | Pred: 'YOU'\n",
            "  True: 'YOU' | Pred: 'YOU'\n",
            "  True: 'HOUSE' | Pred: 'HOUSE'\n",
            "  True: 'OF' | Pred: 'OF'\n",
            "\n",
            "üìà Epoch 68/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.45it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 68 Summary:\n",
            "  Training Loss: 0.0245\n",
            "  Training Word Acc: 0.9883\n",
            "  Validation Word Acc: 0.6980\n",
            "  Validation Char Acc: 0.7998\n",
            "  Learning Rate: 0.000828\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'AS' | Pred: 'AS'\n",
            "  True: 'FUNNY' | Pred: 'FUNY'\n",
            "  True: 'MURDER' | Pred: 'MURDER'\n",
            "  True: 'UNIVERSITY' | Pred: 'UNVERSTY'\n",
            "  True: 'AND' | Pred: 'AND'\n",
            "\n",
            "üìà Epoch 69/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.79it/s, loss=0.023, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 69 Summary:\n",
            "  Training Loss: 0.0323\n",
            "  Training Word Acc: 0.9508\n",
            "  Validation Word Acc: 0.6920\n",
            "  Validation Char Acc: 0.8074\n",
            "  Learning Rate: 0.000824\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'TOM' | Pred: 'TOA'\n",
            "  True: 'VALE' | Pred: 'VALE'\n",
            "  True: 'DREAM' | Pred: 'DREAM'\n",
            "  True: 'PACK' | Pred: 'PACK'\n",
            "  True: '02' | Pred: '02'\n",
            "\n",
            "üìà Epoch 70/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.80it/s, loss=0.006, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 70 Summary:\n",
            "  Training Loss: 0.0199\n",
            "  Training Word Acc: 0.9688\n",
            "  Validation Word Acc: 0.6980\n",
            "  Validation Char Acc: 0.8013\n",
            "  Learning Rate: 0.000819\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'WILT' | Pred: 'WILT'\n",
            "  True: 'HANDY' | Pred: 'HANDY'\n",
            "  True: 'DIAZ' | Pred: 'DIA2'\n",
            "  True: 'AS' | Pred: 'AS'\n",
            "  True: 'VISTA' | Pred: 'VISTA'\n",
            "\n",
            "üìà Epoch 71/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.89it/s, loss=0.016, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 71 Summary:\n",
            "  Training Loss: 0.0172\n",
            "  Training Word Acc: 0.9703\n",
            "  Validation Word Acc: 0.7120\n",
            "  Validation Char Acc: 0.8040\n",
            "  Learning Rate: 0.000814\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'DOT' | Pred: 'DOT'\n",
            "  True: 'JURASSIC' | Pred: 'JUIRASSIC'\n",
            "  True: 'YOUR' | Pred: 'YOUE'\n",
            "  True: 'MASTERFILE' | Pred: 'THIASTERFILE'\n",
            "  True: 'USER' | Pred: 'USER'\n",
            "\n",
            "üìà Epoch 72/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.48it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 72 Summary:\n",
            "  Training Loss: 0.0126\n",
            "  Training Word Acc: 0.9766\n",
            "  Validation Word Acc: 0.7200\n",
            "  Validation Char Acc: 0.8237\n",
            "  Learning Rate: 0.000809\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ECO' | Pred: 'ECO'\n",
            "  True: 'DOT' | Pred: 'DOT'\n",
            "  True: 'PHONE' | Pred: 'PHONE'\n",
            "  True: 'AHEAD' | Pred: 'AHEAD'\n",
            "  True: 'DANGERS' | Pred: 'DANGERS'\n",
            "\n",
            "üìà Epoch 73/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.16it/s, loss=0.069, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 73 Summary:\n",
            "  Training Loss: 0.0122\n",
            "  Training Word Acc: 0.9820\n",
            "  Validation Word Acc: 0.7260\n",
            "  Validation Char Acc: 0.8225\n",
            "  Learning Rate: 0.000804\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'TAXI' | Pred: 'TAXL'\n",
            "  True: 'INDIAN' | Pred: 'INDIAN'\n",
            "  True: 'HEIGHT' | Pred: 'HEIGHT'\n",
            "  True: 'PARKING' | Pred: 'PARKING'\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "\n",
            "üìà Epoch 74/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.90it/s, loss=0.004, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 74 Summary:\n",
            "  Training Loss: 0.0063\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7500\n",
            "  Validation Char Acc: 0.8284\n",
            "  Learning Rate: 0.000799\n",
            "  ‚úÖ New best model saved! Word Acc: 0.7500\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'DAY' | Pred: 'DAY'\n",
            "  True: 'DI' | Pred: 'DI'\n",
            "  True: 'TARKOWSKI' | Pred: 'TARKOWSK'\n",
            "  True: 'BE' | Pred: 'BE'\n",
            "  True: 'PRODAJA' | Pred: 'PRODAJA'\n",
            "\n",
            "üìà Epoch 75/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.77it/s, loss=0.007, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 75 Summary:\n",
            "  Training Loss: 0.0108\n",
            "  Training Word Acc: 0.9844\n",
            "  Validation Word Acc: 0.7180\n",
            "  Validation Char Acc: 0.8130\n",
            "  Learning Rate: 0.000794\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PARKING' | Pred: 'PARKING'\n",
            "  True: 'TRINITYLEEDS' | Pred: 'THMLYLFEIS'\n",
            "  True: 'NOW' | Pred: 'NOW'\n",
            "  True: 'ATM' | Pred: 'ATM'\n",
            "  True: 'FUTURE' | Pred: 'FUTURE'\n",
            "\n",
            "üìà Epoch 76/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.48it/s, loss=0.010, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 76 Summary:\n",
            "  Training Loss: 0.0072\n",
            "  Training Word Acc: 0.9820\n",
            "  Validation Word Acc: 0.7420\n",
            "  Validation Char Acc: 0.8135\n",
            "  Learning Rate: 0.000789\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'SIGN' | Pred: 'SIGN'\n",
            "  True: 'BE' | Pred: '3E'\n",
            "  True: 'MATRIX' | Pred: 'MATRIX'\n",
            "  True: 'BOGART' | Pred: 'BOGART'\n",
            "  True: 'BANK' | Pred: 'BANK'\n",
            "\n",
            "üìà Epoch 77/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.53it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 77 Summary:\n",
            "  Training Loss: 0.0149\n",
            "  Training Word Acc: 0.9609\n",
            "  Validation Word Acc: 0.7120\n",
            "  Validation Char Acc: 0.8113\n",
            "  Learning Rate: 0.000784\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'DIRECTION' | Pred: 'DIRECTION'\n",
            "  True: 'STORES' | Pred: 'STORES'\n",
            "  True: 'SHIZUOKA' | Pred: 'SHIZUOKA'\n",
            "  True: 'JULY' | Pred: 'JULY'\n",
            "  True: '280' | Pred: '280'\n",
            "\n",
            "üìà Epoch 78/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.12it/s, loss=0.078, word_acc=0.900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 78 Summary:\n",
            "  Training Loss: 0.0347\n",
            "  Training Word Acc: 0.9602\n",
            "  Validation Word Acc: 0.7000\n",
            "  Validation Char Acc: 0.8114\n",
            "  Learning Rate: 0.000778\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'THINK' | Pred: 'THIN'\n",
            "  True: 'SPACE' | Pred: 'SPACE'\n",
            "  True: 'DAY' | Pred: 'DAY'\n",
            "  True: 'STORES' | Pred: 'STORES'\n",
            "  True: 'FIRE' | Pred: 'FIRE'\n",
            "\n",
            "üìà Epoch 79/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.08it/s, loss=0.024, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 79 Summary:\n",
            "  Training Loss: 0.0247\n",
            "  Training Word Acc: 0.9625\n",
            "  Validation Word Acc: 0.7140\n",
            "  Validation Char Acc: 0.8206\n",
            "  Learning Rate: 0.000773\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '19' | Pred: '10'\n",
            "  True: 'OLD' | Pred: 'OLD'\n",
            "  True: 'ANSON' | Pred: 'ANSON'\n",
            "  True: '212' | Pred: '212'\n",
            "  True: 'COLBY' | Pred: 'COLBY'\n",
            "\n",
            "üìà Epoch 80/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.52it/s, loss=0.062, word_acc=0.900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 80 Summary:\n",
            "  Training Loss: 0.0174\n",
            "  Training Word Acc: 0.9445\n",
            "  Validation Word Acc: 0.7120\n",
            "  Validation Char Acc: 0.8147\n",
            "  Learning Rate: 0.000768\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'JULY' | Pred: 'JUNY'\n",
            "  True: 'COM' | Pred: 'COM'\n",
            "  True: 'SURVEY' | Pred: 'SURVEY'\n",
            "  True: 'JACKASS' | Pred: 'IACKASS'\n",
            "  True: 'YOURE' | Pred: 'YOURE'\n",
            "\n",
            "üìà Epoch 81/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.71it/s, loss=0.002, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 81 Summary:\n",
            "  Training Loss: 0.0124\n",
            "  Training Word Acc: 0.9805\n",
            "  Validation Word Acc: 0.7440\n",
            "  Validation Char Acc: 0.8333\n",
            "  Learning Rate: 0.000763\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'COTTAGE' | Pred: 'COTTAGE'\n",
            "  True: 'COLBY' | Pred: 'COLBY'\n",
            "  True: 'LOADING' | Pred: 'DADING'\n",
            "  True: 'ONLINE' | Pred: 'ONLINE'\n",
            "  True: '22' | Pred: '28'\n",
            "\n",
            "üìà Epoch 82/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:09<00:00, 15.20it/s, loss=0.003, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 82 Summary:\n",
            "  Training Loss: 0.0140\n",
            "  Training Word Acc: 0.9844\n",
            "  Validation Word Acc: 0.7100\n",
            "  Validation Char Acc: 0.8262\n",
            "  Learning Rate: 0.000757\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'VISTA' | Pred: 'VISTA'\n",
            "  True: '01922' | Pred: '01922'\n",
            "  True: 'ALERT' | Pred: 'ALERT'\n",
            "  True: 'STATE' | Pred: 'STATE'\n",
            "  True: 'SALT' | Pred: 'SALT'\n",
            "\n",
            "üìà Epoch 83/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.03it/s, loss=0.003, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 83 Summary:\n",
            "  Training Loss: 0.0113\n",
            "  Training Word Acc: 0.9883\n",
            "  Validation Word Acc: 0.7240\n",
            "  Validation Char Acc: 0.8245\n",
            "  Learning Rate: 0.000752\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'SCHEMES' | Pred: 'SCHEMES'\n",
            "  True: 'SHOP' | Pred: 'SHOP'\n",
            "  True: 'AN' | Pred: 'AN'\n",
            "  True: 'INDIAN' | Pred: 'INDIAN'\n",
            "  True: 'UNE' | Pred: 'UNE'\n",
            "\n",
            "üìà Epoch 84/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.30it/s, loss=0.012, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 84 Summary:\n",
            "  Training Loss: 0.0094\n",
            "  Training Word Acc: 0.9781\n",
            "  Validation Word Acc: 0.6980\n",
            "  Validation Char Acc: 0.8202\n",
            "  Learning Rate: 0.000746\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'THIS' | Pred: 'THIS'\n",
            "  True: 'TELL' | Pred: 'TEIL'\n",
            "  True: 'NOSTOPPING' | Pred: 'NOSTOPPINO'\n",
            "  True: 'CARING' | Pred: 'CARING'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "\n",
            "üìà Epoch 85/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.99it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 85 Summary:\n",
            "  Training Loss: 0.0097\n",
            "  Training Word Acc: 0.9922\n",
            "  Validation Word Acc: 0.7400\n",
            "  Validation Char Acc: 0.8285\n",
            "  Learning Rate: 0.000741\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'SCHEMES' | Pred: 'SCHEMES'\n",
            "  True: 'CITY' | Pred: 'CITY'\n",
            "  True: 'CALL' | Pred: 'CALL'\n",
            "  True: 'RESET' | Pred: 'RESET'\n",
            "  True: 'SAW' | Pred: 'SAW'\n",
            "\n",
            "üìà Epoch 86/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.22it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 86 Summary:\n",
            "  Training Loss: 0.0072\n",
            "  Training Word Acc: 0.9844\n",
            "  Validation Word Acc: 0.7280\n",
            "  Validation Char Acc: 0.8162\n",
            "  Learning Rate: 0.000735\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'INDIAN' | Pred: 'INDIAN'\n",
            "  True: 'COM' | Pred: 'COM'\n",
            "  True: 'NEWS' | Pred: 'NEWS'\n",
            "  True: 'FOSTER' | Pred: 'FOSTER'\n",
            "  True: 'PRODAJA' | Pred: 'PRODAJA'\n",
            "\n",
            "üìà Epoch 87/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.06it/s, loss=0.022, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 87 Summary:\n",
            "  Training Loss: 0.0243\n",
            "  Training Word Acc: 0.9664\n",
            "  Validation Word Acc: 0.7100\n",
            "  Validation Char Acc: 0.8083\n",
            "  Learning Rate: 0.000730\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'EASTWOOD' | Pred: 'EASTHOOD'\n",
            "  True: 'PENANG' | Pred: 'PENANG'\n",
            "  True: 'HOUSE' | Pred: 'HOUSE'\n",
            "  True: 'IS' | Pred: 'IS'\n",
            "  True: 'CUSTOMERS' | Pred: 'CISTOMIERS'\n",
            "\n",
            "üìà Epoch 88/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.15it/s, loss=0.007, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 88 Summary:\n",
            "  Training Loss: 0.0207\n",
            "  Training Word Acc: 0.9727\n",
            "  Validation Word Acc: 0.7320\n",
            "  Validation Char Acc: 0.8174\n",
            "  Learning Rate: 0.000724\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'GERARO' | Pred: 'GEHPVNM'\n",
            "  True: 'EXIT' | Pred: 'EXIT'\n",
            "  True: 'LOADING' | Pred: 'LAING'\n",
            "  True: '1989' | Pred: '1989'\n",
            "  True: 'OF' | Pred: 'OF'\n",
            "\n",
            "üìà Epoch 89/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.88it/s, loss=0.010, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 89 Summary:\n",
            "  Training Loss: 0.0115\n",
            "  Training Word Acc: 0.9883\n",
            "  Validation Word Acc: 0.7240\n",
            "  Validation Char Acc: 0.8322\n",
            "  Learning Rate: 0.000719\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '2' | Pred: '2'\n",
            "  True: 'RD' | Pred: 'RD'\n",
            "  True: 'IN' | Pred: 'IN'\n",
            "  True: 'JONES' | Pred: 'JONES'\n",
            "  True: 'CANDIDATE' | Pred: 'CANDIOATE'\n",
            "\n",
            "üìà Epoch 90/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.08it/s, loss=0.002, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 90 Summary:\n",
            "  Training Loss: 0.0182\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7220\n",
            "  Validation Char Acc: 0.8283\n",
            "  Learning Rate: 0.000713\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'AN' | Pred: 'AN'\n",
            "  True: 'THAT' | Pred: 'THAT'\n",
            "  True: 'PULP' | Pred: 'PULP'\n",
            "  True: 'TRINITYLEEDS' | Pred: 'TUMTYLECRIS'\n",
            "  True: 'JOSHUA' | Pred: 'JOSHUA'\n",
            "\n",
            "üìà Epoch 91/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.10it/s, loss=0.023, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 91 Summary:\n",
            "  Training Loss: 0.0089\n",
            "  Training Word Acc: 0.9898\n",
            "  Validation Word Acc: 0.7160\n",
            "  Validation Char Acc: 0.8169\n",
            "  Learning Rate: 0.000707\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'YOU' | Pred: 'YOU'\n",
            "  True: 'YOUR' | Pred: 'YOUR'\n",
            "  True: 'INDIASBIN' | Pred: 'INDTAISBIN'\n",
            "  True: 'MOTION' | Pred: 'MOTION'\n",
            "  True: '1800BUYKWIK' | Pred: 'HOBBUYTWIK'\n",
            "\n",
            "üìà Epoch 92/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.01it/s, loss=0.032, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 92 Summary:\n",
            "  Training Loss: 0.0107\n",
            "  Training Word Acc: 0.9820\n",
            "  Validation Word Acc: 0.7180\n",
            "  Validation Char Acc: 0.8203\n",
            "  Learning Rate: 0.000701\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'WINSLET' | Pred: 'WINSLET'\n",
            "  True: 'JACKASS' | Pred: 'IACKASS'\n",
            "  True: 'BOARD' | Pred: 'BOARD'\n",
            "  True: 'HOMICIDE' | Pred: 'HOMICIDE'\n",
            "  True: 'ATMOSPHERE' | Pred: 'AVWGEPNER'\n",
            "\n",
            "üìà Epoch 93/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.82it/s, loss=0.007, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 93 Summary:\n",
            "  Training Loss: 0.0087\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7240\n",
            "  Validation Char Acc: 0.8143\n",
            "  Learning Rate: 0.000696\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'COM' | Pred: 'COM'\n",
            "  True: '4865' | Pred: '4365'\n",
            "  True: 'TELL' | Pred: 'TEIL'\n",
            "  True: 'DRIVE' | Pred: 'DRIAE'\n",
            "  True: 'SWEET' | Pred: 'SWEEY'\n",
            "\n",
            "üìà Epoch 94/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.95it/s, loss=0.057, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 94 Summary:\n",
            "  Training Loss: 0.0101\n",
            "  Training Word Acc: 0.9469\n",
            "  Validation Word Acc: 0.7380\n",
            "  Validation Char Acc: 0.8199\n",
            "  Learning Rate: 0.000690\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'WISHES' | Pred: 'WISHES'\n",
            "  True: 'FIRE' | Pred: 'FIRE'\n",
            "  True: 'HEFNER' | Pred: 'HEFNER'\n",
            "  True: '7' | Pred: '7'\n",
            "  True: 'DAY' | Pred: 'LZCH'\n",
            "\n",
            "üìà Epoch 95/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.09it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 95 Summary:\n",
            "  Training Loss: 0.0075\n",
            "  Training Word Acc: 0.9844\n",
            "  Validation Word Acc: 0.7200\n",
            "  Validation Char Acc: 0.8222\n",
            "  Learning Rate: 0.000684\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'WE' | Pred: 'WE'\n",
            "  True: 'TAKE' | Pred: 'TAKE'\n",
            "  True: 'DAY' | Pred: '1ZH'\n",
            "  True: '50' | Pred: '50'\n",
            "  True: 'HOUSE' | Pred: 'HOLSE'\n",
            "\n",
            "üìà Epoch 96/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.03it/s, loss=0.010, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 96 Summary:\n",
            "  Training Loss: 0.0074\n",
            "  Training Word Acc: 0.9898\n",
            "  Validation Word Acc: 0.7520\n",
            "  Validation Char Acc: 0.8374\n",
            "  Learning Rate: 0.000678\n",
            "  ‚úÖ New best model saved! Word Acc: 0.7520\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'TEXT' | Pred: 'TEXT'\n",
            "  True: 'TWITTERCOMAPLUSK' | Pred: 'MTECOMLEPUIST'\n",
            "  True: 'SURREY' | Pred: 'SUREY'\n",
            "  True: 'GREM' | Pred: 'GREM'\n",
            "  True: 'ISTANBUL' | Pred: 'ISTANBUL'\n",
            "\n",
            "üìà Epoch 97/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.72it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 97 Summary:\n",
            "  Training Loss: 0.0056\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7540\n",
            "  Validation Char Acc: 0.8389\n",
            "  Learning Rate: 0.000672\n",
            "  ‚úÖ New best model saved! Word Acc: 0.7540\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '50' | Pred: '50'\n",
            "  True: 'BUY' | Pred: 'BIY'\n",
            "  True: 'ABOUT' | Pred: 'ABOUT'\n",
            "  True: '60' | Pred: '60'\n",
            "  True: 'CANDIDATE' | Pred: 'CANDIDATE'\n",
            "\n",
            "üìà Epoch 98/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.09it/s, loss=0.002, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 98 Summary:\n",
            "  Training Loss: 0.0038\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7340\n",
            "  Validation Char Acc: 0.8312\n",
            "  Learning Rate: 0.000666\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PACK' | Pred: 'PACK'\n",
            "  True: 'CRITICS' | Pred: 'CRITICS'\n",
            "  True: 'AS' | Pred: 'AS'\n",
            "  True: '02' | Pred: '02'\n",
            "  True: 'HANDY' | Pred: 'HAND'\n",
            "\n",
            "üìà Epoch 99/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.79it/s, loss=0.002, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 99 Summary:\n",
            "  Training Loss: 0.0054\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7240\n",
            "  Validation Char Acc: 0.8168\n",
            "  Learning Rate: 0.000660\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'FRUIT' | Pred: 'FUTIT'\n",
            "  True: 'INDIA' | Pred: 'IDIA'\n",
            "  True: '1800GOGEICO' | Pred: '30DGDGECO'\n",
            "  True: 'PRICES' | Pred: 'PRICES'\n",
            "  True: 'DI' | Pred: 'DI'\n",
            "\n",
            "üìà Epoch 100/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.98it/s, loss=0.026, word_acc=0.900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 100 Summary:\n",
            "  Training Loss: 0.0144\n",
            "  Training Word Acc: 0.9758\n",
            "  Validation Word Acc: 0.7100\n",
            "  Validation Char Acc: 0.8137\n",
            "  Learning Rate: 0.000655\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ISTANBUL' | Pred: 'ISTANGUL'\n",
            "  True: 'WITH' | Pred: 'WITH'\n",
            "  True: 'FOTOLIA' | Pred: 'TOTOLA'\n",
            "  True: 'SANTOMIC' | Pred: 'SANTOMIC'\n",
            "  True: 'MOOD' | Pred: 'MOOD'\n",
            "\n",
            "üìà Epoch 101/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.42it/s, loss=0.005, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 101 Summary:\n",
            "  Training Loss: 0.0243\n",
            "  Training Word Acc: 0.9805\n",
            "  Validation Word Acc: 0.7340\n",
            "  Validation Char Acc: 0.8209\n",
            "  Learning Rate: 0.000649\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'JUKEBOX' | Pred: 'JUKEBOX'\n",
            "  True: 'PITT' | Pred: 'PITT'\n",
            "  True: 'TIMES' | Pred: 'TIMES'\n",
            "  True: 'MATRIX' | Pred: 'MATRIX'\n",
            "  True: 'WISHES' | Pred: 'WISHES'\n",
            "\n",
            "üìà Epoch 102/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.35it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 102 Summary:\n",
            "  Training Loss: 0.0149\n",
            "  Training Word Acc: 0.9609\n",
            "  Validation Word Acc: 0.7460\n",
            "  Validation Char Acc: 0.8225\n",
            "  Learning Rate: 0.000643\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'SAW' | Pred: 'SALL'\n",
            "  True: 'RIKHYS' | Pred: 'MKMN'\n",
            "  True: 'DIRECTION' | Pred: 'DIRECTION'\n",
            "  True: 'NURSERY' | Pred: 'SORSRER'\n",
            "  True: 'OF' | Pred: 'OF'\n",
            "\n",
            "üìà Epoch 103/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.97it/s, loss=0.002, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 103 Summary:\n",
            "  Training Loss: 0.0076\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7260\n",
            "  Validation Char Acc: 0.8212\n",
            "  Learning Rate: 0.000636\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'COTTAGE' | Pred: 'COTIAGE'\n",
            "  True: '7' | Pred: '7'\n",
            "  True: 'HENREID' | Pred: 'HENREID'\n",
            "  True: 'VIEWERS' | Pred: 'VIEWERS'\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "\n",
            "üìà Epoch 104/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.87it/s, loss=0.010, word_acc=0.900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 104 Summary:\n",
            "  Training Loss: 0.0048\n",
            "  Training Word Acc: 0.9797\n",
            "  Validation Word Acc: 0.7240\n",
            "  Validation Char Acc: 0.8202\n",
            "  Learning Rate: 0.000630\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'GLOBAL' | Pred: 'GLOBAL'\n",
            "  True: 'JURASSIC' | Pred: 'JURAGSIC'\n",
            "  True: 'NOVEMBER' | Pred: 'NOVEMBER'\n",
            "  True: 'SQUIRRELS' | Pred: 'FOLUNRRPAE'\n",
            "  True: 'TOM' | Pred: 'TO'\n",
            "\n",
            "üìà Epoch 105/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.95it/s, loss=0.002, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 105 Summary:\n",
            "  Training Loss: 0.0110\n",
            "  Training Word Acc: 0.9883\n",
            "  Validation Word Acc: 0.7100\n",
            "  Validation Char Acc: 0.8279\n",
            "  Learning Rate: 0.000624\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'TITANS' | Pred: 'TITANS'\n",
            "  True: 'DAY' | Pred: 'DAY'\n",
            "  True: 'SHOESTRING' | Pred: 'HLOWOTREE'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'OLD' | Pred: 'OTLO'\n",
            "\n",
            "üìà Epoch 106/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.64it/s, loss=0.007, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 106 Summary:\n",
            "  Training Loss: 0.0089\n",
            "  Training Word Acc: 0.9781\n",
            "  Validation Word Acc: 0.7320\n",
            "  Validation Char Acc: 0.8173\n",
            "  Learning Rate: 0.000618\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PHOENIX' | Pred: 'PHOENX'\n",
            "  True: 'COLOURS' | Pred: 'COLARS'\n",
            "  True: 'ANSON' | Pred: 'ANSON'\n",
            "  True: 'HEIGHT' | Pred: 'HEIGHT'\n",
            "  True: 'BANK' | Pred: 'BANE'\n",
            "\n",
            "üìà Epoch 107/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:09<00:00, 15.00it/s, loss=0.006, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 107 Summary:\n",
            "  Training Loss: 0.0077\n",
            "  Training Word Acc: 0.9922\n",
            "  Validation Word Acc: 0.7500\n",
            "  Validation Char Acc: 0.8311\n",
            "  Learning Rate: 0.000612\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'DANGERS' | Pred: 'DANGERS'\n",
            "  True: '63' | Pred: '63'\n",
            "  True: 'NEW' | Pred: 'NEW'\n",
            "  True: 'TAKE' | Pred: 'TAKE'\n",
            "  True: 'WORTHINGTON' | Pred: 'IEEILIIETLL'\n",
            "\n",
            "üìà Epoch 108/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:09<00:00, 15.67it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 108 Summary:\n",
            "  Training Loss: 0.0043\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8329\n",
            "  Learning Rate: 0.000606\n",
            "  ‚úÖ New best model saved! Word Acc: 0.7580\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'RESIST' | Pred: 'HESIST'\n",
            "  True: 'YOUR' | Pred: 'YOUR'\n",
            "  True: 'WITH' | Pred: 'WITH'\n",
            "  True: 'A' | Pred: 'A'\n",
            "  True: 'ROUTH' | Pred: 'ROUTH'\n",
            "\n",
            "üìà Epoch 109/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.04it/s, loss=0.002, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 109 Summary:\n",
            "  Training Loss: 0.0064\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7360\n",
            "  Validation Char Acc: 0.8262\n",
            "  Learning Rate: 0.000600\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '212' | Pred: '212'\n",
            "  True: 'COUNTRY' | Pred: 'COUNTRY'\n",
            "  True: '02' | Pred: '02'\n",
            "  True: 'ANGELS' | Pred: 'ANGELS'\n",
            "  True: 'NOSTOPPING' | Pred: 'NOSTOPPING'\n",
            "\n",
            "üìà Epoch 110/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.79it/s, loss=0.006, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 110 Summary:\n",
            "  Training Loss: 0.0106\n",
            "  Training Word Acc: 0.9883\n",
            "  Validation Word Acc: 0.7240\n",
            "  Validation Char Acc: 0.8234\n",
            "  Learning Rate: 0.000594\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'EXIT' | Pred: 'EXIT'\n",
            "  True: 'THE' | Pred: 'THO'\n",
            "  True: 'NAME' | Pred: 'NAME'\n",
            "  True: 'AND' | Pred: 'AND'\n",
            "  True: 'OF' | Pred: 'OF'\n",
            "\n",
            "üìà Epoch 111/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.98it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 111 Summary:\n",
            "  Training Loss: 0.0071\n",
            "  Training Word Acc: 0.9883\n",
            "  Validation Word Acc: 0.7320\n",
            "  Validation Char Acc: 0.8404\n",
            "  Learning Rate: 0.000588\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'COTTAGE' | Pred: 'COTTAGE'\n",
            "  True: 'HITAM' | Pred: 'HITAM'\n",
            "  True: 'STATION' | Pred: 'STATION'\n",
            "  True: '63' | Pred: '63'\n",
            "  True: 'GO' | Pred: 'GO'\n",
            "\n",
            "üìà Epoch 112/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.98it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 112 Summary:\n",
            "  Training Loss: 0.0063\n",
            "  Training Word Acc: 0.9883\n",
            "  Validation Word Acc: 0.7320\n",
            "  Validation Char Acc: 0.8414\n",
            "  Learning Rate: 0.000581\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'A' | Pred: 'A'\n",
            "  True: 'RESET' | Pred: 'RESET'\n",
            "  True: 'TWILIGHT' | Pred: 'TWILIGHT'\n",
            "  True: 'HENREID' | Pred: 'HENREID'\n",
            "  True: 'HOURS' | Pred: 'HOURS'\n",
            "\n",
            "üìà Epoch 113/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.88it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 113 Summary:\n",
            "  Training Loss: 0.0057\n",
            "  Training Word Acc: 0.9883\n",
            "  Validation Word Acc: 0.7460\n",
            "  Validation Char Acc: 0.8425\n",
            "  Learning Rate: 0.000575\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'OFFICE' | Pred: 'OFFICE'\n",
            "  True: 'RD' | Pred: 'RD'\n",
            "  True: 'TAKE' | Pred: 'TAKE'\n",
            "  True: 'INVERCARGILL' | Pred: 'INVEREARGIL'\n",
            "  True: 'HELT' | Pred: 'HELT'\n",
            "\n",
            "üìà Epoch 114/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.82it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 114 Summary:\n",
            "  Training Loss: 0.0035\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7380\n",
            "  Validation Char Acc: 0.8356\n",
            "  Learning Rate: 0.000569\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'TELL' | Pred: 'TEIL'\n",
            "  True: 'ONLY' | Pred: 'ONLY'\n",
            "  True: 'LOW' | Pred: 'LOW'\n",
            "  True: 'LOSE' | Pred: 'ZOSE'\n",
            "  True: 'SALT' | Pred: 'SALT'\n",
            "\n",
            "üìà Epoch 115/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.60it/s, loss=0.014, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 115 Summary:\n",
            "  Training Loss: 0.0076\n",
            "  Training Word Acc: 0.9898\n",
            "  Validation Word Acc: 0.7360\n",
            "  Validation Char Acc: 0.8358\n",
            "  Learning Rate: 0.000563\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'IN' | Pred: 'IN'\n",
            "  True: 'EXIT' | Pred: 'EXIT'\n",
            "  True: 'INDIA' | Pred: 'MNAIA'\n",
            "  True: 'WELCOME' | Pred: 'WELCOME'\n",
            "  True: 'BANKNORTH' | Pred: 'BANKNORIH'\n",
            "\n",
            "üìà Epoch 116/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.19it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 116 Summary:\n",
            "  Training Loss: 0.0075\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7340\n",
            "  Validation Char Acc: 0.8324\n",
            "  Learning Rate: 0.000556\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BUY' | Pred: 'BIY'\n",
            "  True: 'GLOBAL' | Pred: 'GLOBAL'\n",
            "  True: 'NOT' | Pred: 'NOT'\n",
            "  True: 'EVERY' | Pred: 'EVEY'\n",
            "  True: '24' | Pred: '24'\n",
            "\n",
            "üìà Epoch 117/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.16it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 117 Summary:\n",
            "  Training Loss: 0.0086\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7400\n",
            "  Validation Char Acc: 0.8315\n",
            "  Learning Rate: 0.000550\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BLUBBER' | Pred: 'BLUBBER'\n",
            "  True: 'ADOPTION' | Pred: 'ADOPTION'\n",
            "  True: 'ANGELS' | Pred: 'ANGELS'\n",
            "  True: 'ONLY' | Pred: 'ONLY'\n",
            "  True: 'NOW' | Pred: 'NOW'\n",
            "\n",
            "üìà Epoch 118/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.60it/s, loss=0.021, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 118 Summary:\n",
            "  Training Loss: 0.0069\n",
            "  Training Word Acc: 0.9898\n",
            "  Validation Word Acc: 0.7440\n",
            "  Validation Char Acc: 0.8366\n",
            "  Learning Rate: 0.000544\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'EXIT' | Pred: 'EXIT'\n",
            "  True: 'STATE' | Pred: 'STATE'\n",
            "  True: 'QUEENSWAY' | Pred: 'CUEENSWOY'\n",
            "  True: 'COM' | Pred: 'COM'\n",
            "  True: 'HANDY' | Pred: 'HANDY'\n",
            "\n",
            "üìà Epoch 119/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.60it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 119 Summary:\n",
            "  Training Loss: 0.0097\n",
            "  Training Word Acc: 0.9883\n",
            "  Validation Word Acc: 0.7260\n",
            "  Validation Char Acc: 0.8223\n",
            "  Learning Rate: 0.000538\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'THEYRE' | Pred: 'THEYRE'\n",
            "  True: 'JUKEBOX' | Pred: 'JUKEBOX'\n",
            "  True: '120' | Pred: '1730'\n",
            "  True: '212' | Pred: '212'\n",
            "  True: 'HOMICIDE' | Pred: 'HOMICIDE'\n",
            "\n",
            "üìà Epoch 120/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.99it/s, loss=0.004, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 120 Summary:\n",
            "  Training Loss: 0.0073\n",
            "  Training Word Acc: 0.9844\n",
            "  Validation Word Acc: 0.7260\n",
            "  Validation Char Acc: 0.8308\n",
            "  Learning Rate: 0.000531\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'SPACE' | Pred: 'SPACE'\n",
            "  True: 'SAJITH' | Pred: 'SAJITH'\n",
            "  True: 'LOVE' | Pred: 'DO'\n",
            "  True: 'THINK' | Pred: 'THINK'\n",
            "  True: 'STORY' | Pred: 'STORY'\n",
            "\n",
            "üìà Epoch 121/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.09it/s, loss=0.029, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 121 Summary:\n",
            "  Training Loss: 0.0041\n",
            "  Training Word Acc: 0.9859\n",
            "  Validation Word Acc: 0.7300\n",
            "  Validation Char Acc: 0.8317\n",
            "  Learning Rate: 0.000525\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'MURDER' | Pred: 'MLRDER'\n",
            "  True: 'YOU' | Pred: 'YOU'\n",
            "  True: 'JALAN' | Pred: 'JALAN'\n",
            "  True: 'HOUSE' | Pred: 'HOUSE'\n",
            "  True: 'DENZEL' | Pred: 'DENZEL'\n",
            "\n",
            "üìà Epoch 122/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.61it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 122 Summary:\n",
            "  Training Loss: 0.0021\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7380\n",
            "  Validation Char Acc: 0.8267\n",
            "  Learning Rate: 0.000519\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BLOOM' | Pred: 'OBIODON'\n",
            "  True: 'CUSTOMERS' | Pred: 'CISTOMMERS'\n",
            "  True: 'ROUTH' | Pred: 'POUTH'\n",
            "  True: 'CURRENCY' | Pred: 'CRREREY'\n",
            "  True: 'ANSON' | Pred: 'ANSON'\n",
            "\n",
            "üìà Epoch 123/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.74it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 123 Summary:\n",
            "  Training Loss: 0.0012\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7380\n",
            "  Validation Char Acc: 0.8321\n",
            "  Learning Rate: 0.000513\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'IMAIZUMI' | Pred: 'WGLUMI'\n",
            "  True: 'STICKY' | Pred: 'STUGSY'\n",
            "  True: '208' | Pred: '208'\n",
            "  True: 'ANSON' | Pred: 'ANSON'\n",
            "  True: 'ON' | Pred: 'ON'\n",
            "\n",
            "üìà Epoch 124/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.19it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 124 Summary:\n",
            "  Training Loss: 0.0008\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7400\n",
            "  Validation Char Acc: 0.8334\n",
            "  Learning Rate: 0.000506\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'LA' | Pred: 'T'\n",
            "  True: 'REVERSE' | Pred: 'REVERSE'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'SALT' | Pred: 'SALT'\n",
            "  True: 'WINSLET' | Pred: 'WINSLET'\n",
            "\n",
            "üìà Epoch 125/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.14it/s, loss=0.003, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 125 Summary:\n",
            "  Training Loss: 0.0007\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7400\n",
            "  Validation Char Acc: 0.8391\n",
            "  Learning Rate: 0.000500\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'OF' | Pred: 'OF'\n",
            "  True: 'ALERT' | Pred: 'ALERT'\n",
            "  True: 'HOMICIDE' | Pred: 'HOMICIDE'\n",
            "  True: 'BANK' | Pred: 'BANK'\n",
            "  True: 'THE' | Pred: 'THO'\n",
            "\n",
            "üìà Epoch 126/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.41it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 126 Summary:\n",
            "  Training Loss: 0.0006\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7440\n",
            "  Validation Char Acc: 0.8410\n",
            "  Learning Rate: 0.000494\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'A' | Pred: 'A'\n",
            "  True: '24' | Pred: '24'\n",
            "  True: 'HOUSE' | Pred: 'HOUSE'\n",
            "  True: 'SURVEY' | Pred: 'SURVEY'\n",
            "  True: 'BADGES' | Pred: 'BADGES'\n",
            "\n",
            "üìà Epoch 127/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 17.01it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 127 Summary:\n",
            "  Training Loss: 0.0006\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7500\n",
            "  Validation Char Acc: 0.8337\n",
            "  Learning Rate: 0.000487\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '280' | Pred: '280'\n",
            "  True: 'COLOURS' | Pred: 'COLONRS'\n",
            "  True: 'HOME' | Pred: 'SOME'\n",
            "  True: 'TERRACE' | Pred: 'TERRACE'\n",
            "  True: 'TAXI' | Pred: 'TAXL'\n",
            "\n",
            "üìà Epoch 128/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.27it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 128 Summary:\n",
            "  Training Loss: 0.0007\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7560\n",
            "  Validation Char Acc: 0.8431\n",
            "  Learning Rate: 0.000481\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "  True: 'VIEW' | Pred: 'VIEW'\n",
            "  True: 'DIRECTION' | Pred: 'DIRECTION'\n",
            "  True: 'VEGETARIAN' | Pred: 'UECSTARIAN'\n",
            "  True: 'GASES' | Pred: 'GASES'\n",
            "\n",
            "üìà Epoch 129/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.13it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 129 Summary:\n",
            "  Training Loss: 0.0011\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7480\n",
            "  Validation Char Acc: 0.8307\n",
            "  Learning Rate: 0.000475\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BPL' | Pred: 'BPL'\n",
            "  True: 'JUBILEE' | Pred: 'JUBILEE'\n",
            "  True: 'WISTERIA' | Pred: 'HCTRRA'\n",
            "  True: 'METER' | Pred: 'METER'\n",
            "  True: '2' | Pred: 'A'\n",
            "\n",
            "üìà Epoch 130/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.37it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 130 Summary:\n",
            "  Training Loss: 0.0008\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7320\n",
            "  Validation Char Acc: 0.8391\n",
            "  Learning Rate: 0.000469\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BLUBBER' | Pred: 'BLUBBER'\n",
            "  True: 'BLOOM' | Pred: 'OBIOON'\n",
            "  True: 'VIJAYAWADA' | Pred: 'VAAWON'\n",
            "  True: 'THINK' | Pred: 'THINK'\n",
            "  True: 'FAILTE' | Pred: 'TAILTE'\n",
            "\n",
            "üìà Epoch 131/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.80it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 131 Summary:\n",
            "  Training Loss: 0.0077\n",
            "  Training Word Acc: 0.9922\n",
            "  Validation Word Acc: 0.7140\n",
            "  Validation Char Acc: 0.8125\n",
            "  Learning Rate: 0.000462\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'WASHINGTON' | Pred: 'WASHINGION'\n",
            "  True: 'ASIA' | Pred: 'ASIA'\n",
            "  True: 'MIRREN' | Pred: 'MIRREN'\n",
            "  True: '3D' | Pred: 'BR'\n",
            "  True: 'BANKNORTH' | Pred: 'BANKNORIH'\n",
            "\n",
            "üìà Epoch 132/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:09<00:00, 15.19it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 132 Summary:\n",
            "  Training Loss: 0.0099\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7060\n",
            "  Validation Char Acc: 0.8078\n",
            "  Learning Rate: 0.000456\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'LEAGUE' | Pred: 'LEAGUE'\n",
            "  True: 'HOUSE' | Pred: 'HOUSE'\n",
            "  True: 'COMPARISON' | Pred: 'COMPARISON'\n",
            "  True: 'DANGERS' | Pred: 'DANGERS'\n",
            "  True: '317' | Pred: '317'\n",
            "\n",
            "üìà Epoch 133/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.01it/s, loss=0.003, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 133 Summary:\n",
            "  Training Loss: 0.0072\n",
            "  Training Word Acc: 0.9844\n",
            "  Validation Word Acc: 0.7420\n",
            "  Validation Char Acc: 0.8339\n",
            "  Learning Rate: 0.000450\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '3D' | Pred: '8R'\n",
            "  True: 'FEW' | Pred: 'FEW'\n",
            "  True: 'STATE' | Pred: 'STATE'\n",
            "  True: 'CREATE' | Pred: 'CREATE'\n",
            "  True: 'PROVINCIAL' | Pred: 'PROVINEIAI'\n",
            "\n",
            "üìà Epoch 134/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.54it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 134 Summary:\n",
            "  Training Loss: 0.0045\n",
            "  Training Word Acc: 0.9883\n",
            "  Validation Word Acc: 0.7300\n",
            "  Validation Char Acc: 0.8272\n",
            "  Learning Rate: 0.000444\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '19' | Pred: '1'\n",
            "  True: 'MIRREN' | Pred: 'MIRREN'\n",
            "  True: 'SOLVE' | Pred: 'SOLVE'\n",
            "  True: 'OF' | Pred: 'OF'\n",
            "  True: 'HOME' | Pred: 'HOME'\n",
            "\n",
            "üìà Epoch 135/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.74it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 135 Summary:\n",
            "  Training Loss: 0.0050\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7180\n",
            "  Validation Char Acc: 0.8220\n",
            "  Learning Rate: 0.000437\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'FARM' | Pred: 'FARM'\n",
            "  True: 'INDIA' | Pred: 'INALA'\n",
            "  True: '208' | Pred: '208'\n",
            "  True: 'ILL' | Pred: 'ILL'\n",
            "\n",
            "üìà Epoch 136/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.00it/s, loss=0.002, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 136 Summary:\n",
            "  Training Loss: 0.0089\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7280\n",
            "  Validation Char Acc: 0.8222\n",
            "  Learning Rate: 0.000431\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'COM' | Pred: 'COM'\n",
            "  True: 'STORES' | Pred: 'STORES'\n",
            "  True: 'SREELAKSHMI' | Pred: 'SRINTARKSSINI'\n",
            "  True: 'SETH' | Pred: 'CALW'\n",
            "  True: 'NEW' | Pred: 'NEW'\n",
            "\n",
            "üìà Epoch 137/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.05it/s, loss=0.002, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 137 Summary:\n",
            "  Training Loss: 0.0059\n",
            "  Training Word Acc: 0.9922\n",
            "  Validation Word Acc: 0.7360\n",
            "  Validation Char Acc: 0.8306\n",
            "  Learning Rate: 0.000425\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'EXIT' | Pred: 'EXIT'\n",
            "  True: 'AUR' | Pred: 'AUR'\n",
            "  True: 'COMPARISON' | Pred: 'COMPARISON'\n",
            "  True: 'WAS' | Pred: 'WAS'\n",
            "  True: 'BE' | Pred: 'BE'\n",
            "\n",
            "üìà Epoch 138/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.00it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 138 Summary:\n",
            "  Training Loss: 0.0028\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7340\n",
            "  Validation Char Acc: 0.8371\n",
            "  Learning Rate: 0.000419\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'CATE' | Pred: 'CATE'\n",
            "  True: 'MELIN' | Pred: 'MELIN'\n",
            "  True: '24' | Pred: '24'\n",
            "  True: 'ONLY' | Pred: 'ONLY'\n",
            "  True: 'VIJAYAWADA' | Pred: 'VAYAMKOR'\n",
            "\n",
            "üìà Epoch 139/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.59it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 139 Summary:\n",
            "  Training Loss: 0.0024\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7300\n",
            "  Validation Char Acc: 0.8378\n",
            "  Learning Rate: 0.000412\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'EARTHSELLERS' | Pred: 'EARTHSEIES'\n",
            "  True: '50' | Pred: '250'\n",
            "  True: 'GERARO' | Pred: 'GEBBID'\n",
            "  True: 'DIRECTLY' | Pred: 'DIRECTLY'\n",
            "  True: 'HALE' | Pred: 'FALE'\n",
            "\n",
            "üìà Epoch 140/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.86it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 140 Summary:\n",
            "  Training Loss: 0.0043\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7460\n",
            "  Validation Char Acc: 0.8304\n",
            "  Learning Rate: 0.000406\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'GO' | Pred: 'ED'\n",
            "  True: 'BLOOM' | Pred: 'OBIOOM'\n",
            "  True: 'CREEK' | Pred: 'CREEX'\n",
            "  True: 'CURRENTLY' | Pred: 'CURRENTLY'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "\n",
            "üìà Epoch 141/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.84it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 141 Summary:\n",
            "  Training Loss: 0.0019\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7380\n",
            "  Validation Char Acc: 0.8295\n",
            "  Learning Rate: 0.000400\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'NEW' | Pred: 'NEW'\n",
            "  True: 'STATE' | Pred: 'STATE'\n",
            "  True: 'IMAIZUMI' | Pred: 'WGLPUMI'\n",
            "  True: 'THINK' | Pred: 'THINK'\n",
            "  True: 'COOPER' | Pred: 'COOPER'\n",
            "\n",
            "üìà Epoch 142/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.82it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 142 Summary:\n",
            "  Training Loss: 0.0009\n",
            "  Training Word Acc: 0.9922\n",
            "  Validation Word Acc: 0.7320\n",
            "  Validation Char Acc: 0.8360\n",
            "  Learning Rate: 0.000394\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'YOUR' | Pred: 'YOUR'\n",
            "  True: 'METER' | Pred: 'METER'\n",
            "  True: '1800GOGEICO' | Pred: '30DGDGECO'\n",
            "  True: 'SQUIRRELS' | Pred: 'FOAUNRRELS'\n",
            "  True: 'CHANDIGARH' | Pred: 'CHANDIGARI'\n",
            "\n",
            "üìà Epoch 143/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.54it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 143 Summary:\n",
            "  Training Loss: 0.0008\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7380\n",
            "  Validation Char Acc: 0.8380\n",
            "  Learning Rate: 0.000388\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'JUKEBOX' | Pred: 'JUKEBOX'\n",
            "  True: 'CIUDAD' | Pred: 'CIUDAD'\n",
            "  True: 'A' | Pred: 'A'\n",
            "  True: 'COLOURS' | Pred: 'CLONS'\n",
            "  True: 'FOSTER' | Pred: 'FOSTER'\n",
            "\n",
            "üìà Epoch 144/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.97it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 144 Summary:\n",
            "  Training Loss: 0.0007\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7420\n",
            "  Validation Char Acc: 0.8358\n",
            "  Learning Rate: 0.000382\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ARK' | Pred: 'BKK'\n",
            "  True: 'CITY' | Pred: 'CITY'\n",
            "  True: 'AUGUST' | Pred: 'AUGUST'\n",
            "  True: 'TOTAL' | Pred: 'TOTAL'\n",
            "  True: 'WAS' | Pred: 'WAS'\n",
            "\n",
            "üìà Epoch 145/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.87it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 145 Summary:\n",
            "  Training Loss: 0.0005\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7480\n",
            "  Validation Char Acc: 0.8341\n",
            "  Learning Rate: 0.000376\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ASUN' | Pred: 'ASUN'\n",
            "  True: 'NIN' | Pred: 'ITIIN'\n",
            "  True: 'CAN' | Pred: 'CAN'\n",
            "  True: 'YOURE' | Pred: 'YOURE'\n",
            "  True: 'CRESTDOWN' | Pred: 'CRESTDOWN'\n",
            "\n",
            "üìà Epoch 146/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:09<00:00, 15.66it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 146 Summary:\n",
            "  Training Loss: 0.0005\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7420\n",
            "  Validation Char Acc: 0.8360\n",
            "  Learning Rate: 0.000370\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'GREAT' | Pred: 'GREAT'\n",
            "  True: 'ANGELS' | Pred: 'ANGELS'\n",
            "  True: 'LOVE' | Pred: 'CE'\n",
            "  True: 'EXIT' | Pred: 'EXIT'\n",
            "  True: 'THEYRE' | Pred: 'THEYRE'\n",
            "\n",
            "üìà Epoch 147/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.41it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 147 Summary:\n",
            "  Training Loss: 0.0004\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7520\n",
            "  Validation Char Acc: 0.8392\n",
            "  Learning Rate: 0.000364\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'SOLVE' | Pred: 'SOLVE'\n",
            "  True: 'WAS' | Pred: 'WAS'\n",
            "  True: 'GO' | Pred: 'CO'\n",
            "  True: 'SREELAKSHMI' | Pred: 'SNINLAKSSINI'\n",
            "  True: 'CARD' | Pred: 'CARD'\n",
            "\n",
            "üìà Epoch 148/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.27it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 148 Summary:\n",
            "  Training Loss: 0.0004\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7440\n",
            "  Validation Char Acc: 0.8378\n",
            "  Learning Rate: 0.000357\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'JACKASS' | Pred: 'IACKASS'\n",
            "  True: 'NEWS' | Pred: 'NEWS'\n",
            "  True: '930' | Pred: '930'\n",
            "  True: 'COM' | Pred: 'COM'\n",
            "  True: 'LETS' | Pred: 'LETS'\n",
            "\n",
            "üìà Epoch 149/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.84it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 149 Summary:\n",
            "  Training Loss: 0.0004\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7540\n",
            "  Validation Char Acc: 0.8433\n",
            "  Learning Rate: 0.000351\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'CRITICS' | Pred: 'CRITICS'\n",
            "  True: 'USER' | Pred: 'USER'\n",
            "  True: 'MEANS' | Pred: 'MEANS'\n",
            "  True: 'JACKASS' | Pred: 'IACKASS'\n",
            "  True: 'OOPS' | Pred: 'OOPS'\n",
            "\n",
            "üìà Epoch 150/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.95it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 150 Summary:\n",
            "  Training Loss: 0.0007\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7360\n",
            "  Validation Char Acc: 0.8323\n",
            "  Learning Rate: 0.000345\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'HANDY' | Pred: 'HANDY'\n",
            "  True: 'ONLY' | Pred: 'ONLY'\n",
            "  True: 'SALT' | Pred: 'SALT'\n",
            "  True: 'A' | Pred: 'A'\n",
            "  True: 'ANSON' | Pred: 'ANSON'\n",
            "\n",
            "üìà Epoch 151/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.11it/s, loss=0.005, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 151 Summary:\n",
            "  Training Loss: 0.0014\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7300\n",
            "  Validation Char Acc: 0.8270\n",
            "  Learning Rate: 0.000340\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'DENZEL' | Pred: 'DENAEL'\n",
            "  True: 'REEVES' | Pred: 'REEVES'\n",
            "  True: 'IMAGE' | Pred: 'IMAGE'\n",
            "  True: 'DONE' | Pred: 'DONE'\n",
            "  True: 'MEANS' | Pred: 'MEANS'\n",
            "\n",
            "üìà Epoch 152/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.60it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 152 Summary:\n",
            "  Training Loss: 0.0008\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7280\n",
            "  Validation Char Acc: 0.8310\n",
            "  Learning Rate: 0.000334\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BOB' | Pred: 'BOB'\n",
            "  True: 'HUNTINGTON' | Pred: 'HUNTINFON'\n",
            "  True: 'ASIA' | Pred: 'ASIA'\n",
            "  True: 'TOM' | Pred: 'TOM'\n",
            "  True: 'WISHES' | Pred: 'WISHES'\n",
            "\n",
            "üìà Epoch 153/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.05it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 153 Summary:\n",
            "  Training Loss: 0.0011\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7300\n",
            "  Validation Char Acc: 0.8340\n",
            "  Learning Rate: 0.000328\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'AHEAD' | Pred: 'AHEAD'\n",
            "  True: 'PHONE' | Pred: 'PHONE'\n",
            "  True: 'BEEN' | Pred: 'BEEN'\n",
            "  True: 'NORTH' | Pred: 'NORTH'\n",
            "\n",
            "üìà Epoch 154/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.08it/s, loss=0.047, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 154 Summary:\n",
            "  Training Loss: 0.0029\n",
            "  Training Word Acc: 0.9898\n",
            "  Validation Word Acc: 0.7360\n",
            "  Validation Char Acc: 0.8262\n",
            "  Learning Rate: 0.000322\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'TRINITYLEEDS' | Pred: 'THMTYLECRDS'\n",
            "  True: 'COLA' | Pred: 'COLA'\n",
            "  True: 'ZIEHEN' | Pred: 'ZIEHEN'\n",
            "  True: 'KINGS' | Pred: 'KINGS'\n",
            "  True: 'UNIVERSITY' | Pred: 'UNTIVERSTIY'\n",
            "\n",
            "üìà Epoch 155/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.07it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 155 Summary:\n",
            "  Training Loss: 0.0035\n",
            "  Training Word Acc: 0.9883\n",
            "  Validation Word Acc: 0.7440\n",
            "  Validation Char Acc: 0.8305\n",
            "  Learning Rate: 0.000316\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'STATION' | Pred: 'STATION'\n",
            "  True: 'JONES' | Pred: 'JONES'\n",
            "  True: 'BANK' | Pred: 'BANK'\n",
            "  True: 'EXIT' | Pred: 'EXIT'\n",
            "  True: 'DENZEL' | Pred: 'DENAEL'\n",
            "\n",
            "üìà Epoch 156/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.02it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 156 Summary:\n",
            "  Training Loss: 0.0031\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7380\n",
            "  Validation Char Acc: 0.8298\n",
            "  Learning Rate: 0.000310\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'FRIENDS' | Pred: 'FRIENDS'\n",
            "  True: 'ON' | Pred: 'ON'\n",
            "  True: 'SHIZUOKA' | Pred: 'SHZUOKA'\n",
            "  True: 'WYNTON' | Pred: 'WGATON'\n",
            "  True: 'YOUR' | Pred: 'YOUR'\n",
            "\n",
            "üìà Epoch 157/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.12it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 157 Summary:\n",
            "  Training Loss: 0.0017\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7400\n",
            "  Validation Char Acc: 0.8284\n",
            "  Learning Rate: 0.000304\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '280' | Pred: '280'\n",
            "  True: 'JURASSIC' | Pred: 'JURASSIC'\n",
            "  True: 'CRESTED' | Pred: 'ERESTED'\n",
            "  True: 'BANK' | Pred: 'BARNKE'\n",
            "  True: '560' | Pred: '560'\n",
            "\n",
            "üìà Epoch 158/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.11it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 158 Summary:\n",
            "  Training Loss: 0.0017\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7420\n",
            "  Validation Char Acc: 0.8359\n",
            "  Learning Rate: 0.000299\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'POINT' | Pred: 'KOLN'\n",
            "  True: 'CRESTED' | Pred: 'ERESTED'\n",
            "  True: 'POSSOBILITIES' | Pred: 'POSSIBITIES'\n",
            "  True: 'PHOENIX' | Pred: 'PHOENDX'\n",
            "  True: 'BUT' | Pred: 'BUT'\n",
            "\n",
            "üìà Epoch 159/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.06it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 159 Summary:\n",
            "  Training Loss: 0.0020\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7260\n",
            "  Validation Char Acc: 0.8253\n",
            "  Learning Rate: 0.000293\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '3D' | Pred: 'BR'\n",
            "  True: 'DIAZ' | Pred: 'DIA'\n",
            "  True: 'GENOAS' | Pred: 'GENOAS'\n",
            "  True: 'ORDER' | Pred: 'DRDER'\n",
            "  True: 'FUTURE' | Pred: 'FUTURE'\n",
            "\n",
            "üìà Epoch 160/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.81it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 160 Summary:\n",
            "  Training Loss: 0.0013\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7460\n",
            "  Validation Char Acc: 0.8245\n",
            "  Learning Rate: 0.000287\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'HIDLY' | Pred: 'HIDLY'\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "  True: 'COM' | Pred: 'COM'\n",
            "  True: 'SHOP' | Pred: 'SHOP'\n",
            "  True: 'BUY' | Pred: 'BIY'\n",
            "\n",
            "üìà Epoch 161/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.23it/s, loss=0.004, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 161 Summary:\n",
            "  Training Loss: 0.0019\n",
            "  Training Word Acc: 0.9922\n",
            "  Validation Word Acc: 0.7520\n",
            "  Validation Char Acc: 0.8417\n",
            "  Learning Rate: 0.000281\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'TWENTY' | Pred: 'INENTT'\n",
            "  True: 'CATE' | Pred: 'CATE'\n",
            "  True: 'UNE' | Pred: 'UNE'\n",
            "  True: 'ALIBABA' | Pred: 'ALIBABA'\n",
            "  True: 'DUKES' | Pred: 'DUKES'\n",
            "\n",
            "üìà Epoch 162/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.17it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 162 Summary:\n",
            "  Training Loss: 0.0014\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7540\n",
            "  Validation Char Acc: 0.8477\n",
            "  Learning Rate: 0.000276\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'QUEENSWAY' | Pred: 'CUEENSWOY'\n",
            "  True: 'ATM' | Pred: 'ATM'\n",
            "  True: 'CORRESLAW' | Pred: 'CORRESLAY'\n",
            "  True: '01922' | Pred: '01922'\n",
            "  True: 'GLOBAL' | Pred: 'GLOBAL'\n",
            "\n",
            "üìà Epoch 163/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.24it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 163 Summary:\n",
            "  Training Loss: 0.0028\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7480\n",
            "  Validation Char Acc: 0.8418\n",
            "  Learning Rate: 0.000270\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'GREM' | Pred: 'GREM'\n",
            "  True: 'SAVE' | Pred: 'SAVE'\n",
            "  True: 'FARM' | Pred: 'FARM'\n",
            "  True: 'MATRIX' | Pred: 'MATRIX'\n",
            "  True: 'NEW' | Pred: 'NEW'\n",
            "\n",
            "üìà Epoch 164/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.84it/s, loss=0.028, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 164 Summary:\n",
            "  Training Loss: 0.0011\n",
            "  Training Word Acc: 0.9938\n",
            "  Validation Word Acc: 0.7480\n",
            "  Validation Char Acc: 0.8463\n",
            "  Learning Rate: 0.000265\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'JONES' | Pred: 'JONES'\n",
            "  True: 'BIG' | Pred: 'BIG'\n",
            "  True: 'COOPER' | Pred: 'COOPER'\n",
            "  True: '23' | Pred: '23'\n",
            "  True: 'NO' | Pred: 'NO'\n",
            "\n",
            "üìà Epoch 165/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.21it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 165 Summary:\n",
            "  Training Loss: 0.0007\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7380\n",
            "  Validation Char Acc: 0.8396\n",
            "  Learning Rate: 0.000259\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'TAXI' | Pred: 'TAXL'\n",
            "  True: 'QAARVAN' | Pred: 'DAARVAN'\n",
            "  True: 'IS' | Pred: 'ISS'\n",
            "  True: '95' | Pred: '95'\n",
            "  True: 'BPL' | Pred: 'BPL'\n",
            "\n",
            "üìà Epoch 166/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.11it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 166 Summary:\n",
            "  Training Loss: 0.0007\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7420\n",
            "  Validation Char Acc: 0.8473\n",
            "  Learning Rate: 0.000254\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'OOPS' | Pred: 'OOPS'\n",
            "  True: 'ON' | Pred: 'ON'\n",
            "  True: 'STATE' | Pred: 'STATE'\n",
            "  True: '560' | Pred: '560'\n",
            "  True: 'GREM' | Pred: 'GREM'\n",
            "\n",
            "üìà Epoch 167/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.19it/s, loss=0.005, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 167 Summary:\n",
            "  Training Loss: 0.0014\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8373\n",
            "  Learning Rate: 0.000248\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'DEMONS' | Pred: 'DEMONS'\n",
            "  True: 'N72' | Pred: 'N'\n",
            "  True: 'SREELAKSHMI' | Pred: 'SRINLAKSSINMI'\n",
            "  True: 'TWILIGHT' | Pred: 'TWILIGHT'\n",
            "  True: 'YOU' | Pred: 'YOU'\n",
            "\n",
            "üìà Epoch 168/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.78it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 168 Summary:\n",
            "  Training Loss: 0.0009\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7440\n",
            "  Validation Char Acc: 0.8249\n",
            "  Learning Rate: 0.000243\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '2' | Pred: '2'\n",
            "  True: 'SCOOTER' | Pred: 'SCOOLER'\n",
            "  True: '1989' | Pred: '1939'\n",
            "  True: '2' | Pred: '2'\n",
            "  True: 'CITROEN' | Pred: 'CITROEN'\n",
            "\n",
            "üìà Epoch 169/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.48it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 169 Summary:\n",
            "  Training Loss: 0.0004\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7560\n",
            "  Validation Char Acc: 0.8415\n",
            "  Learning Rate: 0.000237\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PEACE' | Pred: 'REACE'\n",
            "  True: 'PANCHGANI' | Pred: 'PANCHGANI'\n",
            "  True: 'CURRENTLY' | Pred: 'CURRENTLY'\n",
            "  True: 'TIMES' | Pred: 'TIMES'\n",
            "  True: 'LOVE' | Pred: 'CE'\n",
            "\n",
            "üìà Epoch 170/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.14it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 170 Summary:\n",
            "  Training Loss: 0.0003\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7540\n",
            "  Validation Char Acc: 0.8389\n",
            "  Learning Rate: 0.000232\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'AS' | Pred: 'AS'\n",
            "  True: 'CHRYSLER' | Pred: 'CHAVSLER'\n",
            "  True: 'NIN' | Pred: 'ITIN'\n",
            "  True: 'WAS' | Pred: 'WAS'\n",
            "  True: 'IN' | Pred: 'IN'\n",
            "\n",
            "üìà Epoch 171/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.12it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 171 Summary:\n",
            "  Training Loss: 0.0010\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7480\n",
            "  Validation Char Acc: 0.8402\n",
            "  Learning Rate: 0.000227\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PAUL' | Pred: 'PAUL'\n",
            "  True: 'TOLL' | Pred: 'TOIL'\n",
            "  True: 'AND' | Pred: 'ANO'\n",
            "  True: 'MATRIX' | Pred: 'MATRIX'\n",
            "  True: 'GREAT' | Pred: 'GREAT'\n",
            "\n",
            "üìà Epoch 172/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.72it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 172 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7500\n",
            "  Validation Char Acc: 0.8387\n",
            "  Learning Rate: 0.000222\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '5' | Pred: '5'\n",
            "  True: 'SCHEMES' | Pred: 'SCHEMES'\n",
            "  True: 'JACKASS' | Pred: 'IACKASS'\n",
            "  True: 'FUTURE' | Pred: 'FUTURE'\n",
            "  True: 'DAY' | Pred: 'DAY'\n",
            "\n",
            "üìà Epoch 173/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.59it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 173 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7520\n",
            "  Validation Char Acc: 0.8313\n",
            "  Learning Rate: 0.000216\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '45' | Pred: '45'\n",
            "  True: 'INDIAN' | Pred: 'INDIAN'\n",
            "  True: 'HOURS' | Pred: 'HOURS'\n",
            "  True: 'COM' | Pred: 'COM'\n",
            "  True: 'NOTHING' | Pred: 'NOTHING'\n",
            "\n",
            "üìà Epoch 174/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.82it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 174 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7480\n",
            "  Validation Char Acc: 0.8371\n",
            "  Learning Rate: 0.000211\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'HERE' | Pred: 'HERE'\n",
            "  True: 'BE' | Pred: 'BE'\n",
            "  True: 'PITT' | Pred: 'PITT'\n",
            "  True: 'BORDER' | Pred: 'BORDER'\n",
            "  True: 'PACK' | Pred: 'PACK'\n",
            "\n",
            "üìà Epoch 175/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.86it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 175 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7560\n",
            "  Validation Char Acc: 0.8398\n",
            "  Learning Rate: 0.000206\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'STORY' | Pred: 'STORY'\n",
            "  True: 'MCKINNEY' | Pred: 'MEKINNEY'\n",
            "  True: 'PENANG' | Pred: 'PENANG'\n",
            "  True: 'INSTORE' | Pred: 'INSTORE'\n",
            "  True: 'DAY' | Pred: 'DAY'\n",
            "\n",
            "üìà Epoch 176/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.24it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 176 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8370\n",
            "  Learning Rate: 0.000201\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BE' | Pred: 'BE'\n",
            "  True: '930' | Pred: '930'\n",
            "  True: 'ORDER' | Pred: 'DRDER'\n",
            "  True: 'JONES' | Pred: 'JONES'\n",
            "  True: 'WWWSHUTTERSTOCKCOM' | Pred: 'WWWSHUTTERSTOKCOM'\n",
            "\n",
            "üìà Epoch 177/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.83it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 177 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8389\n",
            "  Learning Rate: 0.000196\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'NOVO' | Pred: 'NOVO'\n",
            "  True: 'MATRIX' | Pred: 'MATRIX'\n",
            "  True: 'COOPER' | Pred: 'COOPER'\n",
            "  True: 'DIRECTION' | Pred: 'DIRECTION'\n",
            "  True: 'ADOPTION' | Pred: 'ADOPTION'\n",
            "\n",
            "üìà Epoch 178/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.93it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 178 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8394\n",
            "  Learning Rate: 0.000191\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'REGENCY' | Pred: 'DLGENCY'\n",
            "  True: 'BOB' | Pred: '90B'\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "  True: '7' | Pred: '7'\n",
            "  True: 'IN' | Pred: 'IN'\n",
            "\n",
            "üìà Epoch 179/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.05it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 179 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8430\n",
            "  Learning Rate: 0.000186\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'SAJITH' | Pred: 'SAIITH'\n",
            "  True: 'AND' | Pred: 'AND'\n",
            "  True: 'ASHTON' | Pred: 'ASHTON'\n",
            "  True: '3D' | Pred: '3D'\n",
            "  True: 'SQUIRRELS' | Pred: 'FOAUNRRES'\n",
            "\n",
            "üìà Epoch 180/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.10it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 180 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7620\n",
            "  Validation Char Acc: 0.8444\n",
            "  Learning Rate: 0.000181\n",
            "  ‚úÖ New best model saved! Word Acc: 0.7620\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'US' | Pred: 'US'\n",
            "  True: 'SURVEY' | Pred: 'SURVEY'\n",
            "  True: 'STATE' | Pred: 'STATE'\n",
            "  True: 'EVERY' | Pred: 'EVEY'\n",
            "  True: 'GRASSROOTSMARKETING' | Pred: 'WWULSTROMATHOHWTHW'\n",
            "\n",
            "üìà Epoch 181/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.87it/s, loss=0.004, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 181 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7560\n",
            "  Validation Char Acc: 0.8408\n",
            "  Learning Rate: 0.000176\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'OF' | Pred: 'OF'\n",
            "  True: '280' | Pred: '280'\n",
            "  True: '25' | Pred: '25'\n",
            "  True: 'INK' | Pred: 'INK'\n",
            "  True: 'CURRENCY' | Pred: 'CURTRERCY'\n",
            "\n",
            "üìà Epoch 182/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.85it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 182 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7540\n",
            "  Validation Char Acc: 0.8419\n",
            "  Learning Rate: 0.000172\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'CARD' | Pred: 'CARD'\n",
            "  True: 'SCOOTER' | Pred: 'SCOOLER'\n",
            "  True: 'BROWN' | Pred: 'BROWN'\n",
            "  True: 'REBELLION' | Pred: 'REBELLION'\n",
            "  True: 'MONTGOMERY' | Pred: 'MONTGOMMERY'\n",
            "\n",
            "üìà Epoch 183/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.95it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 183 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7500\n",
            "  Validation Char Acc: 0.8381\n",
            "  Learning Rate: 0.000167\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'JUKEBOX' | Pred: 'JUKEBOX'\n",
            "  True: '212' | Pred: '212'\n",
            "  True: 'OF' | Pred: 'OF'\n",
            "  True: 'TOLL' | Pred: 'TOIL'\n",
            "  True: 'REEVES' | Pred: 'REEVES'\n",
            "\n",
            "üìà Epoch 184/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.79it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 184 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7540\n",
            "  Validation Char Acc: 0.8384\n",
            "  Learning Rate: 0.000162\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'KINGS' | Pred: 'KINGS'\n",
            "  True: '125' | Pred: '126'\n",
            "  True: '20' | Pred: '20'\n",
            "  True: 'INK' | Pred: 'INK'\n",
            "  True: '3D' | Pred: '3'\n",
            "\n",
            "üìà Epoch 185/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.47it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 185 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7520\n",
            "  Validation Char Acc: 0.8371\n",
            "  Learning Rate: 0.000158\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'SPACE' | Pred: 'SPACE'\n",
            "  True: '120' | Pred: '120'\n",
            "  True: 'TOLL' | Pred: 'TOIL'\n",
            "  True: '15' | Pred: '15'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "\n",
            "üìà Epoch 186/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.15it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 186 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8424\n",
            "  Learning Rate: 0.000153\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'COLOURS' | Pred: 'COLONRS'\n",
            "  True: 'VIEWERS' | Pred: 'VIEWERS'\n",
            "  True: '1800BUYKWIK' | Pred: 'BEODBUYHWEK'\n",
            "  True: 'OFFICE' | Pred: 'OFFICE'\n",
            "  True: 'CREEK' | Pred: 'CREEX'\n",
            "\n",
            "üìà Epoch 187/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.96it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 187 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7560\n",
            "  Validation Char Acc: 0.8424\n",
            "  Learning Rate: 0.000149\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'MOTION' | Pred: 'MOTION'\n",
            "  True: 'EVERY' | Pred: 'EVEY'\n",
            "  True: 'SWEET' | Pred: 'SNEEY'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'ROUTH' | Pred: 'POUTH'\n",
            "\n",
            "üìà Epoch 188/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.02it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 188 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7480\n",
            "  Validation Char Acc: 0.8410\n",
            "  Learning Rate: 0.000144\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'TITANS' | Pred: 'TITANS'\n",
            "  True: 'CITROEN' | Pred: 'CITROEN'\n",
            "  True: 'HOUSE' | Pred: 'HOUSE'\n",
            "  True: 'A' | Pred: 'A'\n",
            "  True: 'NOW' | Pred: 'NOW'\n",
            "\n",
            "üìà Epoch 189/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.34it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 189 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7540\n",
            "  Validation Char Acc: 0.8413\n",
            "  Learning Rate: 0.000140\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'THINK' | Pred: 'THINB'\n",
            "  True: '4865' | Pred: '4365'\n",
            "  True: '1800GOGEICO' | Pred: '400GDGECO'\n",
            "  True: 'LOSE' | Pred: 'LOSE'\n",
            "  True: '99' | Pred: '99'\n",
            "\n",
            "üìà Epoch 190/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.54it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 190 Summary:\n",
            "  Training Loss: 0.0003\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8412\n",
            "  Learning Rate: 0.000136\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BPL' | Pred: 'BPL'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'MIRREN' | Pred: 'MIRREN'\n",
            "  True: 'NOW' | Pred: 'NAY'\n",
            "  True: 'MORE' | Pred: 'MORE'\n",
            "\n",
            "üìà Epoch 191/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.93it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 191 Summary:\n",
            "  Training Loss: 0.0003\n",
            "  Training Word Acc: 0.9961\n",
            "  Validation Word Acc: 0.7520\n",
            "  Validation Char Acc: 0.8360\n",
            "  Learning Rate: 0.000131\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'NOW' | Pred: 'NOW'\n",
            "  True: 'SOLVE' | Pred: 'SOLVE'\n",
            "  True: '317' | Pred: '317'\n",
            "  True: 'ASTAIRE' | Pred: 'ASTAIRE'\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "\n",
            "üìà Epoch 192/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.04it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 192 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7560\n",
            "  Validation Char Acc: 0.8384\n",
            "  Learning Rate: 0.000127\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'SAW' | Pred: 'SALL'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'A' | Pred: 'A'\n",
            "  True: 'YOUR' | Pred: 'YOUR'\n",
            "  True: '2' | Pred: '2'\n",
            "\n",
            "üìà Epoch 193/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.17it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 193 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7620\n",
            "  Validation Char Acc: 0.8446\n",
            "  Learning Rate: 0.000123\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'YOU' | Pred: 'YOU'\n",
            "  True: 'DUKES' | Pred: 'DUKES'\n",
            "  True: 'TERMINAL' | Pred: 'TERMINEL'\n",
            "  True: 'PENANG' | Pred: 'PENANG'\n",
            "  True: 'FARM' | Pred: 'FARM'\n",
            "\n",
            "üìà Epoch 194/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.71it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 194 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8394\n",
            "  Learning Rate: 0.000119\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'FRIENDS' | Pred: 'FRIENDS'\n",
            "  True: 'THROW' | Pred: 'THROW'\n",
            "  True: '212' | Pred: '212'\n",
            "  True: 'WASHINGTON' | Pred: 'WASHINGTON'\n",
            "  True: 'JULY' | Pred: 'JULY'\n",
            "\n",
            "üìà Epoch 195/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.10it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 195 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8389\n",
            "  Learning Rate: 0.000115\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PHONE' | Pred: 'PHONE'\n",
            "  True: 'BLUE' | Pred: 'BLUE'\n",
            "  True: 'OF' | Pred: 'OF'\n",
            "  True: 'EXIT' | Pred: 'EXIT'\n",
            "  True: 'BANK' | Pred: 'BANKK'\n",
            "\n",
            "üìà Epoch 196/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.99it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 196 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7500\n",
            "  Validation Char Acc: 0.8327\n",
            "  Learning Rate: 0.000111\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'NO' | Pred: 'NO'\n",
            "  True: 'BE' | Pred: 'BE'\n",
            "  True: 'NEWS' | Pred: 'NEWS'\n",
            "  True: 'PHONE' | Pred: 'PHONE'\n",
            "  True: 'LOVE' | Pred: 'CO'\n",
            "\n",
            "üìà Epoch 197/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.01it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 197 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7460\n",
            "  Validation Char Acc: 0.8346\n",
            "  Learning Rate: 0.000107\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'TIME' | Pred: 'TVE'\n",
            "  True: 'ASIA' | Pred: 'ASIA'\n",
            "  True: 'REBELLION' | Pred: 'REBELLILON'\n",
            "  True: 'AND' | Pred: 'AND'\n",
            "  True: 'ROLAND' | Pred: 'ROLAND'\n",
            "\n",
            "üìà Epoch 198/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.82it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 198 Summary:\n",
            "  Training Loss: 0.0005\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7520\n",
            "  Validation Char Acc: 0.8423\n",
            "  Learning Rate: 0.000103\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'JURASSIC' | Pred: 'JURASSIC'\n",
            "  True: 'YOUR' | Pred: 'YOUR'\n",
            "  True: 'IS' | Pred: 'IS'\n",
            "  True: 'PHONE' | Pred: 'PHONE'\n",
            "  True: 'SHOESTRING' | Pred: 'PLOESTRE'\n",
            "\n",
            "üìà Epoch 199/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.96it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 199 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7480\n",
            "  Validation Char Acc: 0.8348\n",
            "  Learning Rate: 0.000099\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'AN' | Pred: 'AN'\n",
            "  True: 'LOANS' | Pred: 'LOANS'\n",
            "  True: 'JUBILEE' | Pred: 'JUBILEE'\n",
            "  True: 'NOVEMBER' | Pred: 'NOVEMBER'\n",
            "  True: 'LOSE' | Pred: 'LOSE'\n",
            "\n",
            "üìà Epoch 200/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.13it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 200 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7680\n",
            "  Validation Char Acc: 0.8428\n",
            "  Learning Rate: 0.000095\n",
            "  ‚úÖ New best model saved! Word Acc: 0.7680\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'GENOAS' | Pred: 'GENOAS'\n",
            "  True: 'VIEW' | Pred: 'VIEW'\n",
            "  True: 'JULY' | Pred: 'JULY'\n",
            "  True: 'CHANDIGARH' | Pred: 'CHANDIGARH'\n",
            "  True: 'FIRST' | Pred: 'FIRST'\n",
            "\n",
            "üìà Epoch 201/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.09it/s, loss=0.001, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 201 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7620\n",
            "  Validation Char Acc: 0.8407\n",
            "  Learning Rate: 0.000092\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "  True: 'VEGETARIAN' | Pred: 'VECSTARIAN'\n",
            "  True: '7' | Pred: '7'\n",
            "  True: 'PAUL' | Pred: 'PAUL'\n",
            "  True: 'HITAM' | Pred: 'HITAM'\n",
            "\n",
            "üìà Epoch 202/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.73it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 202 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7640\n",
            "  Validation Char Acc: 0.8442\n",
            "  Learning Rate: 0.000088\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PORTABLE' | Pred: 'PORTABLE'\n",
            "  True: 'FARM' | Pred: 'FARM'\n",
            "  True: 'TOTAL' | Pred: 'TOTAL'\n",
            "  True: 'SBI' | Pred: 'SBR'\n",
            "  True: 'OF' | Pred: 'OF'\n",
            "\n",
            "üìà Epoch 203/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.03it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 203 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8387\n",
            "  Learning Rate: 0.000085\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'REGENCY' | Pred: 'NLGENCY'\n",
            "  True: '208' | Pred: '208'\n",
            "  True: 'KELLIMAR' | Pred: 'KEAMMA'\n",
            "  True: 'SETH' | Pred: 'CEW'\n",
            "\n",
            "üìà Epoch 204/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.99it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 204 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8379\n",
            "  Learning Rate: 0.000081\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '7' | Pred: '7'\n",
            "  True: 'HEFNER' | Pred: 'HEFNER'\n",
            "  True: 'WORTHINGTON' | Pred: 'IIEILIIETLL'\n",
            "  True: 'TWITTERCOMAPLUSK' | Pred: 'MLECOMEPIESY'\n",
            "  True: 'MATRIX' | Pred: 'MATRIX'\n",
            "\n",
            "üìà Epoch 205/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:09<00:00, 14.99it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 205 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7640\n",
            "  Validation Char Acc: 0.8453\n",
            "  Learning Rate: 0.000078\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'LOADING' | Pred: 'CECING'\n",
            "  True: 'THROW' | Pred: 'THROW'\n",
            "  True: 'SPECIAL' | Pred: 'GPTCIOL'\n",
            "  True: 'HUNTINGTON' | Pred: 'HUNTINETON'\n",
            "  True: 'OOPS' | Pred: 'OOPS'\n",
            "\n",
            "üìà Epoch 206/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.80it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 206 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7540\n",
            "  Validation Char Acc: 0.8409\n",
            "  Learning Rate: 0.000075\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'AVAILABLE' | Pred: 'AVALABLE'\n",
            "  True: 'ATM' | Pred: 'ATM'\n",
            "  True: 'BROWN' | Pred: 'BROWN'\n",
            "  True: 'ONLINE' | Pred: 'ONLINE'\n",
            "  True: 'COLOURS' | Pred: 'COLONRS'\n",
            "\n",
            "üìà Epoch 207/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.91it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 207 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7540\n",
            "  Validation Char Acc: 0.8374\n",
            "  Learning Rate: 0.000071\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'YOUR' | Pred: 'YOUR'\n",
            "  True: 'US' | Pred: 'US'\n",
            "  True: 'SANTOMIC' | Pred: 'SANTOMIC'\n",
            "  True: '280' | Pred: '280'\n",
            "  True: 'VEGETARIAN' | Pred: 'VECSTARIAN'\n",
            "\n",
            "üìà Epoch 208/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.90it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 208 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8388\n",
            "  Learning Rate: 0.000068\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '930' | Pred: '930'\n",
            "  True: '2ND' | Pred: '2ND'\n",
            "  True: 'MARZO' | Pred: 'MARZO'\n",
            "  True: 'ALIBABA' | Pred: 'ATIBABA'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "\n",
            "üìà Epoch 209/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.85it/s, loss=0.009, word_acc=0.950]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 209 Summary:\n",
            "  Training Loss: 0.0002\n",
            "  Training Word Acc: 0.9938\n",
            "  Validation Word Acc: 0.7620\n",
            "  Validation Char Acc: 0.8424\n",
            "  Learning Rate: 0.000065\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'JACKASS' | Pred: 'IACKASS'\n",
            "  True: 'VEGETARIAN' | Pred: 'VECSTARIAN'\n",
            "  True: 'MASS' | Pred: 'MASS'\n",
            "  True: '19' | Pred: '1'\n",
            "  True: 'TWENTY' | Pred: 'INENTT'\n",
            "\n",
            "üìà Epoch 210/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.54it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 210 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7520\n",
            "  Validation Char Acc: 0.8356\n",
            "  Learning Rate: 0.000062\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'OFFICE' | Pred: 'OFFICE'\n",
            "  True: 'YOUR' | Pred: 'YOUR'\n",
            "  True: 'REBELLION' | Pred: 'REBELLION'\n",
            "  True: 'TRINITYLEEDS' | Pred: 'TMUMTYLERDS'\n",
            "  True: 'ASHTON' | Pred: 'ASHTON'\n",
            "\n",
            "üìà Epoch 211/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.04it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 211 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8376\n",
            "  Learning Rate: 0.000059\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'TO' | Pred: 'TO'\n",
            "  True: 'FOR' | Pred: 'FOR'\n",
            "  True: 'MONTGOMERY' | Pred: 'MONTCOLIERY'\n",
            "  True: '77' | Pred: '77'\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "\n",
            "üìà Epoch 212/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.89it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 212 Summary:\n",
            "  Training Loss: 0.0003\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7540\n",
            "  Validation Char Acc: 0.8350\n",
            "  Learning Rate: 0.000056\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ANSON' | Pred: 'ANSON'\n",
            "  True: 'HOMICIDE' | Pred: 'HOMICIDE'\n",
            "  True: 'FAILTE' | Pred: 'TAILTE'\n",
            "  True: 'ALIBABA' | Pred: 'ALIBABA'\n",
            "  True: 'AND' | Pred: 'AND'\n",
            "\n",
            "üìà Epoch 213/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.75it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 213 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8371\n",
            "  Learning Rate: 0.000053\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'YOUR' | Pred: 'YOUR'\n",
            "  True: 'TIMES' | Pred: 'TIMES'\n",
            "  True: 'GREAT' | Pred: 'GREAT'\n",
            "  True: 'SAJITH' | Pred: 'SAIITH'\n",
            "  True: 'BOARD' | Pred: 'BOARD'\n",
            "\n",
            "üìà Epoch 214/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.12it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 214 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7560\n",
            "  Validation Char Acc: 0.8354\n",
            "  Learning Rate: 0.000050\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'DREAM' | Pred: 'DREAM'\n",
            "  True: 'FOSTER' | Pred: 'FOSTER'\n",
            "  True: 'HOME' | Pred: 'HOME'\n",
            "  True: 'AND' | Pred: 'ANO'\n",
            "  True: '3D' | Pred: '3D'\n",
            "\n",
            "üìà Epoch 215/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.43it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 215 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7540\n",
            "  Validation Char Acc: 0.8341\n",
            "  Learning Rate: 0.000048\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'CONTACT' | Pred: 'CONTACT'\n",
            "  True: 'THE' | Pred: 'THE'\n",
            "  True: 'LOVE' | Pred: 'CO'\n",
            "  True: 'DI' | Pred: 'DI'\n",
            "  True: 'CAN' | Pred: 'CAN'\n",
            "\n",
            "üìà Epoch 216/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.84it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 216 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7540\n",
            "  Validation Char Acc: 0.8358\n",
            "  Learning Rate: 0.000045\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'CFN' | Pred: 'CFN'\n",
            "  True: 'STATE' | Pred: 'STATE'\n",
            "  True: 'IMAIZUMI' | Pred: 'WOPUMI'\n",
            "  True: 'HOME' | Pred: 'HOIE'\n",
            "  True: 'FRUIT' | Pred: 'FUTIT'\n",
            "\n",
            "üìà Epoch 217/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.68it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 217 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8413\n",
            "  Learning Rate: 0.000042\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'STATE' | Pred: 'SATE'\n",
            "  True: 'MIRREN' | Pred: 'MIRREN'\n",
            "  True: 'CARING' | Pred: 'CARING'\n",
            "  True: 'ZIEHEN' | Pred: 'ZIEHEN'\n",
            "  True: 'SCHEMES' | Pred: 'SCHEMES'\n",
            "\n",
            "üìà Epoch 218/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.69it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 218 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8362\n",
            "  Learning Rate: 0.000040\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'YOU' | Pred: 'YOU'\n",
            "  True: 'MARILYN' | Pred: 'MARILYN'\n",
            "  True: 'PORTABLE' | Pred: 'PORTABLE'\n",
            "  True: 'BE' | Pred: 'BE'\n",
            "  True: 'MONTGOMERY' | Pred: 'MONTCONIERY'\n",
            "\n",
            "üìà Epoch 219/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.45it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 219 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7560\n",
            "  Validation Char Acc: 0.8351\n",
            "  Learning Rate: 0.000037\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'GRAHAMS' | Pred: 'GRAHONTS'\n",
            "  True: '15' | Pred: '15'\n",
            "  True: 'NOTHING' | Pred: 'NOTHING'\n",
            "  True: 'IMAGE' | Pred: 'IMAGE'\n",
            "  True: 'INTO' | Pred: 'INTO'\n",
            "\n",
            "üìà Epoch 220/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:09<00:00, 15.65it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 220 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7560\n",
            "  Validation Char Acc: 0.8316\n",
            "  Learning Rate: 0.000035\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '280' | Pred: '280'\n",
            "  True: '125' | Pred: '125'\n",
            "  True: 'WYNTON' | Pred: 'WGATON'\n",
            "  True: 'KELLIMAR' | Pred: 'KEAMA'\n",
            "  True: 'YOUR' | Pred: 'YOUR'\n",
            "\n",
            "üìà Epoch 221/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:09<00:00, 15.63it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 221 Summary:\n",
            "  Training Loss: 0.0000\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7620\n",
            "  Validation Char Acc: 0.8356\n",
            "  Learning Rate: 0.000033\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '4' | Pred: '4'\n",
            "  True: 'WWWSHUTTERSTOCKCOM' | Pred: 'WWWSHUTTERSTOKCOM'\n",
            "  True: 'LOADING' | Pred: 'CECING'\n",
            "  True: 'ON' | Pred: 'ON'\n",
            "  True: 'PHOENIX' | Pred: 'PHOENDX'\n",
            "\n",
            "üìà Epoch 222/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.69it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 222 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7560\n",
            "  Validation Char Acc: 0.8311\n",
            "  Learning Rate: 0.000031\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'CONSECUTIVE' | Pred: 'CONSECUTIVE'\n",
            "  True: 'UNIVERSITY' | Pred: 'UNIVERSTIY'\n",
            "  True: 'OUTDOOR' | Pred: 'OUTDOOR'\n",
            "  True: 'DAY' | Pred: 'DAY'\n",
            "  True: 'CITY' | Pred: 'CITY'\n",
            "\n",
            "üìà Epoch 223/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.17it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 223 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7540\n",
            "  Validation Char Acc: 0.8356\n",
            "  Learning Rate: 0.000029\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'NEWS' | Pred: 'NEWS'\n",
            "  True: 'BRISTOL' | Pred: 'BRISTOL'\n",
            "  True: 'SOLVE' | Pred: 'SOLVE'\n",
            "  True: 'DOT' | Pred: 'DOT'\n",
            "  True: 'COLOURS' | Pred: 'CLONRS'\n",
            "\n",
            "üìà Epoch 224/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.22it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 224 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8327\n",
            "  Learning Rate: 0.000026\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '23' | Pred: '23'\n",
            "  True: 'HOUSE' | Pred: 'HOUSE'\n",
            "  True: 'HOUSE' | Pred: 'HOUSE'\n",
            "  True: 'INDIASBIN' | Pred: 'INDTADSBIN'\n",
            "  True: 'LOUIS' | Pred: 'LGUIIS'\n",
            "\n",
            "üìà Epoch 225/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.86it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 225 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8327\n",
            "  Learning Rate: 0.000024\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ROLAND' | Pred: 'ROLAND'\n",
            "  True: '50' | Pred: '50'\n",
            "  True: 'ON' | Pred: 'ON'\n",
            "  True: 'IS' | Pred: 'IS'\n",
            "  True: 'HOUSE' | Pred: 'HOUSE'\n",
            "\n",
            "üìà Epoch 226/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.88it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 226 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8356\n",
            "  Learning Rate: 0.000023\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'FIRE' | Pred: 'FIRE'\n",
            "  True: 'BLANKES' | Pred: 'BLANKES'\n",
            "  True: 'JONES' | Pred: 'JONES'\n",
            "  True: 'HELT' | Pred: 'HELT'\n",
            "  True: 'DONE' | Pred: 'DONE'\n",
            "\n",
            "üìà Epoch 227/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.02it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 227 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8318\n",
            "  Learning Rate: 0.000021\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PANCHGANI' | Pred: 'PANCHGANI'\n",
            "  True: '2' | Pred: '2'\n",
            "  True: 'SQUIRRELS' | Pred: 'FOAUNRRELS'\n",
            "  True: 'LOADING' | Pred: 'CECING'\n",
            "  True: 'GSTAR' | Pred: 'GSTAR'\n",
            "\n",
            "üìà Epoch 228/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.54it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 228 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8383\n",
            "  Learning Rate: 0.000019\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'NEWS' | Pred: 'NEWS'\n",
            "  True: 'CRITICS' | Pred: 'CRITICS'\n",
            "  True: 'VEGETARIAN' | Pred: 'VECSTARIAN'\n",
            "  True: 'PAUL' | Pred: 'PAUL'\n",
            "  True: 'ASTAIRE' | Pred: 'ASTAIRE'\n",
            "\n",
            "üìà Epoch 229/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:09<00:00, 15.18it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 229 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8399\n",
            "  Learning Rate: 0.000017\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'JUBILEE' | Pred: 'JUBILEE'\n",
            "  True: 'YOU' | Pred: 'YOU'\n",
            "  True: 'WAS' | Pred: 'WAS'\n",
            "  True: 'AN' | Pred: 'AN'\n",
            "  True: 'OUTDOOR' | Pred: 'OUTDOOR'\n",
            "\n",
            "üìà Epoch 230/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.07it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 230 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8374\n",
            "  Learning Rate: 0.000016\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'DAY' | Pred: 'DAY'\n",
            "  True: 'RESET' | Pred: 'RESET'\n",
            "  True: 'RIGHT' | Pred: 'RIGHT'\n",
            "  True: '1800BUYKWIK' | Pred: 'BODBUYHWIK'\n",
            "  True: 'AHEAD' | Pred: 'AHEAD'\n",
            "\n",
            "üìà Epoch 231/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.08it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 231 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7560\n",
            "  Validation Char Acc: 0.8413\n",
            "  Learning Rate: 0.000014\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'INDIAN' | Pred: 'INDIAD'\n",
            "  True: 'KINGS' | Pred: 'KINGS'\n",
            "  True: 'NOT' | Pred: 'NOT'\n",
            "  True: 'WELCOME' | Pred: 'WELCOME'\n",
            "  True: 'ZAJAZD' | Pred: 'LHEED'\n",
            "\n",
            "üìà Epoch 232/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.89it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 232 Summary:\n",
            "  Training Loss: 0.0000\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8372\n",
            "  Learning Rate: 0.000013\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '1911' | Pred: 'I9LL'\n",
            "  True: 'BE' | Pred: 'BE'\n",
            "  True: 'TERMINAL' | Pred: 'TERMINEL'\n",
            "  True: 'SANTOMIC' | Pred: 'SANTOMIC'\n",
            "  True: 'SIGN' | Pred: 'SIGN'\n",
            "\n",
            "üìà Epoch 233/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.10it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 233 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7560\n",
            "  Validation Char Acc: 0.8389\n",
            "  Learning Rate: 0.000011\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '19' | Pred: '1'\n",
            "  True: 'SHOP' | Pred: 'SHOP'\n",
            "  True: 'WITH' | Pred: 'WITH'\n",
            "  True: 'CREEK' | Pred: 'CREEX'\n",
            "  True: 'AND' | Pred: 'ANOD'\n",
            "\n",
            "üìà Epoch 234/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.02it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 234 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8397\n",
            "  Learning Rate: 0.000010\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'HERE' | Pred: 'HERE'\n",
            "  True: 'SHIZUOKA' | Pred: 'SHZUOKA'\n",
            "  True: '208' | Pred: '208'\n",
            "  True: 'CANDIDATE' | Pred: 'CANDIDATE'\n",
            "  True: 'BIG' | Pred: 'BIG'\n",
            "\n",
            "üìà Epoch 235/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.06it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 235 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8408\n",
            "  Learning Rate: 0.000009\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ECO' | Pred: 'ECO'\n",
            "  True: 'INSTORE' | Pred: 'INSTORE'\n",
            "  True: 'HOURS' | Pred: 'HOURS'\n",
            "  True: 'VISTA' | Pred: 'VISTA'\n",
            "  True: 'TWENTY' | Pred: 'INENTW'\n",
            "\n",
            "üìà Epoch 236/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.87it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 236 Summary:\n",
            "  Training Loss: 0.0000\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8389\n",
            "  Learning Rate: 0.000008\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'PARKING' | Pred: 'PARKING'\n",
            "  True: 'SCHEMES' | Pred: 'SCHEMES'\n",
            "  True: 'WHETLEY' | Pred: 'WHETLEY'\n",
            "  True: '4' | Pred: '4'\n",
            "  True: '02' | Pred: '02'\n",
            "\n",
            "üìà Epoch 237/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.23it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 237 Summary:\n",
            "  Training Loss: 0.0000\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7560\n",
            "  Validation Char Acc: 0.8389\n",
            "  Learning Rate: 0.000007\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'HERE' | Pred: 'HERE'\n",
            "  True: 'TO' | Pred: 'TO'\n",
            "  True: 'SPACE' | Pred: 'SPACE'\n",
            "  True: 'COOPER' | Pred: 'COOPER'\n",
            "  True: 'NURSERY' | Pred: 'HORSEEY'\n",
            "\n",
            "üìà Epoch 238/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.07it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 238 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8350\n",
            "  Learning Rate: 0.000006\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'CAN' | Pred: 'CAN'\n",
            "  True: '50' | Pred: '50'\n",
            "  True: '25' | Pred: '25'\n",
            "  True: 'PRODAJA' | Pred: 'PRODAJA'\n",
            "  True: 'YOURE' | Pred: 'YOURE'\n",
            "\n",
            "üìà Epoch 239/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.99it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 239 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8361\n",
            "  Learning Rate: 0.000005\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'BLUBBER' | Pred: 'BLUBBER'\n",
            "  True: 'CITY' | Pred: 'CITY'\n",
            "  True: '22' | Pred: '88'\n",
            "  True: '4' | Pred: '4'\n",
            "  True: 'HITAM' | Pred: 'HITAM'\n",
            "\n",
            "üìà Epoch 240/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.50it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 240 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8411\n",
            "  Learning Rate: 0.000004\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '930' | Pred: '930'\n",
            "  True: 'INSTORE' | Pred: 'INSTORE'\n",
            "  True: 'BANK' | Pred: 'BANK'\n",
            "  True: 'HOUSE' | Pred: 'HOUSE'\n",
            "  True: 'BUT' | Pred: 'BUT'\n",
            "\n",
            "üìà Epoch 241/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.33it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 241 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8376\n",
            "  Learning Rate: 0.000003\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'INDIA' | Pred: 'INDIA'\n",
            "  True: 'ON' | Pred: 'ON'\n",
            "  True: '3D' | Pred: '3D'\n",
            "  True: 'LOW' | Pred: 'LOW'\n",
            "  True: 'CHANGE' | Pred: 'CHANGE'\n",
            "\n",
            "üìà Epoch 242/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.11it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 242 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8378\n",
            "  Learning Rate: 0.000003\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'UNE' | Pred: 'UNE'\n",
            "  True: 'POSSOBILITIES' | Pred: 'POSSIBITES'\n",
            "  True: '63' | Pred: '63'\n",
            "  True: 'LOSE' | Pred: 'LOSE'\n",
            "  True: '60602974' | Pred: '60602974'\n",
            "\n",
            "üìà Epoch 243/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 15.97it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 243 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8391\n",
            "  Learning Rate: 0.000002\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '02' | Pred: '02'\n",
            "  True: 'CATE' | Pred: 'CATE'\n",
            "  True: 'ECO' | Pred: 'ECO'\n",
            "  True: 'CITROEN' | Pred: 'CITROEN'\n",
            "  True: 'COM' | Pred: 'COM'\n",
            "\n",
            "üìà Epoch 244/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.61it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 244 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7620\n",
            "  Validation Char Acc: 0.8397\n",
            "  Learning Rate: 0.000001\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '95' | Pred: '95'\n",
            "  True: 'ROUTH' | Pred: 'POUTH'\n",
            "  True: 'VIEW' | Pred: 'VIEW'\n",
            "  True: '2' | Pred: '2'\n",
            "  True: 'MIRREN' | Pred: 'MIRREN'\n",
            "\n",
            "üìà Epoch 245/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.60it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 245 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8365\n",
            "  Learning Rate: 0.000001\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: '280' | Pred: '280'\n",
            "  True: 'METER' | Pred: 'METER'\n",
            "  True: 'REVERSE' | Pred: 'REVERSE'\n",
            "  True: 'PEACE' | Pred: 'REACE'\n",
            "  True: 'VALE' | Pred: 'VALE'\n",
            "\n",
            "üìà Epoch 246/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.06it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 246 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7620\n",
            "  Validation Char Acc: 0.8378\n",
            "  Learning Rate: 0.000001\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'ECO' | Pred: 'ECO'\n",
            "  True: 'WITH' | Pred: 'WITH'\n",
            "  True: 'NOW' | Pred: 'NAY'\n",
            "  True: '3D' | Pred: '8'\n",
            "  True: 'KELLIMAR' | Pred: 'KEAMMA'\n",
            "\n",
            "üìà Epoch 247/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.18it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 247 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7580\n",
            "  Validation Char Acc: 0.8353\n",
            "  Learning Rate: 0.000000\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'CHIC' | Pred: 'CHIC'\n",
            "  True: 'RESIST' | Pred: 'HESTST'\n",
            "  True: 'BEEN' | Pred: 'BEEN'\n",
            "  True: 'A' | Pred: 'A'\n",
            "  True: '120' | Pred: '120'\n",
            "\n",
            "üìà Epoch 248/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.31it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 248 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8428\n",
            "  Learning Rate: 0.000000\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'NOW' | Pred: 'NAY'\n",
            "  True: 'STATE' | Pred: 'STATE'\n",
            "  True: 'BLUE' | Pred: 'BLUE'\n",
            "  True: 'SBI' | Pred: 'SBR'\n",
            "  True: '2ND' | Pred: '2ND'\n",
            "\n",
            "üìà Epoch 249/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.61it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 249 Summary:\n",
            "  Training Loss: 0.0001\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7600\n",
            "  Validation Char Acc: 0.8391\n",
            "  Learning Rate: 0.000000\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'LEAGUE' | Pred: 'LEAGUE'\n",
            "  True: 'NOT' | Pred: 'NOT'\n",
            "  True: 'DREAM' | Pred: 'DREAM'\n",
            "  True: 'POD' | Pred: 'POD'\n",
            "  True: 'HOME' | Pred: 'HOME'\n",
            "\n",
            "üìà Epoch 250/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:08<00:00, 16.11it/s, loss=0.000, word_acc=1.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 250 Summary:\n",
            "  Training Loss: 0.0000\n",
            "  Training Word Acc: 1.0000\n",
            "  Validation Word Acc: 0.7620\n",
            "  Validation Char Acc: 0.8391\n",
            "  Learning Rate: 0.000000\n",
            "\n",
            "üéØ Sample predictions:\n",
            "  True: 'JALAN' | Pred: 'JALAN'\n",
            "  True: 'BACK' | Pred: 'BACK'\n",
            "  True: 'CUSTOMERS' | Pred: 'CISTOMERS'\n",
            "  True: 'PEACE' | Pred: 'REACE'\n",
            "  True: 'ROOM' | Pred: 'POORI'\n",
            "\n",
            "üéâ Training completed!\n",
            "Best validation word accuracy: 0.7680\n",
            "\n",
            "üèÜ Final result: 0.7680 word accuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# enhanced_deep_ocr.py\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import scipy.io\n",
        "import random\n",
        "import math\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ======================\n",
        "# 1. ENHANCED DATASET WITH AUGMENTATION\n",
        "# ======================\n",
        "class EnhancedCustomSplitIIIT5KDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', img_height=32, img_width=128,\n",
        "                 train_samples=4500, test_samples=500, random_seed=42, augment=True):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.iiit5k_dir = os.path.join(root_dir, \"IIIT5K\")\n",
        "        self.augment = augment and split == 'train'\n",
        "\n",
        "        # Set random seed for reproducible splits\n",
        "        random.seed(random_seed)\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "        # Define vocabulary\n",
        "        self.chars = string.ascii_letters + string.digits + \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
        "        self.char_to_idx = {char: idx + 1 for idx, char in enumerate(self.chars)}\n",
        "        self.idx_to_char = {idx + 1: char for idx, char in enumerate(self.chars)}\n",
        "        self.idx_to_char[0] = '<BLANK>'\n",
        "        self.num_classes = len(self.chars) + 1  # 95 + 1\n",
        "\n",
        "        print(f\"Creating custom dataset split with {train_samples} train, {test_samples} test samples...\")\n",
        "\n",
        "        # Load all data from both original train and test sets\n",
        "        all_samples = []\n",
        "\n",
        "        # Load original train data\n",
        "        train_mat_file = os.path.join(self.iiit5k_dir, \"traindata.mat\")\n",
        "        train_data_dir = os.path.join(self.iiit5k_dir, \"train\")\n",
        "        if os.path.exists(train_mat_file):\n",
        "            all_samples.extend(self._load_mat_data(train_mat_file, train_data_dir, \"train\"))\n",
        "\n",
        "        # Load original test data\n",
        "        test_mat_file = os.path.join(self.iiit5k_dir, \"testdata.mat\")\n",
        "        test_data_dir = os.path.join(self.iiit5k_dir, \"test\")\n",
        "        if os.path.exists(test_mat_file):\n",
        "            all_samples.extend(self._load_mat_data(test_mat_file, test_data_dir, \"test\"))\n",
        "\n",
        "        print(f\"Total samples loaded: {len(all_samples)}\")\n",
        "\n",
        "        # Shuffle all samples for random split\n",
        "        random.shuffle(all_samples)\n",
        "\n",
        "        # Create custom train/test split\n",
        "        total_needed = train_samples + test_samples\n",
        "        if len(all_samples) < total_needed:\n",
        "            print(f\"‚ö†Ô∏è Warning: Only {len(all_samples)} samples available, but {total_needed} requested\")\n",
        "            train_samples = min(train_samples, len(all_samples) - test_samples)\n",
        "            test_samples = len(all_samples) - train_samples\n",
        "\n",
        "        if split == 'train':\n",
        "            self.samples = all_samples[:train_samples]\n",
        "            print(f\"‚úÖ Created training set with {len(self.samples)} samples\")\n",
        "        else:  # test\n",
        "            self.samples = all_samples[train_samples:train_samples + test_samples]\n",
        "            print(f\"‚úÖ Created test set with {len(self.samples)} samples\")\n",
        "\n",
        "        if self.samples:\n",
        "            sample_texts = [t for _, t in self.samples[:10]]\n",
        "            lengths = [len(t) for _, t in self.samples]\n",
        "            print(f\"Sample texts: {sample_texts}\")\n",
        "            print(f\"Text lengths: min={min(lengths)}, max={max(lengths)}, avg={np.mean(lengths):.1f}\")\n",
        "            print(f\"Vocabulary size: {self.num_classes}\")\n",
        "\n",
        "        # Enhanced transforms with augmentation\n",
        "        if self.augment:\n",
        "            self.transform = T.Compose([\n",
        "                T.ToPILImage(),\n",
        "                T.Resize((img_height, img_width), antialias=True),\n",
        "                T.Grayscale(),\n",
        "                # Data augmentation\n",
        "                T.RandomApply([T.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))], p=0.3),\n",
        "                T.ColorJitter(brightness=0.3, contrast=0.3),\n",
        "                T.RandomApply([T.RandomRotation(degrees=2)], p=0.2),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize((0.5,), (0.5,))  # [-1, 1]\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = T.Compose([\n",
        "                T.ToPILImage(),\n",
        "                T.Resize((img_height, img_width), antialias=True),\n",
        "                T.Grayscale(),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize((0.5,), (0.5,))  # [-1, 1]\n",
        "            ])\n",
        "\n",
        "    def _load_mat_data(self, mat_file, data_dir, original_split):\n",
        "        \"\"\"Load data from .mat file and return list of (img_path, text) tuples\"\"\"\n",
        "        samples = []\n",
        "\n",
        "        try:\n",
        "            mat_data = scipy.io.loadmat(mat_file)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load {mat_file}: {e}\")\n",
        "            return samples\n",
        "\n",
        "        data_key = f\"{original_split}data\"\n",
        "        if data_key not in mat_data:\n",
        "            available = [k for k in mat_data.keys() if not k.startswith('__')]\n",
        "            print(f\"‚ùå Key '{data_key}' not found in {mat_file}. Available: {available}\")\n",
        "            return samples\n",
        "\n",
        "        data = mat_data[data_key]\n",
        "        N = data.shape[1] if data.shape[0] == 1 else data.shape[0]\n",
        "        print(f\"Processing {N} samples from {original_split} data...\")\n",
        "\n",
        "        for i in range(N):\n",
        "            try:\n",
        "                sample = data[0, i] if data.shape[0] == 1 else data[i, 0]\n",
        "\n",
        "                # Extract image path\n",
        "                img_name = self._safe_extract_string(sample[0])\n",
        "                if not img_name:\n",
        "                    img_filename = f\"sample_{i}.png\"\n",
        "                else:\n",
        "                    img_filename = os.path.basename(img_name.strip())\n",
        "                img_path = os.path.join(data_dir, img_filename)\n",
        "\n",
        "                # Extract text\n",
        "                text = self._safe_extract_string(sample[3])\n",
        "                text = self._clean_text(text)\n",
        "\n",
        "                if os.path.exists(img_path) and 1 <= len(text) <= 23:\n",
        "                    samples.append((img_path, text))\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def _safe_extract_string(self, data):\n",
        "        \"\"\"Robustly extract string from .mat field (handles str, numeric array, object array)\"\"\"\n",
        "        try:\n",
        "            if isinstance(data, str):\n",
        "                return data.strip()\n",
        "\n",
        "            if isinstance(data, np.ndarray):\n",
        "                # Case 1: Numeric array (ASCII codes)\n",
        "                if np.issubdtype(data.dtype, np.number):\n",
        "                    chars = []\n",
        "                    for c in data.flatten():\n",
        "                        try:\n",
        "                            c_int = int(c)\n",
        "                            if 32 <= c_int <= 126:  # Printable ASCII\n",
        "                                chars.append(chr(c_int))\n",
        "                        except (ValueError, TypeError):\n",
        "                            continue\n",
        "                    return ''.join(chars).strip()\n",
        "\n",
        "                # Case 2: String array (U/S dtype)\n",
        "                elif data.dtype.kind in ['U', 'S']:\n",
        "                    return str(data.flatten()[0]).strip() if data.size > 0 else \"\"\n",
        "\n",
        "                # Case 3: Object array (common in scipy.io.loadmat)\n",
        "                elif data.dtype == np.object_:\n",
        "                    item = data.item() if data.size == 1 else data[0]\n",
        "                    if isinstance(item, str):\n",
        "                        return item.strip()\n",
        "                    return self._safe_extract_string(item)\n",
        "\n",
        "            return str(data).strip()\n",
        "        except Exception as e:\n",
        "            return \"\"\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Keep only valid characters in full ASCII set\"\"\"\n",
        "        return ''.join(c for c in text if c in self.chars).strip()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, text = self.samples[idx]\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            print(f\"‚ö†Ô∏è Warning: Failed to load image {img_path}, using blank\")\n",
        "            img = np.ones((self.img_height, self.img_width), dtype=np.uint8) * 255\n",
        "\n",
        "        # Additional noise augmentation for training\n",
        "        if self.augment and random.random() < 0.2:\n",
        "            noise = np.random.normal(0, 10, img.shape).astype(np.uint8)\n",
        "            img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
        "\n",
        "        text_indices = [self.char_to_idx[char] for char in text if char in self.char_to_idx]\n",
        "        img_tensor = self.transform(img)\n",
        "        return img_tensor, text_indices, text\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 2. ENHANCED CRNN MODEL WITH ATTENTION\n",
        "# ======================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class EnhancedDeepCRNN(nn.Module):\n",
        "    def __init__(self, img_height=32, num_classes=96, hidden_size=256, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Enhanced CNN backbone with residual connections\n",
        "        self.cnn = nn.Sequential(\n",
        "            # Stage 1\n",
        "            nn.Conv2d(1, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            # Stage 2\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            # Stage 3\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 1), (2, 1)),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            # Stage 4\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 1), (2, 1)),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            # Stage 5\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 1), (2, 1)),\n",
        "        )\n",
        "\n",
        "        # Enhanced RNN with more layers and higher capacity\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=512,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=3,  # Increased from 2\n",
        "            bidirectional=True,\n",
        "            batch_first=False,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size * 2,\n",
        "            num_heads=8,\n",
        "            dropout=dropout,\n",
        "            batch_first=False\n",
        "        )\n",
        "\n",
        "        # Positional encoding for attention\n",
        "        self.pos_encoding = PositionalEncoding(hidden_size * 2)\n",
        "\n",
        "        # Classification layers with residual connection\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.LSTM):\n",
        "                for name, param in m.named_parameters():\n",
        "                    if 'weight' in name:\n",
        "                        nn.init.xavier_uniform_(param)\n",
        "                    elif 'bias' in name:\n",
        "                        nn.init.constant_(param, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN feature extraction\n",
        "        conv = self.cnn(x)  # [B, 512, 1, W]\n",
        "        b, c, h, w = conv.size()\n",
        "        conv = conv.view(b, c * h, w).permute(2, 0, 1)  # [W, B, 512]\n",
        "\n",
        "        # RNN processing\n",
        "        rnn_out, _ = self.rnn(conv)  # [W, B, hidden_size*2]\n",
        "        rnn_out = self.dropout(rnn_out)\n",
        "\n",
        "        # Add positional encoding and apply attention\n",
        "        rnn_out_pe = self.pos_encoding(rnn_out)\n",
        "        attended_out, _ = self.attention(rnn_out_pe, rnn_out_pe, rnn_out_pe)\n",
        "\n",
        "        # Residual connection\n",
        "        attended_out = attended_out + rnn_out\n",
        "        attended_out = self.dropout(attended_out)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(attended_out)\n",
        "        return F.log_softmax(output, dim=2)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 3. ENHANCED CTC UTILITIES WITH BEAM SEARCH\n",
        "# ======================\n",
        "def ctc_collate_fn(batch):\n",
        "    images, text_indices_list, raw_texts = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    targets = []\n",
        "    target_lengths = []\n",
        "    for text_indices in text_indices_list:\n",
        "        if len(text_indices) > 0:\n",
        "            targets.extend(text_indices)\n",
        "            target_lengths.append(len(text_indices))\n",
        "        else:\n",
        "            targets.append(0)\n",
        "            target_lengths.append(1)\n",
        "    return images, torch.tensor(targets), torch.tensor(target_lengths), raw_texts\n",
        "\n",
        "def beam_search_decode(log_probs, idx_to_char, beam_width=5, blank_idx=0):\n",
        "    \"\"\"Beam search CTC decoding for better accuracy\"\"\"\n",
        "    seq_len, batch_size, num_classes = log_probs.shape\n",
        "    predictions = []\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        probs = log_probs[:, b, :].exp().cpu().numpy()  # Convert to probabilities\n",
        "\n",
        "        # Initialize beam\n",
        "        beam = [([], 0.0)]  # (sequence, log_prob)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            new_beam = []\n",
        "\n",
        "            for seq, log_prob in beam:\n",
        "                for c in range(num_classes):\n",
        "                    new_log_prob = log_prob + np.log(probs[t, c] + 1e-8)\n",
        "\n",
        "                    if c == blank_idx:\n",
        "                        # Blank - no character added\n",
        "                        new_beam.append((seq, new_log_prob))\n",
        "                    else:\n",
        "                        # Character\n",
        "                        if len(seq) == 0 or seq[-1] != c:\n",
        "                            # New character or different from previous\n",
        "                            new_seq = seq + [c]\n",
        "                            new_beam.append((new_seq, new_log_prob))\n",
        "                        else:\n",
        "                            # Same as previous - don't add\n",
        "                            new_beam.append((seq, new_log_prob))\n",
        "\n",
        "            # Keep top beam_width candidates\n",
        "            beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "        # Get best sequence\n",
        "        best_seq = beam[0][0]\n",
        "        decoded_text = ''.join([idx_to_char.get(idx, '') for idx in best_seq if idx in idx_to_char])\n",
        "        predictions.append(decoded_text)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def ctc_decode(log_probs, idx_to_char, blank_idx=0, use_beam_search=False, beam_width=5):\n",
        "    \"\"\"Enhanced CTC decoding with optional beam search\"\"\"\n",
        "    if use_beam_search:\n",
        "        return beam_search_decode(log_probs, idx_to_char, beam_width, blank_idx)\n",
        "\n",
        "    # Standard greedy decoding\n",
        "    seq_len, batch_size, num_classes = log_probs.shape\n",
        "    predictions = []\n",
        "    for b in range(batch_size):\n",
        "        pred_indices = log_probs[:, b, :].argmax(dim=1).cpu().numpy()\n",
        "        decoded = []\n",
        "        prev_idx = blank_idx\n",
        "        for idx in pred_indices:\n",
        "            if idx != blank_idx and idx != prev_idx:\n",
        "                if idx in idx_to_char and idx_to_char[idx] != '<BLANK>':\n",
        "                    decoded.append(idx_to_char[idx])\n",
        "            prev_idx = idx\n",
        "        predictions.append(''.join(decoded))\n",
        "    return predictions\n",
        "\n",
        "def calculate_metrics(pred_texts, true_texts):\n",
        "    \"\"\"Calculate word and character accuracy\"\"\"\n",
        "    if not pred_texts or not true_texts:\n",
        "        return 0.0, 0.0\n",
        "    word_acc = sum(p == t for p, t in zip(pred_texts, true_texts)) / len(true_texts)\n",
        "    total_chars = sum(max(len(p), len(t)) for p, t in zip(pred_texts, true_texts))\n",
        "    if total_chars == 0:\n",
        "        return word_acc, 0.0\n",
        "    correct_chars = sum(p[i] == t[i] for p, t in zip(pred_texts, true_texts) for i in range(min(len(p), len(t))))\n",
        "    char_acc = correct_chars / total_chars\n",
        "    return word_acc, char_acc\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 4. ENHANCED TRAINING WITH EARLY STOPPING AND MIXED PRECISION\n",
        "# ======================\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=20, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_score):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_score\n",
        "        elif val_score < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = val_score\n",
        "            self.counter = 0\n",
        "\n",
        "def train_enhanced_deep_ocr():\n",
        "    print(\"üöÄ ENHANCED DEEP OCR TRAINING WITH IMPROVEMENTS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # CONFIGURATION\n",
        "    dataset_root = \"/content/iiit5k_dataset\"\n",
        "\n",
        "    # Verify dataset exists\n",
        "    print(f\"üîç Checking dataset root: {dataset_root}\")\n",
        "    print(f\"üìÅ Path exists: {os.path.exists(dataset_root)}\")\n",
        "\n",
        "    if not os.path.exists(dataset_root):\n",
        "        alternative_paths = [\"./iiit5k_dataset\", \"/content/iiit5k_dataset\"]\n",
        "        for alt_path in alternative_paths:\n",
        "            if os.path.exists(alt_path):\n",
        "                dataset_root = alt_path\n",
        "                print(f\"‚úÖ Found dataset at alternative path: {dataset_root}\")\n",
        "                break\n",
        "        else:\n",
        "            print(f\"‚ùå Dataset root not found at any location\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"‚úÖ Dataset root found: {dataset_root}\")\n",
        "\n",
        "    # Verify IIIT5K subdirectory structure\n",
        "    iiit5k_path = os.path.join(dataset_root, \"IIIT5K\")\n",
        "    print(f\"üîç Checking IIIT5K path: {iiit5k_path}\")\n",
        "    print(f\"üìÅ IIIT5K exists: {os.path.exists(iiit5k_path)}\")\n",
        "\n",
        "    if os.path.exists(iiit5k_path):\n",
        "        contents = os.listdir(iiit5k_path)\n",
        "        print(f\"üìä IIIT5K contents: {contents}\")\n",
        "        required_items = ['train', 'test', 'traindata.mat', 'testdata.mat']\n",
        "        missing_items = [item for item in required_items if item not in contents]\n",
        "        if missing_items:\n",
        "            print(f\"‚ùå Missing required items: {missing_items}\")\n",
        "            return None\n",
        "        else:\n",
        "            print(\"‚úÖ All required dataset components found!\")\n",
        "    else:\n",
        "        print(f\"‚ùå IIIT5K subdirectory not found at: {iiit5k_path}\")\n",
        "        return None\n",
        "\n",
        "    # Enhanced hyperparameters\n",
        "    batch_size = 32\n",
        "    max_epochs = 200  # Reduced since we have early stopping\n",
        "    learning_rate = 2e-3  # Slightly higher for OneCycle\n",
        "    train_samples = 4500\n",
        "    test_samples = 500\n",
        "    hidden_size = 256  # Increased capacity\n",
        "    dropout = 0.3\n",
        "\n",
        "    print(f\"Enhanced configuration:\")\n",
        "    print(f\"  Custom split: {train_samples} train, {test_samples} test\")\n",
        "    print(f\"  Batch size: {batch_size}\")\n",
        "    print(f\"  Max epochs: {max_epochs}\")\n",
        "    print(f\"  Learning rate: {learning_rate}\")\n",
        "    print(f\"  Hidden size: {hidden_size}\")\n",
        "    print(f\"  Dropout: {dropout}\")\n",
        "\n",
        "    # Load enhanced datasets with augmentation\n",
        "    print(\"\\nCreating enhanced datasets with augmentation...\")\n",
        "    try:\n",
        "        train_dataset = EnhancedCustomSplitIIIT5KDataset(\n",
        "            dataset_root, 'train', 32, 128, train_samples, test_samples, augment=True\n",
        "        )\n",
        "        test_dataset = EnhancedCustomSplitIIIT5KDataset(\n",
        "            dataset_root, 'test', 32, 128, train_samples, test_samples, augment=False\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Dataset creation failed: {e}\")\n",
        "        return None\n",
        "\n",
        "    if len(train_dataset) == 0 or len(test_dataset) == 0:\n",
        "        print(\"‚ùå No valid data loaded.\")\n",
        "        return None\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                              collate_fn=ctc_collate_fn, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                             collate_fn=ctc_collate_fn, num_workers=2, pin_memory=True)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\nDevice: {device}\")\n",
        "\n",
        "    # Enhanced model\n",
        "    model = EnhancedDeepCRNN(32, train_dataset.num_classes, hidden_size, dropout).to(device)\n",
        "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "\n",
        "    # OneCycle learning rate scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = OneCycleLR(optimizer, max_lr=learning_rate,\n",
        "                          steps_per_epoch=len(train_loader), epochs=max_epochs)\n",
        "\n",
        "    # Mixed precision training\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(patience=25, min_delta=0.001)\n",
        "\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"Training batches: {len(train_loader)}\")\n",
        "    print(f\"Test batches: {len(test_loader)}\")\n",
        "\n",
        "    best_word_acc = 0.0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_word_acc = 0.0\n",
        "        num_train_batches = 0\n",
        "\n",
        "        print(f\"\\nüìà Epoch {epoch+1}/{max_epochs}\")\n",
        "        pbar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "        for images, targets, target_lengths, raw_texts in pbar:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            targets = targets.to(device, non_blocking=True)\n",
        "            target_lengths = target_lengths.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            with autocast():\n",
        "                log_probs = model(images)\n",
        "                input_lengths = torch.full((images.size(0),), log_probs.size(0),\n",
        "                                         device=device, dtype=torch.long)\n",
        "                loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
        "\n",
        "            # Mixed precision backward pass\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy every 20 batches\n",
        "            if num_train_batches % 20 == 0:\n",
        "                pred_texts = ctc_decode(log_probs.cpu(), train_dataset.idx_to_char)\n",
        "                word_acc, _ = calculate_metrics(pred_texts, raw_texts)\n",
        "                train_word_acc += word_acc\n",
        "                pbar.set_postfix({\"loss\": f\"{loss.item():.3f}\", \"word_acc\": f\"{word_acc:.3f}\"})\n",
        "\n",
        "            num_train_batches += 1\n",
        "\n",
        "        train_loss /= num_train_batches\n",
        "        train_word_acc /= (num_train_batches // 20 + 1)\n",
        "\n",
        "        # Validation with beam search\n",
        "        model.eval()\n",
        "        val_preds, val_truths = [], []\n",
        "        with torch.no_grad():\n",
        "            for images, targets, target_lengths, raw_texts in test_loader:\n",
        "                images = images.to(device, non_blocking=True)\n",
        "                with autocast():\n",
        "                    log_probs = model(images)\n",
        "                # Use beam search for validation\n",
        "                pred_texts = ctc_decode(log_probs.cpu(), test_dataset.idx_to_char,\n",
        "                                      use_beam_search=True, beam_width=3)\n",
        "                val_preds.extend(pred_texts)\n",
        "                val_truths.extend(raw_texts)\n",
        "\n",
        "        val_word_acc, val_char_acc = calculate_metrics(val_preds, val_truths)\n",
        "\n",
        "        print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
        "        print(f\"  Training Loss: {train_loss:.4f}\")\n",
        "        print(f\"  Training Word Acc: {train_word_acc:.4f}\")\n",
        "        print(f\"  Validation Word Acc: {val_word_acc:.4f}\")\n",
        "        print(f\"  Validation Char Acc: {val_char_acc:.4f}\")\n",
        "        print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_word_acc > best_word_acc:\n",
        "            best_word_acc = val_word_acc\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_word_acc': val_word_acc,\n",
        "                'val_char_acc': val_char_acc,\n",
        "                'idx_to_char': train_dataset.idx_to_char,\n",
        "                'num_classes': train_dataset.num_classes,\n",
        "                'hidden_size': hidden_size\n",
        "            }, 'enhanced_deep_ocr_model.pth')\n",
        "            print(f\"  ‚úÖ New best model saved! Word Acc: {best_word_acc:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping check\n",
        "        early_stopping(val_word_acc)\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        # Sample predictions\n",
        "        print(f\"\\nüéØ Sample predictions (with beam search):\")\n",
        "        indices = np.random.choice(len(val_preds), min(5, len(val_preds)), replace=False)\n",
        "        for i in indices:\n",
        "            print(f\"  True: '{val_truths[i]}' | Pred: '{val_preds[i]}'\")\n",
        "\n",
        "    print(f\"\\nüéâ Training completed!\")\n",
        "    print(f\"Best validation word accuracy: {best_word_acc:.4f}\")\n",
        "    return best_word_acc\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 5. MODEL INFERENCE WITH BEAM SEARCH\n",
        "# ======================\n",
        "def load_and_test_model(model_path, test_images_dir=None):\n",
        "    \"\"\"Load trained model and test on new images\"\"\"\n",
        "    print(\"üîç Loading trained model...\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "    # Recreate model\n",
        "    model = EnhancedDeepCRNN(\n",
        "        img_height=32,\n",
        "        num_classes=checkpoint['num_classes'],\n",
        "        hidden_size=checkpoint.get('hidden_size', 256)\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    idx_to_char = checkpoint['idx_to_char']\n",
        "\n",
        "    print(f\"‚úÖ Model loaded successfully!\")\n",
        "    print(f\"   Validation Word Accuracy: {checkpoint['val_word_acc']:.4f}\")\n",
        "    print(f\"   Validation Char Accuracy: {checkpoint['val_char_acc']:.4f}\")\n",
        "\n",
        "    # Transform for inference\n",
        "    transform = T.Compose([\n",
        "        T.ToPILImage(),\n",
        "        T.Resize((32, 128), antialias=True),\n",
        "        T.Grayscale(),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    def predict_text(image_path):\n",
        "        \"\"\"Predict text from single image\"\"\"\n",
        "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            return \"Error: Could not load image\"\n",
        "\n",
        "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with autocast():\n",
        "                log_probs = model(img_tensor)\n",
        "\n",
        "            # Use beam search for better accuracy\n",
        "            pred_texts = ctc_decode(log_probs.cpu(), idx_to_char,\n",
        "                                  use_beam_search=True, beam_width=5)\n",
        "            return pred_texts[0]\n",
        "\n",
        "    return predict_text\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 6. ADVANCED EVALUATION METRICS\n",
        "# ======================\n",
        "def detailed_evaluation(model, test_loader, idx_to_char, device):\n",
        "    \"\"\"Comprehensive model evaluation with detailed metrics\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_truths = []\n",
        "    char_confusion = {}\n",
        "    length_accuracy = {i: {'correct': 0, 'total': 0} for i in range(1, 21)}\n",
        "\n",
        "    print(\"üîç Running detailed evaluation...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets, target_lengths, raw_texts in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "\n",
        "            with autocast():\n",
        "                log_probs = model(images)\n",
        "\n",
        "            # Get predictions with both greedy and beam search\n",
        "            greedy_preds = ctc_decode(log_probs.cpu(), idx_to_char, use_beam_search=False)\n",
        "            beam_preds = ctc_decode(log_probs.cpu(), idx_to_char, use_beam_search=True, beam_width=5)\n",
        "\n",
        "            all_preds.extend(beam_preds)\n",
        "            all_truths.extend(raw_texts)\n",
        "\n",
        "            # Length-based accuracy\n",
        "            for pred, truth in zip(beam_preds, raw_texts):\n",
        "                length = min(len(truth), 20)\n",
        "                length_accuracy[length]['total'] += 1\n",
        "                if pred == truth:\n",
        "                    length_accuracy[length]['correct'] += 1\n",
        "\n",
        "            # Character-level confusion analysis\n",
        "            for pred, truth in zip(beam_preds, raw_texts):\n",
        "                for i, (p_char, t_char) in enumerate(zip(pred, truth)):\n",
        "                    if t_char not in char_confusion:\n",
        "                        char_confusion[t_char] = {'correct': 0, 'total': 0, 'confused_with': {}}\n",
        "                    char_confusion[t_char]['total'] += 1\n",
        "                    if p_char == t_char:\n",
        "                        char_confusion[t_char]['correct'] += 1\n",
        "                    else:\n",
        "                        if p_char not in char_confusion[t_char]['confused_with']:\n",
        "                            char_confusion[t_char]['confused_with'][p_char] = 0\n",
        "                        char_confusion[t_char]['confused_with'][p_char] += 1\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    word_acc, char_acc = calculate_metrics(all_preds, all_truths)\n",
        "\n",
        "    print(f\"\\nüìä Detailed Evaluation Results:\")\n",
        "    print(f\"   Overall Word Accuracy: {word_acc:.4f}\")\n",
        "    print(f\"   Overall Character Accuracy: {char_acc:.4f}\")\n",
        "\n",
        "    # Length-based accuracy\n",
        "    print(f\"\\nüìè Accuracy by text length:\")\n",
        "    for length in range(1, 11):  # Show first 10 lengths\n",
        "        if length_accuracy[length]['total'] > 0:\n",
        "            acc = length_accuracy[length]['correct'] / length_accuracy[length]['total']\n",
        "            print(f\"   Length {length}: {acc:.3f} ({length_accuracy[length]['correct']}/{length_accuracy[length]['total']})\")\n",
        "\n",
        "    # Most confused characters\n",
        "    print(f\"\\nüî§ Most problematic characters:\")\n",
        "    char_errors = []\n",
        "    for char, stats in char_confusion.items():\n",
        "        if stats['total'] > 5:  # Only consider characters that appear frequently\n",
        "            error_rate = 1 - (stats['correct'] / stats['total'])\n",
        "            char_errors.append((char, error_rate, stats['total']))\n",
        "\n",
        "    char_errors.sort(key=lambda x: x[1], reverse=True)\n",
        "    for char, error_rate, total in char_errors[:10]:\n",
        "        print(f\"   '{char}': {error_rate:.3f} error rate ({total} samples)\")\n",
        "        # Show top confusions\n",
        "        if char in char_confusion and char_confusion[char]['confused_with']:\n",
        "            top_confusions = sorted(char_confusion[char]['confused_with'].items(),\n",
        "                                  key=lambda x: x[1], reverse=True)[:3]\n",
        "            confusion_str = ', '.join([f\"'{conf_char}'({count})\" for conf_char, count in top_confusions])\n",
        "            print(f\"      ‚Üí Often confused with: {confusion_str}\")\n",
        "\n",
        "    return word_acc, char_acc\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 7. MAIN EXECUTION WITH ENHANCED OPTIONS\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"‚ö° ENHANCED DEEP OCR SYSTEM\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nChoose option:\")\n",
        "        print(\"1. Train enhanced model with improvements\")\n",
        "        print(\"2. Load and test existing model\")\n",
        "        print(\"3. Detailed evaluation of existing model\")\n",
        "        print(\"4. Exit\")\n",
        "\n",
        "        choice = input(\"Choice: \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            print(\"\\nüöÄ Starting enhanced training...\")\n",
        "            accuracy = train_enhanced_deep_ocr()\n",
        "            if accuracy is not None:\n",
        "                print(f\"\\nüèÜ Final result: {accuracy:.4f} word accuracy\")\n",
        "                print(\"‚úÖ Model saved as 'enhanced_deep_ocr_model.pth'\")\n",
        "            else:\n",
        "                print(\"\\n‚ùå Training failed. Check error messages above.\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            model_path = input(\"Enter model path (default: enhanced_deep_ocr_model.pth): \")\n",
        "            if not model_path:\n",
        "                model_path = \"enhanced_deep_ocr_model.pth\"\n",
        "\n",
        "            if not os.path.exists(model_path):\n",
        "                print(f\"‚ùå Model file not found: {model_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                predict_fn = load_and_test_model(model_path)\n",
        "\n",
        "                while True:\n",
        "                    image_path = input(\"\\nEnter image path (or 'back' to return): \")\n",
        "                    if image_path.lower() == 'back':\n",
        "                        break\n",
        "\n",
        "                    if not os.path.exists(image_path):\n",
        "                        print(f\"‚ùå Image not found: {image_path}\")\n",
        "                        continue\n",
        "\n",
        "                    result = predict_fn(image_path)\n",
        "                    print(f\"üéØ Predicted text: '{result}'\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error loading model: {e}\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            model_path = input(\"Enter model path (default: enhanced_deep_ocr_model.pth): \")\n",
        "            if not model_path:\n",
        "                model_path = \"enhanced_deep_ocr_model.pth\"\n",
        "\n",
        "            if not os.path.exists(model_path):\n",
        "                print(f\"‚ùå Model file not found: {model_path}\")\n",
        "                continue\n",
        "\n",
        "            # Load test dataset for evaluation\n",
        "            dataset_root = \"/content/iiit5k_dataset\"\n",
        "            try:\n",
        "                test_dataset = EnhancedCustomSplitIIIT5KDataset(\n",
        "                    dataset_root, 'test', 32, 128, 4500, 500, augment=False\n",
        "                )\n",
        "                test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
        "                                       collate_fn=ctc_collate_fn, num_workers=2)\n",
        "\n",
        "                # Load model\n",
        "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "                checkpoint = torch.load(model_path, map_location=device)\n",
        "                model = EnhancedDeepCRNN(\n",
        "                    img_height=32,\n",
        "                    num_classes=checkpoint['num_classes'],\n",
        "                    hidden_size=checkpoint.get('hidden_size', 256)\n",
        "                ).to(device)\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "                # Run detailed evaluation\n",
        "                detailed_evaluation(model, test_loader, checkpoint['idx_to_char'], device)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error during evaluation: {e}\")\n",
        "\n",
        "        elif choice == \"4\":\n",
        "            print(\"Goodbye! üëã\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå Invalid choice. Please try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEVGsp3BTp4t",
        "outputId": "19a0f191-9a17-4877-d2be-97bafc2022a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° ENHANCED DEEP OCR SYSTEM\n",
            "==================================================\n",
            "\n",
            "Choose option:\n",
            "1. Train enhanced model with improvements\n",
            "2. Load and test existing model\n",
            "3. Detailed evaluation of existing model\n",
            "4. Exit\n",
            "Choice: 2\n",
            "Enter model path (default: enhanced_deep_ocr_model.pth): /content/deep_ocr_custom_split_model.pth\n",
            "üîç Loading trained model...\n",
            "‚ùå Error loading model: Error(s) in loading state_dict for EnhancedDeepCRNN:\n",
            "\tMissing key(s) in state_dict: \"cnn.3.weight\", \"cnn.3.bias\", \"cnn.4.running_mean\", \"cnn.4.running_var\", \"cnn.17.weight\", \"cnn.17.bias\", \"cnn.17.running_mean\", \"cnn.17.running_var\", \"cnn.24.weight\", \"cnn.24.bias\", \"cnn.25.weight\", \"cnn.25.bias\", \"cnn.25.running_mean\", \"cnn.25.running_var\", \"cnn.27.weight\", \"cnn.27.bias\", \"cnn.28.weight\", \"cnn.28.bias\", \"cnn.28.running_mean\", \"cnn.28.running_var\", \"cnn.32.weight\", \"cnn.32.bias\", \"cnn.33.weight\", \"cnn.33.bias\", \"cnn.33.running_mean\", \"cnn.33.running_var\", \"rnn.weight_ih_l2\", \"rnn.weight_hh_l2\", \"rnn.bias_ih_l2\", \"rnn.bias_hh_l2\", \"rnn.weight_ih_l2_reverse\", \"rnn.weight_hh_l2_reverse\", \"rnn.bias_ih_l2_reverse\", \"rnn.bias_hh_l2_reverse\", \"attention.in_proj_weight\", \"attention.in_proj_bias\", \"attention.out_proj.weight\", \"attention.out_proj.bias\", \"pos_encoding.pe\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.3.weight\", \"classifier.3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"cnn.5.weight\", \"cnn.5.bias\", \"cnn.5.running_mean\", \"cnn.5.running_var\", \"cnn.5.num_batches_tracked\", \"cnn.15.weight\", \"cnn.15.bias\", \"cnn.16.running_mean\", \"cnn.16.running_var\", \"cnn.16.num_batches_tracked\", \"classifier.weight\", \"classifier.bias\". \n",
            "\tsize mismatch for cnn.4.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64]).\n",
            "\tsize mismatch for cnn.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n",
            "\tsize mismatch for cnn.8.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n",
            "\tsize mismatch for cnn.8.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for cnn.9.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for cnn.9.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for cnn.9.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for cnn.9.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for cnn.11.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n",
            "\tsize mismatch for cnn.11.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for cnn.12.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for cnn.12.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for cnn.12.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for cnn.12.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for cnn.16.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n",
            "\tsize mismatch for cnn.16.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for cnn.19.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n",
            "\tsize mismatch for cnn.19.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for cnn.20.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for cnn.20.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for cnn.20.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for cnn.20.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for rnn.weight_ih_l0: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n",
            "\tsize mismatch for rnn.weight_hh_l0: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n",
            "\tsize mismatch for rnn.bias_ih_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n",
            "\tsize mismatch for rnn.bias_hh_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n",
            "\tsize mismatch for rnn.weight_ih_l0_reverse: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n",
            "\tsize mismatch for rnn.weight_hh_l0_reverse: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n",
            "\tsize mismatch for rnn.bias_ih_l0_reverse: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n",
            "\tsize mismatch for rnn.bias_hh_l0_reverse: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n",
            "\tsize mismatch for rnn.weight_ih_l1: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n",
            "\tsize mismatch for rnn.weight_hh_l1: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n",
            "\tsize mismatch for rnn.bias_ih_l1: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n",
            "\tsize mismatch for rnn.bias_hh_l1: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n",
            "\tsize mismatch for rnn.weight_ih_l1_reverse: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n",
            "\tsize mismatch for rnn.weight_hh_l1_reverse: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n",
            "\tsize mismatch for rnn.bias_ih_l1_reverse: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n",
            "\tsize mismatch for rnn.bias_hh_l1_reverse: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n",
            "\n",
            "Choose option:\n",
            "1. Train enhanced model with improvements\n",
            "2. Load and test existing model\n",
            "3. Detailed evaluation of existing model\n",
            "4. Exit\n",
            "Choice: 4\n",
            "Goodbye! üëã\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test  the model\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# ======================\n",
        "# 1. DEEP CRNN MODEL DEFINITION (MUST BE INCLUDED!)\n",
        "# ======================\n",
        "class DeepCRNN(nn.Module):\n",
        "    def __init__(self, img_height=32, num_classes=96, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d((2, 1), (2, 1)),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d((2, 1), (2, 1)),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d((2, 1), (2, 1)),\n",
        "        )\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=512,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=2,\n",
        "            bidirectional=True,\n",
        "            batch_first=False,\n",
        "            dropout=0.3\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_size * 2, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv = self.cnn(x)  # [B, 512, 1, W]\n",
        "        b, c, h, w = conv.size()\n",
        "        conv = conv.view(b, c * h, w).permute(2, 0, 1)  # [W, B, 512]\n",
        "        rnn_out, _ = self.rnn(conv)\n",
        "        rnn_out = self.dropout(rnn_out)\n",
        "        output = self.classifier(rnn_out)\n",
        "        return F.log_softmax(output, dim=2)\n",
        "\n",
        "# ======================\n",
        "# 2. CTC DECODE FUNCTION\n",
        "# ======================\n",
        "def ctc_decode(log_probs, idx_to_char, blank_idx=0):\n",
        "    \"\"\"Greedy CTC decoding\"\"\"\n",
        "    pred_indices = log_probs.argmax(dim=2).squeeze(1).cpu().numpy()  # [T]\n",
        "    decoded = []\n",
        "    prev_idx = blank_idx\n",
        "    for idx in pred_indices:\n",
        "        if idx != blank_idx and idx != prev_idx:\n",
        "            if idx in idx_to_char and idx_to_char[idx] != '<BLANK>':\n",
        "                decoded.append(idx_to_char[idx])\n",
        "        prev_idx = idx\n",
        "    return ''.join(decoded)\n",
        "\n",
        "# ======================\n",
        "# 3. INFERENCE SETUP\n",
        "# ======================\n",
        "model_path = \"/content/deep_ocr_custom_split_model.pth\"\n",
        "image_path = \"/content/OCR_Hamed5.jpg\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Image transform\n",
        "transform = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Resize((32, 128), antialias=True),\n",
        "    T.Grayscale(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# ======================\n",
        "# 4. LOAD MODEL\n",
        "# ======================\n",
        "print(\"üöÄ Loading trained model...\")\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "num_classes = checkpoint['num_classes']\n",
        "idx_to_char = checkpoint['idx_to_char']\n",
        "\n",
        "# Now we can create the model\n",
        "model = DeepCRNN(img_height=32, num_classes=num_classes, hidden_size=128).to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully with {num_classes} classes.\")\n",
        "\n",
        "# ======================\n",
        "# 5. LOAD AND PREPROCESS IMAGE\n",
        "# ======================\n",
        "def preprocess_image(img_path):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(f\"Cannot load image at {img_path}\")\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # To 3-channel for ToPILImage\n",
        "    return transform(img).unsqueeze(0)  # Add batch dim\n",
        "\n",
        "print(f\"üñºÔ∏è  Preprocessing image: {image_path}\")\n",
        "image_tensor = preprocess_image(image_path).to(device)\n",
        "\n",
        "# ======================\n",
        "# 6. RUN INFERENCE\n",
        "# ======================\n",
        "print(\"üîç Running inference...\")\n",
        "with torch.no_grad():\n",
        "    log_probs = model(image_tensor)  # [T, 1, num_classes]\n",
        "    predicted_text = ctc_decode(log_probs, idx_to_char)\n",
        "\n",
        "# ======================\n",
        "# 7. OUTPUT RESULT\n",
        "# ======================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéØ INFERENCE RESULT\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Image: {image_path}\")\n",
        "print(f\"Predicted Text: '{predicted_text}'\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwCyRFZLiIOf",
        "outputId": "41a640ba-1b8d-4336-f398-6c3dcf15ddfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Loading trained model...\n",
            "‚úÖ Model loaded successfully with 95 classes.\n",
            "üñºÔ∏è  Preprocessing image: /content/OCR_Hamed5.jpg\n",
            "üîç Running inference...\n",
            "\n",
            "==================================================\n",
            "üéØ INFERENCE RESULT\n",
            "==================================================\n",
            "Image: /content/OCR_Hamed5.jpg\n",
            "Predicted Text: 'HRERRO'\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}